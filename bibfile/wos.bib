@article{ ISI:000474549100005,
Author = {Le Trieu Phong and Tran Thi Phuong},
Title = {{Privacy-Preserving Deep Learning via Weight Transmission}},
Journal = {{IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY}},
Year = {{2019}},
Volume = {{14}},
Number = {{11}},
Pages = {{3003-3015}},
Month = {{NOV}},
Abstract = {{This paper considers the scenario that multiple data owners wish to
   apply a machine learning method over the combined dataset of all owners
   to obtain the best possible learning output but do not want to share the
   local datasets owing to privacy concerns. We design systems for the
   scenario that the stochastic gradient descent (SGD) algorithm is used as
   the machine learning method, because SGD (or its variants) is at the
   heart of recent deep learning techniques over neural networks. Our
   systems differ from the existing systems in the following features: 1)
   any activation function can be used, meaning that no
   privacy-preserving-friendly approximation is required; 2) gradients
   computed by SGD are not shared but the weight parameters are shared
   instead; and 3) robustness against colluding parties even in the extreme
   case that only one honest party exists. One of our systems requires a
   shared symmetric key among the data owners (trainers) to ensure the
   secrecy of the weight parameters against a central server. We prove that
   our systems, while privacy preserving, achieve the same learning
   accuracy as SGD and, hence, retain the merit of deep learning with
   respect to accuracy. Finally, we conduct several experiments using
   benchmark datasets and show that our systems outperform the previous
   system in terms of learning accuracies.}},
DOI = {{10.1109/TIFS.2019.2911169}},
ISSN = {{1556-6013}},
EISSN = {{1556-6021}},
ORCID-Numbers = {{Phuong, Tran/0000-0002-0383-8891}},
Unique-ID = {{ISI:000474549100005}},
}

@article{ ISI:000469963100020,
Author = {Khan, Rafiullah and Islam, Muhammad Arshad and Ullah, Mohib and Aleem,
   Muhammad and Iqbal, Muhammad Azhar},
Title = {{Privacy Exposure Measure: A Privacy-Preserving Technique for
   Health-Related Web Search}},
Journal = {{JOURNAL OF MEDICAL IMAGING AND HEALTH INFORMATICS}},
Year = {{2019}},
Volume = {{9}},
Number = {{6}},
Pages = {{1196-1204}},
Month = {{AUG}},
Abstract = {{The increasing use of web search engines (WSEs) for searching healthcare
   information has resulted in a growing number of users posting personal
   health information online. A recent survey demonstrates that over 80\%
   of patients use WSE to seek health information. However, WSE stores
   these user's queries to analyze user behavior, result ranking,
   personalization, targeted advertisements, and other activities. Since
   health-related queries contain privacy-sensitive information that may
   infringe users privacy. Therefore, privacy-preserving web search
   techniques such as anonymizing networks, profile obfuscation, private
   information retrieval (PIR) protocols etc. are used to ensure the users
   privacy. In this paper, we propose Privacy Exposure Measure (PEM), a
   technique that facilitates user to control his/her privacy exposure
   while using the PIR protocols. PEM assesses the similarity between the
   users profile and query before posting to WSE and assists the user in
   avoiding privacy exposure. The experiments demonstrate 37.2\% difference
   between users' profile created through PEM-powered-PIR protocol and
   other usual users' profile. Moreover, PEM offers more privacy to the
   user even in case of machine-learning attack.}},
DOI = {{10.1166/jmihi.2019.2709}},
ISSN = {{2156-7018}},
EISSN = {{2156-7026}},
Unique-ID = {{ISI:000469963100020}},
}

@article{ ISI:000475944400004,
Author = {Beaulieu-Jones, Brett K. and Wu, Zhiwei Steven and Williams, Chris and
   Lee, Ran and Bhavnani, Sanjeev P. and Byrd, James Brian and Greene,
   Casey S.},
Title = {{Privacy-Preserving Generative Deep Neural Networks Support Clinical Data
   Sharing}},
Journal = {{CIRCULATION-CARDIOVASCULAR QUALITY AND OUTCOMES}},
Year = {{2019}},
Volume = {{12}},
Number = {{7}},
Month = {{JUL}},
Abstract = {{Background: Data sharing accelerates scientific progress but sharing
   individual-level data while preserving patient privacy presents a
   barrier. Methods and Results: Using pairs of deep neural networks, we
   generated simulated, synthetic participants that closely resemble
   participants of the SPRINT trial (Systolic Blood Pressure Trial). We
   showed that such paired networks can be trained with differential
   privacy, a formal privacy framework that limits the likelihood that
   queries of the synthetic participants' data could identify a real a
   participant in the trial. Machine learning predictors built on the
   synthetic population generalize to the original data set. This finding
   suggests that the synthetic data can be shared with others, enabling
   them to perform hypothesis-generating analyses as though they had the
   original trial data. Conclusions: Deep neural networks that generate
   synthetic participants facilitate secondary analyses and reproducible
   investigation of clinical data sets by enhancing data sharing while
   preserving participant privacy.}},
DOI = {{10.1161/CIRCOUTCOMES.118.005122}},
ISSN = {{1941-7705}},
EISSN = {{1941-7713}},
ResearcherID-Numbers = {{Greene, Casey/L-2057-2015}},
ORCID-Numbers = {{Greene, Casey/0000-0001-8713-9213}},
Unique-ID = {{ISI:000475944400004}},
}

@article{ ISI:000470946600015,
Author = {Ashburner, John and Brudfors, Mikael and Bronik, Kevin and Balbastre,
   Yael},
Title = {{An algorithm for learning shape and appearance models without
   annotations}},
Journal = {{MEDICAL IMAGE ANALYSIS}},
Year = {{2019}},
Volume = {{55}},
Pages = {{197-215}},
Month = {{JUL}},
Abstract = {{This paper presents a framework for automatically learning shape and
   appearance models for medical (and certain other) images. The algorithm
   was developed with the aim of eventually enabling distributed
   privacy-preserving analysis of brain image data, such that shared
   information (shape and appearance basis functions) may be passed across
   sites, whereas latent variables that encode individual images remain
   secure within each site. These latent variables are proposed as features
   for privacy-preserving data mining applications.
   The approach is demonstrated qualitatively on the KDEF dataset of 2D
   face images, showing that it can align images that traditionally require
   shape and appearance models trained using manually annotated data
   (manually defined landmarks etc.). It is applied to the MNIST dataset of
   handwritten digits to show its potential for machine learning
   applications, particularly when training data is limited. The model is
   able to handle ``missing data{''}, which allows it to be cross-validated
   according to how well it can predict left-out voxels. The suitability of
   the derived features for classifying individuals into patient groups was
   assessed by applying it to a dataset of over 1900 segmented T1-weighted
   MR images, which included images from the COBRE and ABIDE datasets. (C)
   2019 Wellcome Centre for Human Neuroimaging. Published by Elsevier B.V.}},
DOI = {{10.1016/j.media.2019.04.008}},
ISSN = {{1361-8415}},
EISSN = {{1361-8423}},
ORCID-Numbers = {{Bronik, Kevin/0000-0002-3032-8192}},
Unique-ID = {{ISI:000470946600015}},
}

@article{ ISI:000473762500085,
Author = {Ukil, Arijit and Jara, Antonio J. and Marin, Leandro},
Title = {{Data-Driven Automated Cardiac Health Management with Robust Edge
   Analytics and De-Risking}},
Journal = {{SENSORS}},
Year = {{2019}},
Volume = {{19}},
Number = {{12}},
Month = {{JUN 2}},
Abstract = {{Remote and automated healthcare management has shown the prospective to
   significantly impact the future of human prognosis rate. Internet of
   Things (IoT) enables the development and implementation ecosystem to
   cater the need of large number of relevant stakeholders. In this paper,
   we consider the cardiac health management system to demonstrate that
   data-driven techniques produce substantial performance merits in terms
   of clinical efficacy by employing robust machine learning methods with
   relevant and selected signal processing features. We consider
   phonocardiogram (PCG) or heart sound as the exemplary physiological
   signal. PCG carries substantial cardiac health signature to establish
   our claim of data-centric superior clinical utility. Our method
   demonstrates close to 85\% accuracy on publicly available MIT-Physionet
   PCG datasets and outperform relevant state-of-the-art algorithm. Due to
   its simpler computational architecture of shallow classifier with just
   three features, the proposed analytics method is performed at edge
   gateway. However, it is to be noted that healthcare analytics deal with
   number of sensitive data and subsequent inferences, which need privacy
   protection. Additionally, the problem of healthcare data privacy
   prevention is addressed by de-risking of sensitive data management using
   differential privacy, such that controlled privacy protection on
   sensitive healthcare data can be enabled. When a user sets for privacy
   protection, appropriate privacy preservation is guaranteed for defense
   against privacy-breaching knowledge mining attacks. In this era of IoT
   and machine intelligence, this work is of practical importance, which
   enables on-demand automated screening of cardiac health under minimizing
   the privacy breaching risk.}},
DOI = {{10.3390/s19122733}},
Article-Number = {{2733}},
ISSN = {{1424-8220}},
ResearcherID-Numbers = {{Marin, Leandro/L-2116-2014}},
ORCID-Numbers = {{Marin, Leandro/0000-0002-4258-1187}},
Unique-ID = {{ISI:000473762500085}},
}

@article{ ISI:000472596200153,
Author = {Ma, Zhuo and Liu, Yang and Liu, Ximeng and Ma, Jianfeng and Ren, Kui},
Title = {{Lightweight Privacy-Preserving Ensemble Classification for Face
   Recognition}},
Journal = {{IEEE INTERNET OF THINGS JOURNAL}},
Year = {{2019}},
Volume = {{6}},
Number = {{3}},
Pages = {{5778-5790}},
Month = {{JUN}},
Abstract = {{The development of machine learning technology and visual sensors is
   promoting the wider applications of face recognition into our daily
   life. However, if the face features in the servers are abused by the
   adversary, our privacy and wealth can be faced with great threat. Many
   security experts have pointed out that, by 3-D-printing technology, the
   adversary can utilize the leaked face feature data to masquerade others
   and break the E-bank accounts. Therefore, in this paper, we propose a
   lightweight privacy-preserving adaptive boosting (AdaBoost)
   classification framework for face recognition (POR) based on the
   additive secret sharing and edge computing. First, we improve the
   current additive secret sharing-based exponentiation and logarithm
   functions by expanding the effective input range. Then, by utilizing the
   protocols, two edge servers are deployed to cooperatively complete the
   ensemble classification of AdaBoost for face recognition. The
   application of edge computing ensures the efficiency and robustness of
   POR. Furthermore, we prove the correctness and security of our protocols
   by theoretic analysis. And experiment results show that, POR can reduce
   about 58\% computation error compared with the existing differential
   privacy-based framework.}},
DOI = {{10.1109/JIOT.2019.2905555}},
ISSN = {{2327-4662}},
ORCID-Numbers = {{Liu, Ximeng/0000-0002-4238-3295}},
Unique-ID = {{ISI:000472596200153}},
}

@article{ ISI:000465367100013,
Author = {Senavirathne, Navoda and Torra, Vicenc},
Title = {{Integrally private model selection for decision trees}},
Journal = {{COMPUTERS \& SECURITY}},
Year = {{2019}},
Volume = {{83}},
Pages = {{167-181}},
Month = {{JUN}},
Abstract = {{Privacy attacks targeting machine learning models are evolving. One of
   the primary goals of such attacks is to infer information about the
   training data used to construct the models. ``Integral Privacy{''}
   focuses on machine learning and statistical models which explain how we
   can utilize intruder's uncertainty to provide a privacy guarantee
   against model comparison attacks.
   Through experimental results, we show how the distribution of models can
   be used to achieve integral privacy. Here, we observe two categories of
   machine learning models based on their frequency of occurrence in the
   model space. Then we explain the privacy implications of selecting each
   of them based on a new attack model and empirical results. Also, we
   provide recommendations for private model selection based on the
   accuracy and stability of the models along with the diversity of
   training data that can be used to generate the models. (C) 2019 The
   Author. Published by Elsevier Ltd.}},
DOI = {{10.1016/j.cose.2019.01.006}},
ISSN = {{0167-4048}},
EISSN = {{1872-6208}},
Unique-ID = {{ISI:000465367100013}},
}

@article{ ISI:000469323000052,
Author = {Ma, Hui and Guo, Xuyang and Ping, Yuan and Wang, Baocang and Yang,
   Yuehua and Zhang, Zhili and Zhou, Jingxian},
Title = {{PPCD: Privacy-preserving clinical decision with cloud support}},
Journal = {{PLOS ONE}},
Year = {{2019}},
Volume = {{14}},
Number = {{5}},
Month = {{MAY 29}},
Abstract = {{With the prosperity of machine learning and cloud computing, meaningful
   information can be mined from mass electronic medical data which help
   physicians make proper disease diagnosis for patients. However, using
   medical data and disease information of patients frequently raise
   privacy concerns. In this paper, based on single-layer perceptron, we
   propose a scheme of privacy-preserving clinical decision with cloud
   support (PPCD), which securely conducts disease model training and
   prediction for the patient. Each party learns nothing about the other's
   private information. In PPCD, a lightweight secure multiplication is
   presented and introduced to improve the model training. Security
   analysis and experimental results on real data confirm the high accuracy
   of disease prediction achieved by the proposed PPCD without the risk of
   privacy disclosure.}},
DOI = {{10.1371/journal.pone.0217349}},
Article-Number = {{e0217349}},
ISSN = {{1932-6203}},
Unique-ID = {{ISI:000469323000052}},
}

@article{ ISI:000470023300011,
Author = {Yu, Yong and Li, Huilin and Chen, Ruonan and Zhao, Yanqi and Yang,
   Haomiao and Du, Xiaojiang},
Title = {{Enabling Secure Intelligent Network with Cloud-Assisted
   Privacy-Preserving Machine Learning}},
Journal = {{IEEE NETWORK}},
Year = {{2019}},
Volume = {{33}},
Number = {{3}},
Pages = {{82-87}},
Month = {{MAY-JUN}},
Abstract = {{Intelligent networks are regarded as existing networks incorporating
   some intelligent mechanisms such as cognitive and cooperative approaches
   to improve network performance. Security is highly essential in
   intelligent networks but has received less attention so far. In this
   article, we propose a framework that enables a secure intelligent
   network with the assistance of cloud-assisted privacy-preserving machine
   learning. In the framework, the cloud server can first generate a model
   using outsourced machine learning algorithms and then process testing
   data from the network with the generated model in real time, which
   reflects to the network and makes it more intelligent. At the same time,
   the proposal guarantees the security and privacy of both the training
   data and the testing data in the sense that the proposed framework takes
   advantage of differential privacy to perform privacy-preserving data
   analysis and homomorphic encryption to conduct valid operations over
   encrypted data. The performance evaluations of the core primitives
   employed in the framework including differential privacy and homomorphic
   encryption algorithms demonstrate the practicability of our proposal.}},
DOI = {{10.1109/MNET.2019.1800362}},
ISSN = {{0890-8044}},
EISSN = {{1558-156X}},
Unique-ID = {{ISI:000470023300011}},
}

@article{ ISI:000465119800003,
Author = {Kuo, Tsung-Ting and Gabriel, Rodney A. and Ohno-Machado, Lucila},
Title = {{Fair compute loads enabled by blockchain: sharing models by alternating
   client and server roles}},
Journal = {{JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION}},
Year = {{2019}},
Volume = {{26}},
Number = {{5}},
Pages = {{392-403}},
Month = {{MAY}},
Abstract = {{Objective: Decentralized privacy-preserving predictive modeling enables
   multiple institutions to learn a more generalizable model on healthcare
   or genomic data by sharing the partially trained models instead of
   patient-level data, while avoiding risks such as single point of
   control. State-of-the-art blockchain-based methods remove the server
   role but can be less accurate than models that rely on a server.
   Therefore, we aim at developing a general model sharing framework to
   preserve predictive correctness, mitigate the risks of a centralized
   architecture, and compute the models in a fair way. Materials and
   Methods: We propose a framework that includes both server and client
   roles to preserve correctness. We adopt a blockchain network to obtain
   the benefits of decentralization, by alternating the roles for each site
   to ensure computational fairness. Also, we developed GloreChain (Grid
   Binary LOgistic REgression on Permissioned BlockChain) as a concrete
   example, and compared it to a centralized algorithm on 3 healthcare or
   genomic datasets to evaluate predictive correctness, number of learning
   iterations and execution time. Results: GloreChain performs exactly the
   same as the centralized method in terms of correctness and number of
   iterations. It inherits the advantages of blockchain, at the cost of
   increased time to reach a consensus model. Discussion: Our framework is
   general or flexible and can also address intrinsic challenges of
   blockchain networks. Further investigations will focus on
   higher-dimensional datasets, additional use cases, privacy-preserving
   quality concerns, and ethical, legal, and social implications.
   Conclusions: Our framework provides a promising potential for
   institutions to learn a predictive model based on healthcare or genomic
   data in a privacy-preserving and decentralized way.}},
DOI = {{10.1093/jamia/ocy180}},
ISSN = {{1067-5027}},
EISSN = {{1527-974X}},
Unique-ID = {{ISI:000465119800003}},
}

@article{ ISI:000461531700014,
Author = {Dou, Wanchun and Tang, Wenda and Li, Shu and Yu, Shui and Choo,
   Kim-Kwang Raymond},
Title = {{A heuristic line piloting method to disclose malicious taxicab driver's
   privacy over GPS big data}},
Journal = {{INFORMATION SCIENCES}},
Year = {{2019}},
Volume = {{483}},
Pages = {{247-261}},
Month = {{MAY}},
Abstract = {{While privacy preservation is important, there are occasions when an
   individual's privacy should not be preserved (e.g., those involved in
   the case of a terrorist attack). Existing works do not generally make
   such a distinction. We posit the importance of classifying an
   individual's privacy as positive and negative, say in the case of a
   misbehaving driver (e.g., a driver involved in a hit-and-run or
   terrorist attack). This will allow us to revoke the right of the
   misbehaving driver's right to privacy to facilitate investigation.
   Hence, we propose a heuristic line piloting method, hereafter referred
   to as HelpMe. Using taxi services as a case study, we explain how the
   proposed method constantly accumulates the knowledge of taxi routes from
   related historical GPS datasets using machine-learning techniques.
   Hence, a taxi deviating from the typical route could be detected in
   real-time, which may be used to raise an alert (e.g., the taxi may be
   hijacked by criminals). We also evaluate the utility of our method on
   real-life GPS datasets. (C) 2018 Published by Elsevier Inc.}},
DOI = {{10.1016/j.ins.2018.12.056}},
ISSN = {{0020-0255}},
EISSN = {{1872-6291}},
ResearcherID-Numbers = {{Choo, Kim-Kwang Raymond/A-3634-2009}},
ORCID-Numbers = {{Choo, Kim-Kwang Raymond/0000-0001-9208-5336}},
Unique-ID = {{ISI:000461531700014}},
}

@article{ ISI:000459846300032,
Author = {Ma, Xu and Chen, Xiaofeng and Zhang, Xiaoyu},
Title = {{Non-interactive privacy-preserving neural network prediction}},
Journal = {{INFORMATION SCIENCES}},
Year = {{2019}},
Volume = {{481}},
Pages = {{507-519}},
Month = {{MAY}},
Abstract = {{Neural network is a particular machine learning framework which has
   gained widespread popularity due to its superior performance in many
   applications, such as complex board games, face recognition and disease
   diagnosis. A new service paradigm which offers online
   neural-network-based prediction to clients is increasingly popular.
   Although the prediction service has clear benefits, serious privacy
   issues have also emerged from the clients' sensitive data and the neural
   network model itself. In this paper, we present a new outsourcing model
   for privacy-preserving neural network prediction under two non-colluding
   servers framework. In this model, the original neural network owner can
   securely outsource an existing neural network model to the two servers
   who will thereafter provide prediction service to the public users
   utilize encrypted input data. We propose the first fully non interactive
   privacy-preserving neural network prediction scheme. Extensive security
   and efficiency analysis demonstrate that the proposed scheme satisfies
   the security requirement of model privacy and data privacy, and highly
   efficient with respect to computation and communication overhead. (C)
   2019 Elsevier Inc. All rights reserved.}},
DOI = {{10.1016/j.ins.2018.12.015}},
ISSN = {{0020-0255}},
EISSN = {{1872-6291}},
Unique-ID = {{ISI:000459846300032}},
}

@article{ ISI:000467751100042,
Author = {Tang, Fengyi and Wu, Wei and Liu, Jian and Wang, Huimei and Xian, Ming},
Title = {{Privacy-Preserving Distributed Deep Learning via Homomorphic
   Re-Encryption}},
Journal = {{ELECTRONICS}},
Year = {{2019}},
Volume = {{8}},
Number = {{4}},
Month = {{APR}},
Abstract = {{The flourishing deep learning on distributed training datasets arouses
   worry about data privacy. The recent work related to privacy-preserving
   distributed deep learning is based on the assumption that the server and
   any learning participant do not collude. Once they collude, the server
   could decrypt and get data of all learning participants. Moreover, since
   the private keys of all learning participants are the same, a learning
   participant must connect to the server via a distinct TLS/SSL secure
   channel to avoid leaking data to other learning participants. To fix
   these problems, we propose a privacy-preserving distributed deep
   learning scheme with the following improvements: (1) no information is
   leaked to the server even if any learning participant colludes with the
   server; (2) learning participants do not need different secure channels
   to communicate with the server; and (3) the deep learning model accuracy
   is higher. We achieve them by introducing a key transform server and
   using homomorphic re-encryption in asynchronous stochastic gradient
   descent applied to deep learning. We show that our scheme adds tolerable
   communication cost to the deep learning system, but achieves more
   security properties. The computational cost of learning participants is
   similar. Overall, our scheme is a more secure and more accurate deep
   learning scheme for distributed learning participants.}},
DOI = {{10.3390/electronics8040411}},
Article-Number = {{411}},
ISSN = {{2079-9292}},
Unique-ID = {{ISI:000467751100042}},
}

@article{ ISI:000467564700067,
Author = {Shen, Meng and Ma, Baoli and Zhu, Liehuang and Du, Xiaojiang and Xu, Ke},
Title = {{Secure Phrase Search for Intelligent Processing of Encrypted Data in
   Cloud-Based IoT}},
Journal = {{IEEE INTERNET OF THINGS JOURNAL}},
Year = {{2019}},
Volume = {{6}},
Number = {{2}},
Pages = {{1998-2008}},
Month = {{APR}},
Abstract = {{Phrase search allows retrieval of documents containing an exact phrase,
   which plays an important role in many machine learning applications for
   cloud-based Internet of Things (IoT), such as intelligent medical data
   analytics. In order to protect sensitive information from being leaked
   by service providers, documents (e.g., clinic records) are usually
   encrypted by data owners before being outsourced to the cloud. This,
   however, makes the search operation an extremely challenging task.
   Existing searchable encryption schemes for multikeyword search
   operations fail to perform phrase search, as they are unable to
   determine the location relationship of multiple keywords in a queried
   phrase over encrypted data on the cloud server side. In this paper, we
   propose P3, an efficient privacy-preserving phrase search scheme for
   intelligent encrypted data processing in cloud-based IoT. Our scheme
   exploits the homomorphic encryption and bilinear map to determine the
   location relationship of multiple queried keywords over encrypted data.
   It also utilizes a probabilistic trapdoor generation algorithm to
   protect users' search patterns. Thorough security analysis demonstrates
   the security guarantees achieved by P3. We implement a prototype and
   conduct extensive experiments on real-world datasets. The evaluation
   results show that compared with existing multikeyword search schemes, P3
   can greatly improve the search accuracy with moderate overheads.}},
DOI = {{10.1109/JIOT.2018.2871607}},
ISSN = {{2327-4662}},
Unique-ID = {{ISI:000467564700067}},
}

@article{ ISI:000458229200023,
Author = {Yu, Xixun and Yan, Zheng and Zhang, Rui},
Title = {{Verifiable outsourced computation over encrypted data}},
Journal = {{INFORMATION SCIENCES}},
Year = {{2019}},
Volume = {{479}},
Pages = {{372-385}},
Month = {{APR}},
Abstract = {{In recent years, cloud computing has become the most popular and
   promising service platform. A cloud user can outsource its heavy
   computation overhead to a cloud service provider (CSP) and let the CSP
   make the computation instead. In order to guarantee the correctness of
   the outsourced processing (e.g., machine learning and data mining), a
   proof should be provided by the CSP in order to make sure that the
   processing is carried out properly. On the other hand, from the security
   and privacy points of view, users will always encrypt their sensitive
   data first before they are outsourced to the CSP rather than sending the
   raw data directly. However, processing and verifying of encrypted data
   computation has always been a challenging problem. Homomorphic
   Encryption (HE) has been proposed to tackle this task on computations
   over encrypted data and ensure the confidentiality of the data. However,
   original HE cannot provide an efficient approach to verify the
   correctness of computation over encrypted data that is processed by CSP.
   In this paper, we propose a verifiable outsourced computation scheme
   over encrypted data with the help of fully homomorphic encryption and
   polynomial factorization algorithm. Our scheme protects user data
   security in outsourced processing and allows public verification on the
   computation result processed by CSP with zero knowledge. We then prove
   the security of our scheme and analyze its performance by comparing it
   with some latest related works. Performances analysis shows that our
   scheme reduces the overload of both the cloud users and the verifier.
   (C) 2018 Elsevier Inc. All rights reserved.}},
DOI = {{10.1016/j.ins.2018.12.022}},
ISSN = {{0020-0255}},
EISSN = {{1872-6291}},
ORCID-Numbers = {{Yan, Zheng/0000-0002-9697-2108}},
Unique-ID = {{ISI:000458229200023}},
}

@article{ ISI:000457666500026,
Author = {Zhao, Yanqi and Yu, Yong and Li, Yannan and Han, Gang and Du, Xiaojiang},
Title = {{Machine learning based privacy-preserving fair data trading in big data
   market}},
Journal = {{INFORMATION SCIENCES}},
Year = {{2019}},
Volume = {{478}},
Pages = {{449-460}},
Month = {{APR}},
Abstract = {{In the era of big data, the produced and collected data explode due to
   the emerging technologies and applications that pervade everywhere in
   our daily lives, including internet of things applications such as smart
   home, smart city, smart grid, e-commerce applications and social
   network. Big data market can carry out efficient data trading, which
   provides a way to share data and further enhances the utility of data.
   However, to realize effective data trading in big data market, several
   challenges need to be resolved. The first one is to verify the data
   availability for a data consumer. The second is privacy of a data
   provider who is unwilling to reveal his real identity to the data
   consumer. The third is the payment fairness between a data provider and
   a data consumer with atomic exchange. In this paper, we address these
   challenges by proposing a new blockchain-based fair data trading
   protocol in big data market. The proposed protocol integrates ring
   signature, double authentication-preventing signature and similarity
   learning to guarantee the availability of trading data, privacy of data
   providers and fairness between data providers and data consumers. We
   show the proposed protocol achieves the desirable security properties
   that a secure data trading protocol should have. The implementation
   results with Solidity smart contract demonstrate the validity of the
   proposed blockchain-based fair data trading protocol. (C) 2018 Elsevier
   Inc. All rights reserved.}},
DOI = {{10.1016/j.ins.2018.11.028}},
ISSN = {{0020-0255}},
EISSN = {{1872-6291}},
ResearcherID-Numbers = {{Li, Wang/M-1612-2019}},
Unique-ID = {{ISI:000457666500026}},
}

@article{ ISI:000449082400007,
Author = {Yang, Qing and Peng, Ge and Gasti, Paolo and Balagani, Kiran S. and Li,
   Yantao and Zhou, Gang},
Title = {{MEG: Memory and Energy Efficient Garbled Circuit Evaluation on
   Smartphones}},
Journal = {{IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY}},
Year = {{2019}},
Volume = {{14}},
Number = {{4}},
Pages = {{913-922}},
Month = {{APR}},
Abstract = {{Garbled circuits are general tools that allow two parties to compute any
   function without disclosing their respective inputs. Applications of
   this technique vary from distributed privacy-preserving machine learning
   tasks to secure outsourced authentication. Unfortunately, the energy
   cost of garbled circuit evaluation protocols is substantial. This limits
   the applicability of garbled circuits in scenarios that involve
   battery-operated devices, such as Internet-of-Things (IoT) devices and
   smartphones. In this paper, we propose MEG, a Memory-and Energyefficient
   Garbled circuit evaluation mechanism. MEG utilizes batch data
   transmission and multi-threading to reduce memory and energy
   consumption. We implement MEG on an Android smartphone and compare its
   performance and energy consumption with state-of-the-art techniques
   using two garbled circuits of widely different sizes (AES-128 and
   256-bit edit distance). Our results show that, compared with ``
   plain{''} garbled circuit evaluation, MEG decreases memory consumption
   by more than 90\%. When compared with current pipelined garbled circuit
   evaluation techniques, MEG's energy usage was 42\% lower for AES-128 and
   23\% lower for EDT-256. Furthermore, our multi-thread implementation of
   MEG decreased circuit evaluation time by up to 56.7\% for AES-128, and
   by up to 13.5\% for EDT-256, compared with state-of-the-art pipelining
   techniques.}},
DOI = {{10.1109/TIFS.2018.2868221}},
ISSN = {{1556-6013}},
EISSN = {{1556-6021}},
ORCID-Numbers = {{Zhou, Gang/0000-0002-4425-9837}},
Unique-ID = {{ISI:000449082400007}},
}

@article{ ISI:000466777700001,
Author = {Singh, Siddharth},
Title = {{Big Dreams With Big Data! Use of Clinical Informatics to Inform
   Biomarker Discovery}},
Journal = {{CLINICAL AND TRANSLATIONAL GASTROENTEROLOGY}},
Year = {{2019}},
Volume = {{10}},
Month = {{MAR 21}},
Abstract = {{As the complexity of biomedical data increases, so do the opportunities
   to leverage them to advance science and clinical care. Electronic health
   records form a rich but complex source of large amounts of data gathered
   during routine clinical care. Through the use of codified and free-text
   concepts identified using clinical informatics tools such as natural
   language processing, disease phenotyping can be performed with a high
   degree of accuracy. Technologies such as genome sequencing, gene
   expression profiling, proteomic and metabolomic analyses, and electronic
   devices and wearables are generating large amounts of data from various
   populations, cell types, and disorders (big data). However, to make
   these data useable for the next step of biomarker discovery, precision
   medicine, and clinical practice, it is imperative to harmonize and
   integrate these diverse data sources. In this article, we introduce
   important building blocks for precision medicine, including common data
   models, text mining and natural language processing, privacy-preserved
   record linkage, machine learning for predictive modeling, and health
   information exchange.}},
DOI = {{10.14309/ctg.0000000000000018}},
Article-Number = {{UNSP e-00018}},
ISSN = {{2155-384X}},
Unique-ID = {{ISI:000466777700001}},
}

@article{ ISI:000463065700006,
Author = {Al-Rubaie, Mohammad and Chang, J. Morris},
Title = {{Privacy-Preserving Machine Learning: Threats and Solutions}},
Journal = {{IEEE SECURITY \& PRIVACY}},
Year = {{2019}},
Volume = {{17}},
Number = {{2}},
Pages = {{49-58}},
Month = {{MAR-APR}},
Abstract = {{For privacy concerns to be addressed adequately in today's
   machine-learning (ML) systems, the knowledge gap between the ML and
   privacy communities must be bridged. This article aims to provide an
   introduction to the intersection of both fields with special emphasis on
   the techniques used to protect the data.}},
DOI = {{10.1109/MSEC.2018.2888775}},
ISSN = {{1540-7993}},
EISSN = {{1558-4046}},
ResearcherID-Numbers = {{Chang, Jien/C-7672-2019}},
ORCID-Numbers = {{Chang, Jien/0000-0002-0660-7191}},
Unique-ID = {{ISI:000463065700006}},
}

@article{ ISI:000461357500003,
Author = {De Cock, Martine and Dowsley, Rafael and Horst, Caleb and Katti, Raj and
   Nascimento, Anderson C. A. and Poon, Wing-Sea and Truex, Stacey},
Title = {{Efficient and Private Scoring of Decision Trees, Support Vector Machines
   and Logistic Regression Models Based on Pre-Computation}},
Journal = {{IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING}},
Year = {{2019}},
Volume = {{16}},
Number = {{2}},
Pages = {{217-230}},
Month = {{MAR-APR}},
Abstract = {{Many data-driven personalized services require that private data of
   users is scored against a trained machine learning model. In this paper
   we propose a novel protocol for privacy-preserving classification of
   decision trees, a popular machine learning model in these scenarios. Our
   solutions is composed out of building blocks, namely a secure comparison
   protocol, a protocol for obliviously selecting inputs, and a protocol
   for multiplication. By combining some of the building blocks for our
   decision tree classification protocol, we also improve previously
   proposed solutions for classification of support vector machines and
   logistic regression models. Our protocols are information theoretically
   secure and, unlike previously proposed solutions, do not require modular
   exponentiations. We show that our protocols for privacy-preserving
   classification lead to more efficient results from the point of view of
   computational and communication complexities. We present accuracy and
   runtime results for seven classification benchmark datasets from the UCI
   repository.}},
DOI = {{10.1109/TDSC.2017.2679189}},
ISSN = {{1545-5971}},
EISSN = {{1941-0018}},
ResearcherID-Numbers = {{Poon, Wai Sang/F-1558-2011}},
Unique-ID = {{ISI:000461357500003}},
}

@inproceedings{ ISI:000474419500107,
Author = {Chew, Yee Jian and Ooi, Shih Yin and Wong, Kok-Seng and Pang, Ying Han},
Book-Group-Author = {{ACM}},
Title = {{Privacy Preserving of IP Address through Truncation Method in
   Network-based Intrusion Detection System}},
Booktitle = {{2019 8TH INTERNATIONAL CONFERENCE ON SOFTWARE AND COMPUTER APPLICATIONS
   (ICSCA 2019)}},
Year = {{2019}},
Pages = {{569-573}},
Note = {{8th International Conference on Software and Computer Applications
   (ICSCA), Penang, MALAYSIA, FEB 19-21, 2019}},
Organization = {{Univ Malaysia Pahang}},
Abstract = {{Network-based Intrusion Detection System (IDS) is gaining wide attention
   from the research community since the past decades. While having a
   precise classification model in separating the normal and malicious
   network traffics is still remain as the ultimate goal, the privacy
   protection for network traffic database cannot be ignore as well. The
   impetuous ignorance of database privacy will continue to restrain
   governments, organisations and individuals in releasing the real and
   ontological network traces. The common solution to tackle this matter is
   anonymising the database through the statistical approach. Anonymising
   can be referred to masking, hiding or removing certain sensitive
   information from the database. Thus, this will be subsequently resulting
   in information loss. In this paper, a truncation method is explored to
   preserve the sensitive information of the network traffic database (i.e.
   IP addresses). The truncated database is then tested with 10 machine
   learning classifiers from Weka. We tested four different options of IP
   address truncation against the 6 percent of GureKDDCup dataset.}},
DOI = {{10.1145/3316615.3316626}},
ISBN = {{978-1-4503-6573-4}},
Unique-ID = {{ISI:000474419500107}},
}

@article{ ISI:000475878800001,
Author = {Kim, Taehoon and Yang, Jihoon},
Title = {{Latent-Space-Level Image Anonymization With Adversarial Protector
   Networks}},
Journal = {{IEEE ACCESS}},
Year = {{2019}},
Volume = {{7}},
Pages = {{84992-84999}},
Abstract = {{Along with recent achievements in deep learning empowered by enormous
   amounts of training data, preserving the privacy of an individual
   related to the gathered data has been becoming an essential part of the
   public data collection and publication. Advancements in deep learning
   threaten traditional image anonymization techniques with model inversion
   attacks that try to reconstruct the original image from the anonymized
   image. In this paper, we propose a privacy-preserving adversarial
   protector network (PPAPNet) as an image anonymization tool to convert an
   image into another synthetic image that is both realistic and immune to
   model inversion attacks. Our experiments on various datasets show that
   PPAPNet can effectively convert a sensitive image into a high-quality
   and attack-immune synthetic image.}},
DOI = {{10.1109/ACCESS.2019.2924479}},
ISSN = {{2169-3536}},
ORCID-Numbers = {{Kim, Taehoon/0000-0003-4788-4651}},
Unique-ID = {{ISI:000475878800001}},
}

@inproceedings{ ISI:000474353100079,
Author = {Domingo-Ferrer, Josep and Perez-Sola, Cristina and Blanco-Justicia,
   Alberto},
Book-Group-Author = {{ACM}},
Title = {{Collaborative Explanation of Deep Models with Limited Interaction for
   Trade Secret and Privacy Preservation}},
Booktitle = {{COMPANION OF THE WORLD WIDE WEB CONFERENCE (WWW 2019 )}},
Year = {{2019}},
Pages = {{501-507}},
Note = {{World Wide Web Conference (WWW), San Francisco, CA, MAY 13-17, 2019}},
Organization = {{Assoc Comp Machinery; Microsoft; Amazon; Bloomberg; Google; Criteo AI
   Lab; CISCO; NTENT; Spotify; Yahoo Res; Wikimedia Fdn; Baidu; DiDi; eBay;
   Facebook; LinkedIn; Megagon Labs; Mix; Mozilla; Netflix Res; NE Univ;
   Pinterest; Quora; Visa Res; Walmart Labs; Airbnb; Letgo; Moore Fdn;
   Webcastor}},
Abstract = {{An ever increasing number of decisions affecting our lives are made by
   algorithms. For this reason, algorithmic transparency is becoming a
   pressing need: automated decisions should be explainable and unbiased. A
   straightforward solution is to make the decision algorithms open-source,
   so that everyone can verify them and reproduce their outcome. However,
   in many situations, the source code or the training data of algorithms
   cannot be published for industrial or intellectual property reasons, as
   they are the result of long and costly experience (e.g. this is
   typically the case in banking or insurance). We present an approach
   whereby individual subjects on whom automated decisions are made can
   elicit in a collaborative and privacy-preserving manner a rule-based
   approximation of the model underlying the decision algorithm, based on
   limited interaction with the algorithm or even only on how they have
   been classified. Furthermore, being rule-based, the approximation thus
   obtained can be used to detect potential discrimination. We present
   empirical work to demonstrate the practicality of our ideas.}},
ISBN = {{978-1-4503-6675-5}},
Unique-ID = {{ISI:000474353100079}},
}

@inproceedings{ ISI:000474353100216,
Author = {Kenthapadi, Krishnaram and Mironov, Ilya and Thakurta, Abhradeep Guha},
Book-Group-Author = {{ACM}},
Title = {{Privacy-preserving Data Mining in Industry}},
Booktitle = {{COMPANION OF THE WORLD WIDE WEB CONFERENCE (WWW 2019 )}},
Year = {{2019}},
Pages = {{1308-1310}},
Note = {{World Wide Web Conference (WWW), San Francisco, CA, MAY 13-17, 2019}},
Organization = {{Assoc Comp Machinery; Microsoft; Amazon; Bloomberg; Google; Criteo AI
   Lab; CISCO; NTENT; Spotify; Yahoo Res; Wikimedia Fdn; Baidu; DiDi; eBay;
   Facebook; LinkedIn; Megagon Labs; Mix; Mozilla; Netflix Res; NE Univ;
   Pinterest; Quora; Visa Res; Walmart Labs; Airbnb; Letgo; Moore Fdn;
   Webcastor}},
Abstract = {{Preserving privacy of users is a key requirement of web-scale data
   mining applications and systems such as web search, recommender systems,
   crowdsourced platforms, and analytics applications, and has witnessed a
   renewed focus in light of recent data breaches and new regulations such
   as GDPR. In this tutorial, we will first present an overview of privacy
   breaches over the last two decades and the lessons learned, key
   regulations and laws, and evolution of privacy techniques leading to
   differential privacy definition / techniques. Then, we will focus on the
   application of privacy-preserving data mining techniques in practice, by
   presenting case studies such as Apple's differential privacy deployment
   for iOS / macOS, Google's RAPPOR, LinkedIn Salary, and Microsoft's
   differential privacy deployment for collecting Windows telemetry. We
   will conclude with open problems and challenges for the data mining /
   machine learning community, based on our experiences in industry.}},
DOI = {{10.1145/3308560.3320085}},
ISBN = {{978-1-4503-6675-5}},
Unique-ID = {{ISI:000474353100216}},
}

@inproceedings{ ISI:000473801100007,
Author = {Jiang, Linshan and Tan, Rui and Lou, Xin and Lin, Guosheng},
Editor = {{Ramachandran, GS and Ortiz, J}},
Title = {{On Lightweight Privacy-Preserving Collaborative Learning for
   Internet-of-Things Objects}},
Booktitle = {{PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON INTERNET OF THINGS
   DESIGN AND IMPLEMENTATION (IOTDI `19)}},
Year = {{2019}},
Pages = {{70-81}},
Note = {{ACM/IEEE International Conference on Internet of Things Design and
   Implementation (IoTDI), Montreal, CANADA, APR 15-18, 2019}},
Organization = {{Assoc Comp Machinery; IEEE; ARM; IBM; IEEE Comp Soc; ACM SIGBED}},
Abstract = {{The Internet of Things (IoT) will be a main data generation
   infrastructure for achieving better system intelligence. This paper
   considers the design and implementation of a practical
   privacy-preserving collaborative learning scheme, inwhich a curious
   learning coordinator trains a better machine learning model based on the
   data samples contributed by a number of IoT objects, while the
   confidentiality of the raw forms of the training data is protected
   against the coordinator. Existing distributed machine learning and data
   encryption approaches incur significant computation and communication
   overhead, rendering themill-suited for resource-constrained IoT objects.
   We study an approach that applies independent Gaussian random projection
   at each IoT object to obfuscate data and trains a deep neural network at
   the coordinator based on the projected data from the IoT objects. This
   approach introduces light computation overhead to the IoT objects and
   moves most workload to the coordinator that can have suficient computing
   resources. Although the independent projections performed by the IoT
   objects address the potential collusion between the curious coordinator
   and some compromised IoT objects, they significantly increase the
   complexity of the projected data. In this paper, we leverage the
   superior learning capability of deep learning in capturing sophisticated
   patterns to maintain good learning performance. Extensive comparative
   evaluation shows that this approach outperforms other lightweight
   approaches that apply additive noisification for difierential privacy
   and/or support vector machines for learning in the applications with
   light data pattern complexities.}},
DOI = {{10.1145/3302505.3310070}},
ISBN = {{978-1-4503-6283-2}},
Unique-ID = {{ISI:000473801100007}},
}

@inproceedings{ ISI:000473544500002,
Author = {Ghemri, Lila},
Book-Group-Author = {{ACM}},
Title = {{Preserving Privacy in Data Analytics}},
Booktitle = {{PROCEEDINGS OF THE ACM INTERNATIONAL WORKSHOP ON SECURITY AND PRIVACY
   ANALYTICS (IWSPA `19)}},
Year = {{2019}},
Pages = {{3-4}},
Note = {{5th ACM International Workshop on Security and Privacy Analytics
   (IWSPA), Richardson, TX, MAR 27, 2019}},
Organization = {{Assoc Comp Machinery; ACM SIGSAC}},
Abstract = {{Data Analytics is becoming an essential business tool for many data
   intensive companies and organizations. However, the increased use of
   such methods comes with the threat of data disclosure.
   Privacy-preserving methods have been developed with varying degrees of
   efficiency with the main goal of protecting individuals' privacy. This
   tutorial aims at presenting models and techniques of preserving privacy
   in machine learning and data mining.}},
DOI = {{10.1145/3309182.3311786}},
ISBN = {{978-1-4503-6178-1}},
Unique-ID = {{ISI:000473544500002}},
}

@inproceedings{ ISI:000470891000009,
Author = {Hesamifard, Ehsan and Takabi, Hassan and Ghasemi, Mehdi},
Book-Group-Author = {{ACM}},
Title = {{Deep Neural Networks Classification over Encrypted Data}},
Booktitle = {{PROCEEDINGS OF THE NINTH ACM CONFERENCE ON DATA AND APPLICATION SECURITY
   AND PRIVACY (CODASPY `19)}},
Year = {{2019}},
Pages = {{97-108}},
Note = {{9th ACM Conference on Data and Application Security and Privacy
   (CODASPY), Richardson, TX, MAR 25-27, 2019}},
Organization = {{Assoc Comp Machinery; ACM SIGSAC; Univ Texas Dallas}},
Abstract = {{Deep Neural Networks (DNNs) have overtaken classic machine learning
   algorithms due to their superior performance in big data analysis in a
   broad range of applications. On the other hand, in recent years Machine
   Learning as a Service (MLaaS) has become more widespread in which a
   client uses cloud services for analyzing its data. However, the client's
   data may be sensitive which raises privacy concerns. In this paper, we
   address the issue of privacy preserving classification in a Machine
   Learning as a Service (MLaaS) settings and focus on convolutional neural
   networks (CNN). To achieve this goal, we develop new techniques to run
   CNNs over encrypted data. First, we design methods to approximate
   commonly used activation functions in CNNs (i.e. ReLU, Sigmoid, and
   Tanh) with low degree polynomials which is essential for a practical and
   efficient solution. Then, we train CNNs with approximation polynomials
   instead of original activation functions and implement CNNs
   classification over encrypted data. We evaluate the performance of our
   modified models at each step. The results of our experiments using
   several CNNs with a varying number of layers and structures are
   promising. When applied to the MNIST optical character recognition
   tasks, our approach achieved 99.25\% accuracy which significantly
   outperforms state-of-the-art solutions and is close to the accuracy of
   the best non-private version. Furthermore, it can make up to 164000
   predictions per hour. These results show that our approach provides
   accurate, efficient, and scalable privacy-preserving predictions in
   CNNs.}},
DOI = {{10.1145/3292006.3300044}},
ISBN = {{978-1-4503-6099-9}},
Unique-ID = {{ISI:000470891000009}},
}

@inproceedings{ ISI:000470891000011,
Author = {Celik, Z. Berkay and Acar, Abbas and Aksu, Hidayet and Atsley, Ryan She
   and McDaniel, Patrick and Uluagac, A. Selcuk},
Book-Group-Author = {{ACM}},
Title = {{CURIE: Policy-based Secure Data Exchange}},
Booktitle = {{PROCEEDINGS OF THE NINTH ACM CONFERENCE ON DATA AND APPLICATION SECURITY
   AND PRIVACY (CODASPY `19)}},
Year = {{2019}},
Pages = {{121-132}},
Note = {{9th ACM Conference on Data and Application Security and Privacy
   (CODASPY), Richardson, TX, MAR 25-27, 2019}},
Organization = {{Assoc Comp Machinery; ACM SIGSAC; Univ Texas Dallas}},
Abstract = {{Data sharing among partners-users, companies, organizations-is crucial
   for the advancement of collaborative machine learning in many domains
   such as healthcare, finance, and security. Sharing through secure
   computation and other means allow these partners to perform
   privacy-preserving computations on their private data in controlled
   ways. However, in reality, there exist complex relationships among
   members (partners). Politics, regulations, interest, trust, data demands
   and needs prevent members from sharing their complete data. Thus, there
   is a need for a mechanism to meet these conflicting relationships on
   data sharing. This paper presents CURIE1, an approach to exchange data
   among members who have complex relationships. A novel policy language,
   CPL, that allows members to define the specifications of data exchange
   requirements is introduced. With CPL, members can easily assert who and
   what to exchange through their local policies and negotiate a global
   sharing agreement. The agreement is implemented in a distributed
   privacy-preserving model that guarantees sharing among members will
   comply with the policy as negotiated. The use of CURIE is validated
   through an example healthcare application built on recently introduced
   secure multi-party computation and differential privacy frameworks, and
   policy and performance trade-offs are explored.}},
DOI = {{10.1145/3292006.3300042}},
ISBN = {{978-1-4503-6099-9}},
Unique-ID = {{ISI:000470891000011}},
}

@inproceedings{ ISI:000470922300036,
Author = {Fu, Yingwei and Wang, Huaimin and Xu, Kele and Mi, Haibo and Wang, Yijie},
Book-Group-Author = {{IEEE}},
Title = {{Mixup Based Privacy Preserving Mixed Collaboration Learning}},
Booktitle = {{2019 13TH IEEE INTERNATIONAL CONFERENCE ON SERVICE-ORIENTED SYSTEM
   ENGINEERING (SOSE) / 10TH INTERNATIONAL WORKSHOP ON JOINT CLOUD
   COMPUTING (JCC) / IEEE INTERNATIONAL WORKSHOP ON CLOUD COMPUTING IN
   ROBOTIC SYSTEMS (CCRS)}},
Year = {{2019}},
Pages = {{275-280}},
Note = {{13th IEEE International Conference on Service-Oriented System
   Engineering (SOSE) / 10th International Workshop on Joint Cloud
   Computing (JCC) / IEEE International Workshop on Cloud Computing in
   Robotic Systems (CCRS), San Francisco, CA, APR 04-09, 2019}},
Organization = {{IEEE; IEEE Comp Soc}},
Abstract = {{The amount of high-quality data determines the performance of the deep
   learning model. In reality, the data is often physically distributed in
   different organizations, and model averaging can train a deep model on
   the distributed data, while providing competitive performance compared
   with training a model on the centralized data. However, it cannot
   prevent inversion attack, as the intermediate parameters are transmitted
   during training. Some data enhancement methods, such as mixup, can
   effectively enhance the data privacy. In this paper, we propose a novel
   model averaging method combined with mixup, which provides protection
   against inversion attack. Besides we conduct experiments using
   state-of-the-art deep network architectures on multiple types of dataset
   to show that our method improves the classification accuracy of models.}},
DOI = {{10.1109/SOSE.2019.00047}},
ISBN = {{978-1-7281-1442-2}},
Unique-ID = {{ISI:000470922300036}},
}

@inproceedings{ ISI:000469483700130,
Author = {Khan, Ahmad Neyaz and Fan, Ming Yu and Malik, Asad and Memon, Raheel
   Ahmed},
Book-Group-Author = {{IEEE}},
Title = {{Learning from Privacy Preserved Encrypted Data on Cloud Through
   Supervised and Unsupervised Machine Learning}},
Booktitle = {{2019 2ND INTERNATIONAL CONFERENCE ON COMPUTING, MATHEMATICS AND
   ENGINEERING TECHNOLOGIES (ICOMET)}},
Year = {{2019}},
Note = {{2nd International Conference on Computing, Mathematics and Engineering
   Technologies (iCoMET), Sukkur, PAKISTAN, JAN 30-31, 2019}},
Abstract = {{With the advent of new technologies and the ever increasing use of Cloud
   in nearly every sphere of our day to day life, the data owner using
   cloud is still less confident. This lack of trust is obvious as the data
   owner entrusts the data with a third party which stores, manages and
   processes the data. Whatever be the level of security, there is always
   some loop hole for the misusers. Various machine learning techniques
   have been in use to learn from the data available for analysis and to
   use the results accordingly for benefits. Homomorphic secure multi-party
   computation (SMC) or homomorphic encryption (HE) encryption schemes have
   been one means to securely process the data on cloud while preserving
   the privacy of the data. In this work we have tried to investigate both
   supervised and unsupervised machine learning capability through neural
   networks over encrypted data from a semantically secure cryptosystem
   based on Homomorphic properties. This work will provide a base for the
   machine learning performance over the data on cloud whose privacy is
   claimed to be preserved using Homomorphic encryption. The findings are
   supported with experimental results.}},
ISBN = {{978-1-5386-9509-8}},
Unique-ID = {{ISI:000469483700130}},
}

@inproceedings{ ISI:000469462800116,
Author = {Badsha, Shahriar and Vakilinia, Iman and Sengupta, Shamik},
Editor = {{Chakrabarti, S and Saha, HN}},
Title = {{Privacy Preserving Cyber Threat Information Sharing and Learning for
   Cyber Defense}},
Booktitle = {{2019 IEEE 9TH ANNUAL COMPUTING AND COMMUNICATION WORKSHOP AND CONFERENCE
   (CCWC)}},
Year = {{2019}},
Pages = {{708-714}},
Note = {{9th IEEE Annual Computing and Communication Workshop and Conference
   (CCWC), Univ Nevada, Las Vegas, NV, JAN 07-09, 2019}},
Organization = {{IEEE; UNLV; IEEE Reg R1; IEEE Reg 6; IEEE USA; Inst Engn \& Management;
   Univ Engn \& Management}},
Abstract = {{To secure cyber infrastructure against intentional and potentially
   malicious threats, a growing collaborative effort between cybersecurity
   professionals and researchers from institutions, private industries,
   academia, and government agencies has engaged in exploiting and
   designing a variety of cyber defense systems. Cybersecurity researchers
   and designers aim to maintain the confidentiality, integrity, and
   availability of information and information management systems through
   various cyber defense systems that protect computers and networks from
   hackers who may want to steal financial, medical, or other
   identity-based information. The Cooperative Cyber-defense has been
   recognized as an essential strategy to fight against cyberattacks.
   Cybersecurity information sharing among various organizations and
   leveraging the aggregated cyber information to build proactive cyber
   defense system is nontrivial for organizations. However, building such
   cyber defense system is challenged by two issues: (1) organizations are
   reluctant to share their private information to others (2) even when
   they agree on a solution where information can be shared in privacy
   preserving manner, the obfuscated cyber threat information has to be
   processed to build the trained model for future prediction of any new or
   unknown cyber incident. To address these issues, in this paper, we
   propose a privacy preserving protocol where organizations can share
   their private information as an encrypted form with others and they can
   learn the information for future prediction without disclosing any
   private information. More specifically we propose a privacy preserving
   decision tree algorithm, where each organization can build and learn the
   decision tree based on overall organizations' training spam/ham email
   data without disclosing any private information of any party. Once the
   building of a decision tree is done, the organizations can predict if
   any new email is spam or ham locally.}},
ISBN = {{978-1-7281-0554-3}},
Unique-ID = {{ISI:000469462800116}},
}

@article{ ISI:000468833500001,
Author = {Han, Kyoohyung and Hhan, Minki and Cheon, Jung Hee},
Title = {{Improved Homomorphic Discrete Fourier Transforms and FHE Bootstrapping}},
Journal = {{IEEE ACCESS}},
Year = {{2019}},
Volume = {{7}},
Pages = {{57361-57370}},
Abstract = {{Homomorphic encryption (HE), which enables computation on ciphertexts
   without any leakage, rise as a most promising solution for
   privacy-preserving data processing, including secure machine learning
   and secure out-sourcing computation. Despite the extensive applicability
   of HE, the current constructions are sometimes considered as impractical
   due to its inefficiency. In this paper, we propose improvements on the
   linear transformation in bootstrapping, a technique allowing the in
   finite number of operation for HE, and homomorphic discrete Fourier
   transformation (DFT) using batch homomorphic encryption. We observe that
   the multiplication of a sparse diagonal matrix and ciphertext of a
   vector can be done within O(1) homomorphic computations. This
   observation induces the faster algorithm for linear transformation in
   bootstrapping and homomorphic DFT. To achieve this, we use Cooley-Tukey
   matrix factorization and construct a new recursive factorization of the
   linear transformation in bootstrapping. Our method with radix r only
   requires O (r log(r) n) constant vector multiplication and O (root r
   log(r) n) rotations by consuming O (log(r) n) depth when the input
   vector size is n. The previous method used in the library, a library
   that implements homomorphic encryption for approximate computation,
   requires O (n) and O (root n), respectively. To show the performance
   improvement, we implement our method on top of the library. Our
   implementation, along with further few techniques, of these algorithms
   show the significant improvements compared to the previous algorithm.
   New homomorphic DFT with length 2(14) only takes about 8s which results
   150 times faster than the previous method. Furthermore, the
   bootstrapping takes about 2 minutes for C-32768 plaintext space with
   8-bit precision, which takes 26 hours with same bit precision using the
   previous method.}},
DOI = {{10.1109/ACCESS.2019.2913850}},
ISSN = {{2169-3536}},
Unique-ID = {{ISI:000468833500001}},
}

@article{ ISI:000465474000001,
Author = {Lv, Zefang and Wang, Lirong and Guan, Zhitao and Wu, Jun and Du,
   Xiaojiang and Zhao, Hongtao and Guizani, Mohsen},
Title = {{An Optimizing and Differentially Private Clustering Algorithm for Mixed
   Data in SDN-Based Smart Grid}},
Journal = {{IEEE ACCESS}},
Year = {{2019}},
Volume = {{7}},
Pages = {{45773-45782}},
Abstract = {{Software-defined network (SDN) is widely used in smart grid for
   monitoring and managing the communication network. Big data analytics
   for SDN-based smart grid has got increasing attention. It is a promising
   approach to use machine learning technologies to analyze a large amount
   of data generated in SDN-based smart grid. However, the disclosure of
   personal privacy information must receive considerable attention. For
   instance, data clustering in user electricity behavior analysis may lead
   to the disclosure of personal privacy information. In this paper, an
   optimizing and differentially private clustering algorithm named ODPCA
   is proposed. In the ODPCA, the differentially private K-means algorithm
   and K-modes algorithm are combined to cluster mixed data in a
   privacy-preserving manner. The allocation of privacy budgets is
   optimized to improve the accuracy of clustering results. Specifically,
   the loss function that considers both the numerical and categorical
   attributes between true centroids and noisy centroids is analyzed to
   optimize the allocation the privacy budget; the number of iterations of
   clustering is set to a fixed value based on the total privacy budget and
   the minimal privacy budget allocated to each iteration. It is proved
   that the ODPCA can meet the differential privacy requirements and has
   better performance by comparing with other popular algorithms.}},
DOI = {{10.1109/ACCESS.2019.2909048}},
ISSN = {{2169-3536}},
ORCID-Numbers = {{Guan, Zhitao/0000-0003-0901-8621}},
Unique-ID = {{ISI:000465474000001}},
}

@inproceedings{ ISI:000465022600066,
Author = {Liu, Wenchao and Pan, Feng and Wang, Xu An and Cao, Yunfei and Tang,
   Dianhua},
Editor = {{Barolli, L and Kryvinska, N and Enokido, T and Takizawa, M}},
Title = {{Privacy-Preserving All Convolutional Net Based on Homomorphic Encryption}},
Booktitle = {{ADVANCES IN NETWORK-BASED INFORMATION SYSTEMS, NBIS-2018}},
Series = {{Lecture Notes on Data Engineering and Communications Technologies}},
Year = {{2019}},
Volume = {{22}},
Pages = {{752-762}},
Note = {{21st International Conference on Network-Based Information Systems
   (NBiS), Comenius Univ, Bratislava, SLOVAKIA, SEP 05-07, 2018}},
Abstract = {{Machine learning servers with mass storage and computing power is an
   ideal platform to store, manage, and analyze data and support
   decision-making. However, the main issue is providing security and
   privacy to the data, as the data is stored in a public way. Recently,
   homomorphic data encryption has been proposed as a solution due to its
   capabilities in performing computations over encrypted data. In this
   paper, we proposed an encrypted all convolutional net that transformed
   traditional all convolutional net into a net based on homomorphic
   encryption. This scheme allows different data holders to send their
   encrypted data to cloud service, complete predictions, and return them
   in encrypted form as the cloud service provider does not have a secret
   key. Therefore, the cloud service provider and others cannot get
   unencrypted raw data. When applied to the MNIST database,
   privacy-preserving all convolutional based on homomorphic encryption
   predict efficiently, accurately and with privacy protection.}},
DOI = {{10.1007/978-3-319-98530-5\_66}},
ISSN = {{2367-4512}},
ISBN = {{978-3-319-98530-5; 978-3-319-98529-9}},
Unique-ID = {{ISI:000465022600066}},
}

@article{ ISI:000458593900010,
Author = {Malathi, D. and Logesh, R. and Subramaniyaswamy, V and Vijayakumar, V.
   and Sangaiah, Arun Kumar},
Title = {{Hybrid Reasoning-based Privacy-Aware Disease Prediction Support System}},
Journal = {{COMPUTERS \& ELECTRICAL ENGINEERING}},
Year = {{2019}},
Volume = {{73}},
Pages = {{114-127}},
Month = {{JAN}},
Abstract = {{Recent developments in Information and Communication Technologies (ICT)
   and online healthcare services have created a huge volume of health
   data. With the advancements in machine learning approaches, the research
   on Disease Prediction Support System (DPSS) has attracted many
   researchers globally. In this article, we present a hybrid reasoning
   based methodology on predicting diseases. The combinatorial advantage of
   Fuzzy sety theory, k-nearest neighbor and case-based reasoning helps to
   yield enhanced prediction results. Though DPSS facilitates promising
   healthcare services, data security and privacy are still crucial
   challenging issues to be addressed. The DPSS is extended as a Privacy
   Aware Disease Prediction Support System (PDPSS) using Paillier
   Homomorphic Encryption to preserve patients' sensitive information from
   unauthorized user access. The proposed prediction model is evaluated
   with the statistical evaluation metrics, and the experimental results
   reveal the improved performance of PDPSS in enhanced prediction accuracy
   and better security. (C) 2018 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.compeleceng.2018.11.009}},
ISSN = {{0045-7906}},
EISSN = {{1879-0755}},
ORCID-Numbers = {{Varadarajan, Vijayakumar/0000-0003-3752-7220
   , Subramaniyaswamy/0000-0001-5328-7672
   Sangaiah, Arun Kumar/0000-0002-0229-2460}},
Unique-ID = {{ISI:000458593900010}},
}

@article{ ISI:000457149400008,
Author = {Rouhani, Bita Darvish and Hussain, Siam Umar and Lauter, Kristin and
   Koushanfar, Farinaz},
Title = {{ReDCrypt: Real-Time Privacy-Preserving Deep Learning Inference in Clouds
   Using FPGAs}},
Journal = {{ACM TRANSACTIONS ON RECONFIGURABLE TECHNOLOGY AND SYSTEMS}},
Year = {{2018}},
Volume = {{11}},
Number = {{3, SI}},
Month = {{DEC}},
Abstract = {{Artificial Intelligence (AI) is increasingly incorporated into the cloud
   business in order to improve the functionality (e.g., accuracy) of the
   service. The adoption of AI as a cloud service raises serious privacy
   concerns in applications where the risk of data leakage is not
   acceptable. Examples of such applications include scenarios where
   clients hold potentially sensitive private information such as medical
   records, financial data, and/or location. This article proposes
   ReDCrypt, the first reconfigurable hardware-accelerated framework that
   empowers privacy-preserving inference of deep learning models in cloud
   servers. ReDCrypt is well-suited for streaming (a.k.a., real-time AI)
   settings where clients need to dynamically analyze their data as it is
   collected over time without having to queue the samples to meet a
   certain batch size. Unlike prior work, ReDCrypt neither requires to
   change how AI models are trained nor relies on two non-colluding servers
   to perform. The privacy-preserving computation in ReDCrypt is executed
   using Yao's Garbled Circuit (GC) protocol We break down the deep
   learning inference task into two phases: (i) privacy-insensitive (local)
   computation, and (ii) privacy-sensitive (interactive) computation. We
   devise a high-throughput and power-efficient implementation of GC
   protocol on FPGA for the privacy-sensitive phase. ReDCrypt's
   accompanying API provides support for seamless integration of ReDCrypt
   into any deep learning framework. Proof-of-concept evaluations for
   different DL applications demonstrate up to 57-fold higher throughput
   per core compared to the best prior solution with no drop in the
   accuracy.}},
DOI = {{10.1145/3242899}},
Article-Number = {{21}},
ISSN = {{1936-7406}},
EISSN = {{1936-7414}},
Unique-ID = {{ISI:000457149400008}},
}

@article{ ISI:000454221700026,
Author = {Imtiaz, Hafiz and Sarwate, Anand D.},
Title = {{Distributed Differentially Private Algorithms for Matrix and Tensor
   Factorization}},
Journal = {{IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING}},
Year = {{2018}},
Volume = {{12}},
Number = {{6}},
Pages = {{1449-1464}},
Month = {{DEC}},
Abstract = {{In many signal processing and machine learning applications, datasets
   containing private information are held at different locations,
   requiring the development of distributed privacy-preserving algorithms.
   Tensor and matrix factorizations are key components of many processing
   pipelines. In the distributed setting, differentially private algorithms
   suffer because they introduce noise to guarantee privacy. This paper
   designs new and improved distributed and differentially private
   algorithms for two popular matrix and tensor factorization methods:
   principal component analysis and orthogonal tensor decomposition. The
   new algorithms employ a correlated noise design scheme to alleviate the
   effects of noise and can achieve the same noise level as the centralized
   scenario. Experiments on synthetic and real data illustrate the regimes
   in which the correlated noise allows performance matching with the
   centralized setting, outperforming previous methods and demonstrating
   that meaningful utility is possible while guaranteeing differential
   privacy.}},
DOI = {{10.1109/JSTSP.2018.2877842}},
ISSN = {{1932-4553}},
EISSN = {{1941-0484}},
ORCID-Numbers = {{Sarwate, Anand/0000-0001-6123-5282
   Imtiaz, Hafiz/0000-0002-2042-5941}},
Unique-ID = {{ISI:000454221700026}},
}

@article{ ISI:000451472000005,
Author = {Zhang, Xiaoyu and Chen, Xiaofeng and Wang, Jianfeng and Zhan, Zhihui and
   Li, Jin},
Title = {{Verifiable privacy-preserving single-layer perceptron training scheme in
   cloud computing}},
Journal = {{SOFT COMPUTING}},
Year = {{2018}},
Volume = {{22}},
Number = {{23, SI}},
Pages = {{7719-7732}},
Month = {{DEC}},
Abstract = {{With the advent of artificial intelligence, machine learning has been
   well explored and extensively applied into numerous fields, such as
   pattern recognition, image processing and cloud computing. Very
   recently, machine learning hosted in a cloud service has gained more
   attentions due to the benefits from the outsourcing paradigm. Based on
   cloud-aided computation techniques, the heavy computation tasks involved
   in machine learning process can be off-loaded into the cloud server in a
   pay-per-use manner, whereas outsourcing large-scale collection of
   sensitive data risks privacy leakage since the cloud server is
   semi-honest. Therefore, privacy preservation for the client and
   verification for the returned results become two challenges to be dealt
   with. In this paper, we focus on designing a novel privacy-preserving
   single-layer perceptron training scheme which supports batch patterns
   training and verification for the training results on the client side.
   In addition, adopting classical secure two-party computation method, we
   design a novel lightweight privacy-preserving predictive algorithm. Both
   two participants learns nothing about other's inputs, and the
   calculation result is only known by one party. Detailed security
   analysis shows that the proposed scheme can achieve the desired security
   properties. We also demonstrate the efficiency of our scheme by
   providing the experimental evaluation on two different real datasets.}},
DOI = {{10.1007/s00500-018-3233-7}},
ISSN = {{1432-7643}},
EISSN = {{1433-7479}},
Unique-ID = {{ISI:000451472000005}},
}

@article{ ISI:000450158600012,
Author = {Wang, Qian and Du, Minxin and Chen, Xiuying and Chen, Yanjiao and Zhou,
   Pan and Chen, Xiaofeng and Huang, Xinyi},
Title = {{Privacy-Preserving Collaborative Model Learning: The Case of Word Vector
   Training}},
Journal = {{IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING}},
Year = {{2018}},
Volume = {{30}},
Number = {{12}},
Pages = {{2381-2393}},
Month = {{DEC 1}},
Abstract = {{Nowadays, machine learning is becoming a new paradigm for mining hidden
   knowledge in big data. The collection and manipulation of big data not
   only create considerable values, but also raise serious privacy
   concerns. To protect the huge amount of potentially sensitive data, a
   straightforward approach is to encrypt data with specialized
   cryptographic tools. However, it is challenging to utilize or operate on
   encrypted data, especially to perform machine learning algorithms. In
   this paper, we investigate the problem of training high quality word
   vectors over large-scale encrypted data (from distributed data owners)
   with the privacy-preserving collaborative neural network learning
   algorithms. We leverage and also design a suite of arithmetic primitives
   (e.g., multiplication, fixed-point representation, sigmoid function
   computation, etc.) on encrypted data, served as components of our
   construction. We theoretically analyze the security and efficiency of
   our proposed construction, and conduct extensive experiments on
   representative real-world datasets to verify its practicality and
   effectiveness.}},
DOI = {{10.1109/TKDE.2018.2819673}},
ISSN = {{1041-4347}},
EISSN = {{1558-2191}},
ORCID-Numbers = {{Du, Minxin/0000-0001-6620-6923
   Wang, Qian/0000-0002-8967-8525
   Chen, Yanjiao/0000-0002-1382-0679}},
Unique-ID = {{ISI:000450158600012}},
}

@article{ ISI:000452063900006,
Author = {Bonte, Charlotte and Vercauteren, Frederik},
Title = {{Privacy-preserving logistic regression training}},
Journal = {{BMC MEDICAL GENOMICS}},
Year = {{2018}},
Volume = {{11}},
Number = {{4}},
Month = {{OCT 11}},
Note = {{6th iDASH Privacy and Security Workshop, Orlando, FL, OCT 14, 2017}},
Organization = {{iDASH}},
Abstract = {{Background: Logistic regression is a popular technique used in machine
   learning to construct classification models. Since the construction of
   such models is based on computing with large datasets, it is an
   appealing idea to outsource this computation to a cloud service. The
   privacy-sensitive nature of the input data requires appropriate privacy
   preserving measures before outsourcing it. Homomorphic encryption
   enables one to compute on encrypted data directly, without decryption
   and can be used to mitigate the privacy concerns raised by using a cloud
   service.
   Methods: In this paper, we propose an algorithm (and its implementation)
   to train a logistic regression model on a homomorphically encrypted
   dataset. The core of our algorithm consists of a new iterative method
   that can be seen as a simplified form of the fixed Hessian method, but
   with a much lower multiplicative complexity.
   Results: We test the new method on two interesting real life
   applications: the first application is in medicine and constructs a
   model to predict the probability for a patient to have cancer, given
   genomic data as input; the second application is in finance and the
   model predicts the probability of a credit card transaction to be
   fraudulent. The method produces accurate results for both applications,
   comparable to running standard algorithms on plaintext data.
   Conclusions: This article introduces a new simple iterative algorithm to
   train a logistic regression model that is tailored to be applied on a
   homomorphically encrypted dataset. This algorithm can be used as a
   privacy-preserving technique to build a binary classification model and
   can be applied in a wide range of problems that can be modelled with
   logistic regression. Our implementation results show that our method can
   handle the large datasets used in logistic regression training.}},
DOI = {{10.1186/s12920-018-0398-y}},
Article-Number = {{86}},
ISSN = {{1755-8794}},
ResearcherID-Numbers = {{Vercauteren, Frederik/A-1558-2019}},
ORCID-Numbers = {{Vercauteren, Frederik/0000-0002-7208-9599}},
Unique-ID = {{ISI:000452063900006}},
}

@article{ ISI:000447216700001,
Author = {Wang, Jian and Qiao, Kuoyuan and Zhang, Zhiyong},
Title = {{Trust evaluation based on evidence theory in online social networks}},
Journal = {{INTERNATIONAL JOURNAL OF DISTRIBUTED SENSOR NETWORKS}},
Year = {{2018}},
Volume = {{14}},
Number = {{10}},
Month = {{OCT 11}},
Abstract = {{Trust is an important criterion for access control in the field of
   online social networks privacy preservation. In the present methods, the
   subjectivity and individualization of the trust is ignored and a fixed
   model is built for all the users. In fact, different users probably take
   different trust features into their considerations when making trust
   decisions. Besides, in the present schemes, only users' static features
   are mapped into trust values, without the risk of privacy leakage. In
   this article, the features that each user cares about when making trust
   decisions are mined by machine learning to be User-Will. The privacy
   leakage risk of the evaluated user is estimated through information flow
   predicting. Then the User-Will and the privacy leakage risk are all
   mapped into trust evidence to be combined by an improved evidence
   combination rule of the evidence theory. In the end, several typical
   methods and the proposed scheme are implemented to compare the
   performance on dataset Epinions. Our scheme is verified to be more
   advanced than the others by comparing the F-Score and the Mean Error of
   the trust evaluation results.}},
DOI = {{10.1177/1550147718794629}},
Article-Number = {{1550147718794629}},
ISSN = {{1550-1477}},
Unique-ID = {{ISI:000447216700001}},
}

@article{ ISI:000450251900067,
Author = {Yang, Jianfei and Zou, Han and Jiang, Hao and Xie, Lihua},
Title = {{Device-Free Occupant Activity Sensing Using WiFi-Enabled IoT Devices for
   Smart Homes}},
Journal = {{IEEE INTERNET OF THINGS JOURNAL}},
Year = {{2018}},
Volume = {{5}},
Number = {{5}},
Pages = {{3991-4002}},
Month = {{OCT}},
Abstract = {{Intelligent occupancy sensing is becoming a vital underpinning for
   various emerging applications in smart homes, such as security
   surveillance and human behavior analysis. However, prevailing approaches
   mainly rely on video camera, ambient sensors, or wearable devices, which
   either requires arduous deployment or arouses privacy concerns. In this
   paper, we present a novel real-time, device-free, and privacy-preserving
   WiFi-enabled Internet of Things platform for occupancy sensing, which
   can promote a myriad of emerging applications. It is designed to achieve
   an optimal tradeoff between performance and scalability. Our system
   empowers commercial off-the-shelf WiFi routers to collect channel state
   information (CSI) measurements and provides an efficient cloud server
   for computing via a lightweight communication protocol. To demonstrate
   the usefulness of our platform, an occupancy detection system is
   developed by exploiting the CSI curve of human presence. Furthermore, we
   also design an innovative activity recognition system based on our
   platform and machine learning techniques with high availability and
   extensibility. In the evaluation, the experimental results show that our
   platform enables these applications efficiently, with the accuracy of
   96.8\% and 90.6\% in terms of occupancy detection and recognition,
   respectively.}},
DOI = {{10.1109/JIOT.2018.2849655}},
ISSN = {{2327-4662}},
ResearcherID-Numbers = {{Zou, Han/Q-4862-2017
   Jiang, Hao/A-1016-2014}},
ORCID-Numbers = {{Xie, Lihua/0000-0002-7137-4136
   Zou, Han/0000-0002-8063-5211
   Jiang, Hao/0000-0002-6902-9245}},
Unique-ID = {{ISI:000450251900067}},
}

@article{ ISI:000448661500299,
Author = {Paolanti, Marina and Romeo, Luca and Liciotti, Daniele and Pietrini,
   Rocco and Cenci, Annalisa and Frontoni, Emanuele and Zingaretti, Primo},
Title = {{Person Re-Identification with RGB-D Camera in Top-View Configuration
   through Multiple Nearest Neighbor Classifiers and Neighborhood Component
   Features Selection}},
Journal = {{SENSORS}},
Year = {{2018}},
Volume = {{18}},
Number = {{10}},
Month = {{OCT}},
Abstract = {{Person re-identification is an important topic in retail, scene
   monitoring, human-computer interaction, people counting, ambient
   assisted living and many other application fields. A dataset for person
   re-identification TVPR (Top View Person Re-Identification) based on a
   number of significant features derived from both depth and color images
   has been previously built. This dataset uses an RGB-D camera in a
   top-view configuration to extract anthropometric features for the
   recognition of people in view of the camera, reducing the problem of
   occlusions while being privacy preserving. In this paper, we introduce a
   machine learning method for person re-identification using the TVPR
   dataset. In particular, we propose the combination of multiple k-nearest
   neighbor classifiers based on different distance functions and feature
   subsets derived from depth and color images. Moreover, the neighborhood
   component feature selection is used to learn the depth features'
   weighting vector by minimizing the leave-one-out regularized training
   error. The classification process is performed by selecting the first
   passage under the camera for training and using the others as the
   testing set. Experimental results show that the proposed methodology
   outperforms standard supervised classifiers widely used for the
   re-identification task. This improvement encourages the application of
   this approach in the retail context in order to improve retail
   analytics, customer service and shopping space management.}},
DOI = {{10.3390/s18103471}},
Article-Number = {{3471}},
ISSN = {{1424-8220}},
ResearcherID-Numbers = {{Frontoni, Emanuele/D-9838-2013
   }},
ORCID-Numbers = {{Paolanti, Marina/0000-0002-5523-7174
   Frontoni, Emanuele/0000-0002-8893-9244
   Liciotti, Daniele/0000-0001-8397-4983}},
Unique-ID = {{ISI:000448661500299}},
}

@article{ ISI:000437997500028,
Author = {Li, Ping and Li, Tong and Ye, Heng and Li, Jin and Chen, Xiaofeng and
   Xiang, Yang},
Title = {{Privacy-preserving machine learning with multiple data providers}},
Journal = {{FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE}},
Year = {{2018}},
Volume = {{87}},
Pages = {{341-350}},
Month = {{OCT}},
Abstract = {{With the fast development of cloud computing, more and more data storage
   and computation are moved from the local to the cloud, especially the
   applications of machine learning and data analytics. However, the cloud
   servers are run by a third party and cannot be fully trusted by users.
   As a result, how to perform privacy-preserving machine learning over
   cloud data from different data providers becomes a challenge. Therefore,
   in this paper, we propose a novel scheme that protects the data sets of
   different providers and the data sets of cloud. To protect the privacy
   requirement of different providers, we use public-key encryption with a
   double decryption algorithm (DD-PKE) to encrypt their data sets with
   different public keys. To protect the privacy of data sets on the cloud,
   we use is an element of-differential privacy. Furthermore, the noises
   for the is an element of-differential privacy are added by the cloud
   server, instead of data providers, for different data analytics. Our
   scheme is proven to be secure in the security model. The experiments
   also demonstrate the efficiency of our protocol with different classical
   machine learning algorithms. (C) 2018 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.future.2018.04.076}},
ISSN = {{0167-739X}},
EISSN = {{1872-7115}},
ORCID-Numbers = {{Ye, Heng/0000-0002-6560-9883}},
Unique-ID = {{ISI:000437997500028}},
}

@article{ ISI:000437997500070,
Author = {Zhu, Tianqing and Li, Gang and Xiong, Ping and Zhou, Wanlei},
Title = {{Answering differentially private queries for continual datasets release}},
Journal = {{FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE}},
Year = {{2018}},
Volume = {{87}},
Pages = {{816-827}},
Month = {{OCT}},
Abstract = {{Privacy preserving data release is a hot topic that attracts a lot of
   attentions in data mining, machine learning, and social network
   communities. Most studies on privacy preserving focus on static data
   releases; however, data are usually updated periodically. As a potential
   solution, differential privacy addresses continual data release by
   simplifying it into an event stream release problem. This approach
   overlooks the relationship between events, which is defined as coupled
   information in this paper. We argue that datasets cannot be simplified
   as an event stream due to the coupled information. In addition, the
   coupled information may reveal more private information than expected.
   This work proposes a privacy-preserving mechanism that explicitly
   identify the coupled information in continually released datasets. In
   stead of simplifying datasets to event streams, this mechanism considers
   the continual released datasets as coupled datasets based on the
   relationship between the same individual in different datasets, and the
   relationship between different individuals in the same dataset. We also
   propose the notion of coupled sensitivity for answering differentially
   private queries and develop an iterative based coupled continual release
   algorithm, called CCR, that answers these queries with a large set of
   differentially private results. Theoretical analysis proves the privacy
   of this method, and an extensive performance study shows that CCR
   outperforms traditional differential privacy mechanisms when answering a
   large set of queries. (C) 2017 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.future.2017.05.007}},
ISSN = {{0167-739X}},
EISSN = {{1872-7115}},
Unique-ID = {{ISI:000437997500070}},
}

@article{ ISI:000452544100059,
Author = {Hynes, Nick and Dao, David and Yan, David and Cheng, Raymond and Song,
   Dawn},
Title = {{A Demonstration of Sterling: A Privacy-Preserving Data Marketplace}},
Journal = {{PROCEEDINGS OF THE VLDB ENDOWMENT}},
Year = {{2018}},
Volume = {{11}},
Number = {{12}},
Pages = {{2086-2089}},
Month = {{AUG}},
Abstract = {{In this work, we demonstrate Sterling, a decentralized marketplace for
   private data. Sterling enables privacy-preserving distribution and use
   of data by using privacy-preserving smart contracts which run on a
   permissionless blockchain. The privacy-preserving smart contracts,
   written by data providers and consumers, immutably and irrevocably
   represent the interests of their creators. In particular, we provide a
   mechanism for data providers to control the use of their data through
   automatic verification of data consumer contracts, allowing providers to
   express constraints such as pricing and differential privacy. Through
   smart contracts and trusted execution environments, Sterling enables
   privacy-preserving analytics and machine learning over private data in
   an efficient manner. The resulting economy ensures that the interests of
   all parties are aligned.
   For the demonstration, we highlight the use of Sterling for training
   machine learning models on individuals' health data. In doing so, we
   showcase novel approaches to automatically appraising training data,
   verifying and enforcing model privacy properties, and efficiently
   training private models on the blockchain using trusted hardware.}},
DOI = {{10.14778/3229863.3236266}},
ISSN = {{2150-8097}},
Unique-ID = {{ISI:000452544100059}},
}

@article{ ISI:000442317300010,
Author = {Du, Miao and Wang, Kun and Chen, Yuanfang and Wang, Xiaoyan and Sun,
   Yanfei},
Title = {{Big Data Privacy Preserving in Multi-Access Edge Computing for
   Heterogeneous Internet of Things}},
Journal = {{IEEE COMMUNICATIONS MAGAZINE}},
Year = {{2018}},
Volume = {{56}},
Number = {{8}},
Pages = {{62-67}},
Month = {{AUG}},
Abstract = {{With the popularity of smart devices, multi-access edge computing (MEC)
   has become the mainstream of dealing with big data in heterogeneous
   Internet of Things (H-IoT). MEC makes full use of the computing power of
   edge nodes, which greatly reduces the computing pressure of data
   centers, and brings great convenience to the storage and processing of
   big data. However, it is easy to become the object of hacker attacks due
   to the lack of centralized management of distributed nodes. Once these
   nodes are compromised, a series of privacy issues can happen. In this
   article, we first overview the architecture of MEC for H-IoT. The MEC
   covers three-level advanced functional entities, including moblie edge
   (ME) system-level, ME host-level and ME network-level. Second, we draw
   our attention to the privacy issues in the MEC, especially in data
   aggregation and data mining. In addition, we consider machine learning
   privacy preserving as a case study in the application of MEC. Simulation
   results are shown to reveal the feasibility of the proposed method.
   Finally, we propose open issues for future work.}},
DOI = {{10.1109/MCOM.2018.1701148}},
ISSN = {{0163-6804}},
EISSN = {{1558-1896}},
Unique-ID = {{ISI:000442317300010}},
}

@article{ ISI:000438962800010,
Author = {Jia, Qi and Guo, Linke and Jin, Zhanpeng and Fang, Yuguang},
Title = {{Preserving Model Privacy for Machine Learning in Distributed Systems}},
Journal = {{IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS}},
Year = {{2018}},
Volume = {{29}},
Number = {{8}},
Pages = {{1808-1822}},
Month = {{AUG 1}},
Abstract = {{Machine Learning based data classification is a widely used data mining
   technique. By learning massive data collected from the real world, data
   classification helps learners discover hidden data patterns. These
   hidden data patterns are represented by the learned model in different
   machine learning schemes. Based on such models, a user can classify
   whether the new incoming data belongs to an existing class; or, multiple
   entities may test the similarity of their datasets. However, due to data
   locality and privacy concerns, it is infeasible for large-scale
   distributed systems to share each individual's datasets for classifying
   or testing. On the one hand, the learned model is an entity's private
   asset and may leak private information, which should be well protected
   from all other non-collaborative entities. On the other hand, the new
   incoming data may contain sensitive information which cannot be
   disclosed directly for classification. To address the above privacy
   issues, we propose an approach to preserve the model privacy of the data
   classification and similarity evaluation for distributed systems. With
   our scheme, neither new data nor learned models are directly revealed
   during the classification and similarity evaluation procedures. Based on
   extensive real-world experiments, we have evaluated the privacy
   preservation, feasibility, and efficiency of the proposed scheme.}},
DOI = {{10.1109/TPDS.2018.2809624}},
ISSN = {{1045-9219}},
EISSN = {{1558-2183}},
ResearcherID-Numbers = {{Fang, Yuguang/A-7484-2009
   }},
ORCID-Numbers = {{Fang, Yuguang/0000-0002-1079-3871
   Jin, Zhanpeng/0000-0002-3020-3736}},
Unique-ID = {{ISI:000438962800010}},
}

@article{ ISI:000438647600001,
Author = {Gonzalez-Serrano, Francisco-Javier and Amor-Martin, Adrian and
   Casamayon-Anton, Jorge},
Title = {{Supervised machine learning using encrypted training data}},
Journal = {{INTERNATIONAL JOURNAL OF INFORMATION SECURITY}},
Year = {{2018}},
Volume = {{17}},
Number = {{4}},
Pages = {{365-377}},
Month = {{AUG}},
Abstract = {{Preservation of privacy in data mining and machine learning has emerged
   as an absolute prerequisite in many practical scenarios, especially when
   the processing of sensitive data is outsourced to an external third
   party. Currently, privacy preservation methods are mainly based on
   randomization and/or perturbation, secure multiparty computations and
   cryptographic methods. In this paper, we take advantage of the partial
   homomorphic property of some cryptosystems to train simple machine
   learning models with encrypted data. Our basic scenario has three
   parties: multiple Data Owners, which provide encrypted training
   examples; the Algorithm Owner (or Application), which processes them to
   adjust the parameters of its models; and a semi-trusted third party,
   which provides privacy and secure computation services to the
   Application in some operations not supported by the homomorphic
   cryptosystem. In particular, we focus on two issues: the use of
   multiple-key cryptosystems, and the impact of the quantization of
   real-valued input data required before encryption. In addition, we
   develop primitives based on the outsourcing of a reduced set of
   operations that allows to implement general machine learning algorithms
   using efficient dedicated hardware. As applications, we consider the
   training of classifiers using privacy-protected data and the tracking of
   a moving target using encrypted distance measurements.}},
DOI = {{10.1007/s10207-017-0381-1}},
ISSN = {{1615-5262}},
EISSN = {{1615-5270}},
ResearcherID-Numbers = {{Gonzalez-Serrano, Francisco/K-6183-2014}},
ORCID-Numbers = {{Gonzalez-Serrano, Francisco/0000-0001-7514-7451}},
Unique-ID = {{ISI:000438647600001}},
}

@article{ ISI:000455631900001,
Author = {Tarik, Boudheb and Zakaria, Elberrichi},
Title = {{Privacy Preserving Feature Selection for Vertically Distributed Medical
   Data based on Genetic Algorithms and Naive Bayes}},
Journal = {{INTERNATIONAL JOURNAL OF INFORMATION SYSTEM MODELING AND DESIGN}},
Year = {{2018}},
Volume = {{9}},
Number = {{3}},
Month = {{JUL-SEP}},
Abstract = {{Machine learning is a powerful tool to mine useful knowledge from vast
   databases. Many establishments in the medical area such as hospitals,
   laboratories want to join their efforts with the ambition to extract
   models that are more accurate. However, this approach faces problems.
   Due to the laws protecting patient privacy or other similar concerns,
   parties are reluctant to share their data. In vast amounts of data,
   which are useful and pertinent in constructing accurate data mining
   models? In this article, the researchers deal with these challenges for
   vertically distributed medical data. They propose an original secure
   wrapper solution to perform feature selection based on genetic
   algorithms and distributed Naive Bayes. Contrary to the previous
   solutions, the original data is not perturbed. Therefore, the data
   utility and performance are preserved. They prove that the proposed
   solution selects relevant attributes to increase performance, preserving
   patient privacy.}},
DOI = {{10.4018/IJISMD.2018070101}},
Article-Number = {{UNSP 1}},
ISSN = {{1947-8186}},
EISSN = {{1947-8194}},
Unique-ID = {{ISI:000455631900001}},
}

@article{ ISI:000434294800006,
Author = {Wang, Yi and Wan, Jianwu and Guo, Jun and Cheung, Yiu-Ming and Yuen,
   Pong C.},
Title = {{Inference-Based Similarity Search in Randomized Montgomery Domains for
   Privacy-Preserving Biometric Identification}},
Journal = {{IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE}},
Year = {{2018}},
Volume = {{40}},
Number = {{7}},
Pages = {{1611-1624}},
Month = {{JUL}},
Abstract = {{Similarity search is essential to many important applications and often
   involves searching at scale on high-dimensional data based on their
   similarity to a query. In biometric applications, recent vulnerability
   studies have shown that adversarial machine learning can compromise
   biometric recognition systems by exploiting the biometric similarity
   information. Existing methods for biometric privacy protection are in
   general based on pairwise matching of secured biometric templates and
   have inherent limitations in search efficiency and scalability. In this
   paper, we propose an inference-based framework for privacy-preserving
   similarity search in Hamming space. Our approach builds on an obfuscated
   distance measure that can conceal Hamming distance in a dynamic
   interval. Such a mechanism enables us to systematically design
   statistically reliable methods for retrieving most likely candidates
   without knowing the exact distance values. We further propose to apply
   Montgomery multiplication for generating search indexes that can
   withstand adversarial similarity analysis, and show that information
   leakage in randomized Montgomery domains can be made negligibly small.
   Our experiments on public biometric datasets demonstrate that the
   inference-based approach can achieve a search accuracy close to the best
   performance possible with secure computation methods, but the associated
   cost is reduced by orders of magnitude compared to cryptographic
   primitives.}},
DOI = {{10.1109/TPAMI.2017.2727048}},
ISSN = {{0162-8828}},
EISSN = {{1939-3539}},
ResearcherID-Numbers = {{Wang, Yi/L-8254-2016
   }},
ORCID-Numbers = {{Wang, Yi/0000-0002-8448-8570
   Yuen, Pong Chi/0000-0002-9343-2202}},
Unique-ID = {{ISI:000434294800006}},
}

@article{ ISI:000436774300088,
Author = {Ricciuti, Manola and Spinsante, Susanna and Gambi, Ennio},
Title = {{Accurate Fall Detection in a Top View Privacy Preserving Configuration}},
Journal = {{SENSORS}},
Year = {{2018}},
Volume = {{18}},
Number = {{6}},
Month = {{JUN}},
Abstract = {{Fall detection is one of the most investigated themes in the research on
   assistive solutions for aged people. In particular, a false-alarm-free
   discrimination between falls and non-falls is indispensable, especially
   to assist elderly people living alone. Current technological solutions
   designed to monitor several types of activities in indoor environments
   can guarantee absolute privacy to the people that decide to rely on
   them. Devices integrating RGB and depth cameras, such as the Microsoft
   Kinect, can ensure privacy and anonymity, since the depth information is
   considered to extract only meaningful information from video streams. In
   this paper, we propose an accurate fall detection method investigating
   the depth frames of the human body using a single device in a top-view
   configuration, with the subjects located under the device inside a room.
   Features extracted from depth frames train a classifier based on a
   binary support vector machine learning algorithm. The dataset includes
   32 falls and 8 activities considered for comparison, for a total of 800
   sequences performed by 20 adults. The system showed an accuracy of
   98.6\% and only one false positive.}},
DOI = {{10.3390/s18061754}},
Article-Number = {{1754}},
ISSN = {{1424-8220}},
ResearcherID-Numbers = {{Spinsante, Susanna/J-8850-2019
   }},
ORCID-Numbers = {{Ricciuti, Manola/0000-0003-4870-0914
   Gambi, Ennio/0000-0001-6852-8483}},
Unique-ID = {{ISI:000436774300088}},
}

@article{ ISI:000429760800006,
Author = {Li, Tong and Li, Jin and Liu, Zheli and Li, Ping and Jia, Chunfu},
Title = {{Differentially private Naive Bayes learning over multiple data sources}},
Journal = {{INFORMATION SCIENCES}},
Year = {{2018}},
Volume = {{444}},
Pages = {{89-104}},
Month = {{MAY}},
Abstract = {{For meeting diverse requirements of data analysis, the machine learning
   classifier has been provided as a tool to evaluate data in many
   applications. Due to privacy concerns of preventing disclosing sensitive
   information, data owners often suppress their data for an untrusted
   trainer to train a classifier. Some existing work proposed
   privacy-preserving solutions for learning algorithms, which allow a
   trainer to build a classifier over the data from a single owner.
   However, they cannot be directly used in the multi-owner setting where
   each owner is not totally trusted for each other. In this paper, we
   propose a novel privacy preserving Naive Bayes learning scheme with
   multiple data sources. The proposed scheme enables a trainer to train a
   Naive Bayes classifier over the dataset provided jointly by different
   data owners, without the help of a trusted curator. The training result
   can achieve is an element of-differential privacy while the training
   will not break the privacy of each owner. We implement the prototype of
   the scheme and conduct corresponding experiment. (C) 2018 Elsevier Inc.
   All rights reserved.}},
DOI = {{10.1016/j.ins.2018.02.056}},
ISSN = {{0020-0255}},
EISSN = {{1872-6291}},
Unique-ID = {{ISI:000429760800006}},
}

@article{ ISI:000429396700005,
Author = {Park, Heejin and Kim, Pyung and Kim, Heeyoul and Park, Ki-Woong and Lee,
   Younho},
Title = {{Efficient machine learning over encrypted data with non-interactive
   communication}},
Journal = {{COMPUTER STANDARDS \& INTERFACES}},
Year = {{2018}},
Volume = {{58}},
Pages = {{87-108}},
Month = {{MAY}},
Abstract = {{In this paper, we describe a protocol framework that can perform
   classification tasks in a privacy-preserving manner. To demonstrate the
   feasibility of the proposed framework, we implement two protocols
   supporting Naive Bayes classification. We overcome the heavy
   computational load of conventional fully homomorphic encryption based
   privacy-preserving protocols by using various optimization techniques.
   The proposed method differs from previous techniques insofar as it
   requires no intermediate interactions between the server and the client
   while executing the protocol, except for the mandatory interaction to
   obtain the decryption result of the encrypted classification output. As
   a result of this minimal interaction, the proposed method is relatively
   stable. Furthermore, the decryption key is used only once during the
   execution of the protocol, overcoming a potential security issue caused
   by the frequent exposure of the decryption key in memory. The proposed
   implementation uses a cryptographic primitive that is secure against
   attacks with quantum computers. Therefore, the framework described in
   this paper is expected to be robust against future quantum computer
   attacks. (C) 2017 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.csi.2017.12.004}},
ISSN = {{0920-5489}},
EISSN = {{1872-7018}},
ORCID-Numbers = {{Lee, Younho/0000-0003-1767-6165
   Kim, Heeyoul/0000-0001-6341-580X
   Park, Ki-Woong/0000-0002-3377-223X}},
Unique-ID = {{ISI:000429396700005}},
}

@article{ ISI:000437104600004,
Author = {Rahman, Md Atiqur and Rahman, Tanzila and Laganiere, Robert and
   Mohammed, Noman and Wang, Yang},
Title = {{Membership Inference Attack against Differentially Private Deep Learning
   Model}},
Journal = {{TRANSACTIONS ON DATA PRIVACY}},
Year = {{2018}},
Volume = {{11}},
Number = {{1}},
Pages = {{61-79}},
Month = {{APR}},
Abstract = {{The unprecedented success of deep learning is largely dependent on the
   availability of massive amount of training data. In many cases, these
   data are crowd-sourced and may contain sensitive and confidential
   information, therefore, pose privacy concerns. As a result,
   privacy-preserving deep learning has been gaining increasing focus
   nowadays. One of the promising approaches for privacy-preserving deep
   learning is to employ differential privacy during model training which
   aims to prevent the leakage of sensitive information about the training
   data via the trained model. While these models are considered to be
   immune to privacy attacks, with the advent of recent and sophisticated
   attack models, it is not clear how well these models trade-off utility
   for privacy. In this paper, we systematically study the impact of a
   sophisticated machine learning based privacy attack called the
   membership inference attack against a state-of-the-art differentially
   private deep model. More specifically, given a differentially private
   deep model with its associated utility, we investigate how much we can
   infer about the model's training data. Our experimental results show
   that differentially private deep models may keep their promise to
   provide privacy protection against strong adversaries by only offering
   poor model utility, while exhibit moderate vulnerability to the
   membership inference attack when they offer an acceptable utility. For
   evaluating our experiments, we use the CIFAR-10 and MNIST datasets and
   the corresponding classification tasks.}},
ISSN = {{1888-5063}},
EISSN = {{2013-1631}},
Unique-ID = {{ISI:000437104600004}},
}

@article{ ISI:000429971100066,
Author = {Liu, Jianqing and Zhang, Chi and Fang, Yuguang},
Title = {{EPIC: A Differential Privacy Framework to Defend Smart Homes Against
   Internet Traffic Analysis}},
Journal = {{IEEE INTERNET OF THINGS JOURNAL}},
Year = {{2018}},
Volume = {{5}},
Number = {{2, SI}},
Pages = {{1206-1217}},
Month = {{APR}},
Abstract = {{The Internet of Things (IoT) becomes a novel paradigm as more and more
   devices are connected to the Internet, enabling several innovative
   applications such as smart home, industrial automation, and connected
   health. However, the cyber-attack to these applications is a big issue
   and countermeasures are in dire need to provide system security and user
   privacy. In this paper, we address the traffic analysis attack to smart
   homes, where adversaries intercept the Internet traffic from/to the
   smart home gateway and profile residents' behaviors through digital
   traces. Traditional cryptographic tools may not work well due to the
   effectiveness of adversaries' machine learning algorithms in classifying
   encrypted traffic, so here we propose a privacy-preserving traffic
   obfuscation framework to achieve the goal. To be specific, we leverage
   the smart community network of wirelessly connected smart homes and
   intentionally direct each smart home's traffic to another home gateway
   before entering the Internet. The design jointly considers the network
   energy consumption and the resource constraints in IoT devices, while
   achieving strong differential privacy guarantee so that adversaries
   cannot link any traffic flow to a specific smart home. Besides, we
   consider a hostile smart community network and develop secure multihop
   routing protocols to guarantee the source/destination unlinkability and
   satisfy each user's personalized privacy requirement. To evaluate the
   effectiveness of our framework in protecting privacy and reducing
   network energy consumption, extensive simulations are conducted and the
   results demonstrate that our design outperforms other differential
   privacy mechanism in preserving privacy and minimizing network utility
   cost.}},
DOI = {{10.1109/JIOT.2018.2799820}},
ISSN = {{2327-4662}},
ResearcherID-Numbers = {{Fang, Yuguang/A-7484-2009}},
ORCID-Numbers = {{Fang, Yuguang/0000-0002-1079-3871}},
Unique-ID = {{ISI:000429971100066}},
}

@article{ ISI:000426698400002,
Author = {Huguenin, Kevin and Bilogrevic, Igor and Machado, Joana Soares and
   Mihaila, Stefan and Shokri, Reza and Dacosta, Italo and Hubaux,
   Jean-Pierre},
Title = {{A Predictive Model for User Motivation and Utility Implications of
   Privacy-Protection Mechanisms in Location Check-Ins}},
Journal = {{IEEE TRANSACTIONS ON MOBILE COMPUTING}},
Year = {{2018}},
Volume = {{17}},
Number = {{4}},
Pages = {{760-774}},
Month = {{APR 1}},
Abstract = {{Location check-ins contain both geographical and semantic information
   about the visited venues. Semantic information is usually represented by
   means of tags (e.g., ``restaurant{''}). Such data can reveal some
   personal information about users beyond what they actually expect to
   disclose, hence their privacy is threatened. To mitigate such threats,
   several privacy protection techniques based on location generalization
   have been proposed. Although the privacy implications of such techniques
   have been extensively studied, the utility implications are mostly
   unknown. In this paper, we propose a predictive model for quantifying
   the effect of a privacy-preserving technique (i.e., generalization) on
   the perceived utility of check-ins. We first study the users'
   motivations behind their location check-ins, based on a study targeted
   at Foursquare users (N = 77). We propose a machine-learning method for
   determining the motivation behind each check-in, and we design a
   motivation-based predictive model for the utility implications of
   generalization. Based on the survey data, our results show that the
   model accurately predicts the fine-grained motivation behind a check-in
   in {[}43\%] of the cases and in {[}63\%] of the cases for the
   coarse-grained motivation. It also predicts, with a mean error of
   {[}0.52] (on a scale from 1 to 5), the loss of utility caused by
   semantic and geographical generalization. This model makes it possible
   to design of utility-aware, privacy-enhancing mechanisms in
   location-based online social networks. It also enables service providers
   to implement location-sharing mechanisms that preserve both the utility
   and privacy for their users.}},
DOI = {{10.1109/TMC.2017.2741958}},
ISSN = {{1536-1233}},
EISSN = {{1558-0660}},
ORCID-Numbers = {{Huguenin, Kevin/0000-0001-7147-1828}},
Unique-ID = {{ISI:000426698400002}},
}

@article{ ISI:000425571700007,
Author = {Dupoux, Emmanuel},
Title = {{Cognitive science in the era of artificial intelligence: A roadmap for
   reverse-engineering the infant language-learner}},
Journal = {{COGNITION}},
Year = {{2018}},
Volume = {{173}},
Pages = {{43-59}},
Month = {{APR}},
Abstract = {{Spectacular progress in the information processing sciences (machine
   learning, wearable sensors) promises to revolutionize the study of
   cognitive development. Here, we analyse the conditions under which
   `reverse engineering' language development, i.e., building an effective
   system that mimics infant's achievements, can contribute to our
   scientific understanding of early language development. We argue that,
   on the computational side, it is important to move from toy problems to
   the full complexity of the learning situation, and take as input as
   faithful reconstructions of the sensory signals available to infants as
   possible. On the data side, accessible but privacy-preserving
   repositories of home data have to be setup. On the psycholinguistic
   side, specific tests have to be constructed to benchmark humans and
   machines at different linguistic levels. We discuss the feasibility of
   this approach and present an overview of current results.}},
DOI = {{10.1016/j.cognition.2017.11.008}},
ISSN = {{0010-0277}},
EISSN = {{1873-7838}},
Unique-ID = {{ISI:000425571700007}},
}

@article{ ISI:000419535900018,
Author = {Gunasinghe, Hasini and Bertino, Elisa},
Title = {{PrivBioMTAuth: Privacy Preserving Biometrics-Based and User Centric
   Protocol for User Authentication From Mobile Phones}},
Journal = {{IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY}},
Year = {{2018}},
Volume = {{13}},
Number = {{4}},
Pages = {{1042-1057}},
Month = {{APR}},
Abstract = {{We introduce a privacy preserving biometrics-based authentication
   solution by which users can authenticate to different service providers
   from mobile phones without involving identity providers in the
   transactions. Authentication is performed via zero-knowledge proof of
   knowledge, based on a cryptographic identity token that encodes the
   biometric identifier of the user and a secret provided by the user,
   making it three-factor authentication. Our approach for generating a
   unique, repeatable, and revocable biometric identifier from the user's
   biometric image is based on a machine learning-based classification
   technique, which involves the features extracted from the user's
   biometric image. We have implemented a prototype of the proposed
   authentication solution and evaluated our solution with respect to its
   performance, security, and privacy. The evaluation has been performed on
   a public data set of face images.}},
DOI = {{10.1109/TIFS.2017.2777787}},
ISSN = {{1556-6013}},
EISSN = {{1556-6021}},
Unique-ID = {{ISI:000419535900018}},
}

@article{ ISI:000428095700008,
Author = {Li, Tong and Huang, Zhengan and Li, Ping and Liu, Zheli and Jia, Chunfu},
Title = {{Outsourced privacy-preserving classification service over encrypted data}},
Journal = {{JOURNAL OF NETWORK AND COMPUTER APPLICATIONS}},
Year = {{2018}},
Volume = {{106}},
Pages = {{100-110}},
Month = {{MAR 15}},
Abstract = {{With the diversity of cloud services, remote data services based on the
   machine learning classification have been provided in many applications
   including risk assessment and image recognition. In a classification
   service, a classifier owner that acts a service provider establishes a
   protocol to allow a user to query for the evaluation of his/her data.
   However, such an owner has to keep on-line continuously and equip with
   enough bandwidth and computing resources. Although the owner can
   outsource the service to a powerful service, there remains a challenge
   that is protecting the privacy of the data and the classifier. In this
   paper, we propose a novel scheme for a classifier owner to delegate a
   remote server to provide the privacy-preserving classification service
   for users. In the proposed scheme, we design efficient classification
   protocols for two concrete classifiers respectively. We implement the
   prototype of the scheme and conduct experiments. The experimental
   results show that the scheme is practical.}},
DOI = {{10.1016/j.jnca.2017.12.021}},
ISSN = {{1084-8045}},
Unique-ID = {{ISI:000428095700008}},
}

@article{ ISI:000457272700023,
Author = {Li, Ping and Li, Jin and Huang, Zhengan and Gao, Chong-Zhi and Chen,
   Wen-Bin and Chen, Kai},
Title = {{Privacy-preserving outsourced classification in cloud computing}},
Journal = {{CLUSTER COMPUTING-THE JOURNAL OF NETWORKS SOFTWARE TOOLS AND
   APPLICATIONS}},
Year = {{2018}},
Volume = {{21}},
Number = {{1, SI}},
Pages = {{277-286}},
Month = {{MAR}},
Abstract = {{Classifier has been widely applied in machine learning, such as pattern
   recognition, medical diagnosis, credit scoring, banking and weather
   prediction. Because of the limited local storage at user side, data and
   classifier has to be outsourced to cloud for storing and computing.
   However, due to privacy concerns, it is important to preserve the
   confidentiality of data and classifier in cloud computing because the
   cloud servers are usually untrusted. In this work, we propose a
   framework for privacy-preserving outsourced classification in cloud
   computing (POCC). Using POCC, an evaluator can securely train a
   classification model over the data encrypted with different public keys,
   which are outsourced from the multiple data providers. We prove that our
   scheme is secure in the semi-honest model}},
DOI = {{10.1007/s10586-017-0849-9}},
ISSN = {{1386-7857}},
EISSN = {{1573-7543}},
Unique-ID = {{ISI:000457272700023}},
}

@article{ ISI:000435855400008,
Author = {Chen, Kunjin and He, Ziyu and Wang, Shan X. and Hu, Jun and Li, Licheng
   and He, Jinliang},
Title = {{Learning-based Data Analytics: Moving Towards Transparent Power Grids}},
Journal = {{CSEE JOURNAL OF POWER AND ENERGY SYSTEMS}},
Year = {{2018}},
Volume = {{4}},
Number = {{1}},
Pages = {{67-82}},
Month = {{MAR}},
Abstract = {{In this paper, we present the learning-based data analytics moving
   towards transparent power grids and provide some possible extensions
   including machine learning, big data analytics, and knowledge
   transferring. The closed loops of data and knowledge are illustrated and
   the challenges for establishing the closed loops are discussed. General
   ideas and recent developments in supervised learning, unsupervised
   learning, and reinforcement learning are presented together with
   extensions for power system applications. Furthermore, much emphasis is
   placed on privacy-preserving data analysis, transfer of knowledge,
   machine learning for causal inference, scalability and flexibility of
   data analytics, and efficiency and reliability of computation. Existing
   integrated solutions in the industry featuring the Industrial Internet
   and the digital grid are also introduced.}},
DOI = {{10.17775/CSEEJPES.2017.01070}},
ISSN = {{2096-0042}},
Unique-ID = {{ISI:000435855400008}},
}

@article{ ISI:000426036000013,
Author = {Zhang, Tao and Zhu, Quanyan},
Title = {{Distributed Privacy-Preserving Collaborative Intrusion Detection Systems
   for VANETs}},
Journal = {{IEEE TRANSACTIONS ON SIGNAL AND INFORMATION PROCESSING OVER NETWORKS}},
Year = {{2018}},
Volume = {{4}},
Number = {{1, SI}},
Pages = {{148-161}},
Month = {{MAR}},
Abstract = {{Vehicular ad hoc network (VANET) is an enabling technology in modern
   transportation systems for providing safety and valuable information,
   and yet vulnerable to a number of attacks from passive eavesdropping to
   active interfering. Intrusion detection systems (IDSs) are important
   devices that can mitigate the threats by detecting malicious behaviors.
   Furthermore, the collaborations among vehicles in VANETs can improve the
   detection accuracy by communicating their experiences between nodes. To
   this end, distributed machine learning is a suitable framework for the
   design of scalable and implementable collaborative detection algorithms
   over VANETs. One fundamental barrier to collaborative learning is the
   privacy concern as nodes exchange data among them. A malicious node can
   obtain sensitive information of other nodes by inferring from the
   observed data. In this paper, we propose a privacy-preserving
   machine-learning based collaborative IDS (PML-CIDS) for VANETs. The
   proposed algorithm employs the alternating direction method of
   multipliers to a class of empirical risk minimization problems and
   trains a classifier to detect the intrusions in the VANETs. We use the
   differential privacy to capture the privacy notation of the PML-CIDS and
   propose amethod of dual-variable perturbation to provide dynamic
   differential privacy. We analyze theoretical performance and
   characterize the fundamental tradeoff between the security and privacy
   of the PML-CIDS. We also conduct numerical experiments using the network
   security laboratory-knowledge discovery and data mining (NSL-KDD)
   dataset to corroborate the results on the detection accuracy,
   security-privacy tradeoffs, and design.}},
DOI = {{10.1109/TSIPN.2018.2801622}},
ISSN = {{2373-776X}},
Unique-ID = {{ISI:000426036000013}},
}

@article{ ISI:000425166900012,
Author = {Liu, Xiaoxia and Zhu, Hui and Lu, Rongxing and Li, Hui},
Title = {{Efficient privacy-preserving online medical primary diagnosis scheme on
   naive bayesian classification}},
Journal = {{PEER-TO-PEER NETWORKING AND APPLICATIONS}},
Year = {{2018}},
Volume = {{11}},
Number = {{2, SI}},
Pages = {{334-347}},
Month = {{MAR}},
Abstract = {{With the advances of machine learning algorithms and the pervasiveness
   of network terminals, online medical primary diagnosis scheme, which can
   provide the primary diagnosis service anywhere anytime, has attracted
   considerable interest recently. However, the flourish of online medical
   primary diagnosis scheme still faces many challenges including
   information security and privacy preservation. In this paper, we propose
   an efficient and privacy-preserving medical primary diagnosis scheme,
   called PDiag, on naive Bayes classification. With PDiag, the sensitive
   personal health information can be processed without privacy disclosure
   during online medical primary diagnosis service. Specifically, based on
   an improved expression for the naive Bayes classifier, an efficient and
   privacy-preserving classification scheme is introduced with lightweight
   polynomial aggregation technique. The encrypted user query is directly
   operated at the service provider without decryption, and the diagnosis
   result can only be decrypted by user. Through extensive analysis, we
   show that PDiag ensures users' health information and service provider's
   prediction model are kept confidential, and has significantly less
   computation and communication overhead than existing schemes. In
   addition, performance evaluations via implementing PDiag on smartphone
   and computer demonstrate PDiag's effectiveness in term of real
   environment.}},
DOI = {{10.1007/s12083-016-0506-8}},
ISSN = {{1936-6442}},
EISSN = {{1936-6450}},
ORCID-Numbers = {{Zhu, Hui/0000-0002-5853-633X}},
Unique-ID = {{ISI:000425166900012}},
}

@article{ ISI:000429782000006,
Author = {Wang, Cong and Zheng, Yifeng and Jiang, Jinghua and Ren, Kui},
Title = {{Toward Privacy-Preserving Personalized Recommendation Services}},
Journal = {{ENGINEERING}},
Year = {{2018}},
Volume = {{4}},
Number = {{1, SI}},
Pages = {{21-28}},
Month = {{FEB}},
Abstract = {{Recommendation systems are crucially important for the delivery of
   personalized services to users. With personalized recommendation
   services, users can enjoy a variety of targeted recommendations such as
   movies, books, ads, restaurants, and more. In addition, personalized
   recommendation services have become extremely effective revenue drivers
   for online business. Despite the great benefits, deploying personalized
   recommendation services typically requires the collection of users'
   personal data for processing and analytics, which undesirably makes
   users susceptible to serious privacy violation issues. Therefore, it is
   of paramount importance to develop practical privacy-preserving
   techniques to maintain the intelligence of personalized recommendation
   services while respecting user privacy. In this paper, we provide a
   comprehensive survey of the literature related to personalized
   recommendation services with privacy protection. We present the general
   architecture of personalized recommendation systems, the privacy issues
   therein, and existing works that focus on privacy-preserving
   personalized recommendation services. We classify the existing works
   according to their underlying techniques for personalized recommendation
   and privacy protection, and thoroughly discuss and compare their merits
   and demerits, especially in terms of privacy and recommendation
   accuracy. We also identity some future research directions. (C) 2018 THE
   AUTHORS. Published by Elsevier LTD on behalf of Chinese Academy of
   Engineering and Higher Education Press Limited Company.}},
DOI = {{10.1016/j.eng.2018.02.005}},
ISSN = {{2095-8099}},
EISSN = {{2096-0026}},
ORCID-Numbers = {{Zheng, Yifeng/0000-0001-7852-6051}},
Unique-ID = {{ISI:000429782000006}},
}

@inproceedings{ ISI:000470974900004,
Author = {Canillas, Remi and Talbi, Rania and Bouchenak, Sara and Hasan, Omar and
   Brunie, Lionel and Sarrat, Laurent},
Book-Group-Author = {{Assoc Comp Machinery}},
Title = {{Exploratory Study of Privacy Preserving Fraud Detection}},
Booktitle = {{MIDDLEWARE INDUSTRY'18: PROCEEDINGS OF THE 2018 ACM/IFIP/USENIX
   MIDDLEWARE CONFERENCE (INDUSTRIAL TRACK)}},
Year = {{2018}},
Pages = {{25-31}},
Note = {{19th ACM/IFIP/USENIX Middleware Conference (Industrial Track), Rennes,
   FRANCE, DEC 10-14, 2018}},
Organization = {{Assoc Comp Machinery; Int Federat Informat Proc; USENIX}},
Abstract = {{With the wide adoption of the Internet, digital transactions surge
   exponentially and so do the impersonation fraud. While machine learning
   techniques show strong promise to be the building block for digital
   fraud detection systems, clients may be reluctant to share the raw data
   with such systems due to privacy concerns. The emerging privacy
   preserving machine learning techniques that employ homomorphic
   encryption to resolve this conundrum unfortunately increases the
   computational overhead of detection. In this paper, we present a
   first-of-a-kind empirical performance study of a private fraud detection
   system developed at SiS ID, a French business security platform. A
   privacy-preserving decision tree which can classify transactions into
   four risk classes (safe, moderately risky, very risky and fraud) is
   trained on more than 160000 real world transactions, and we
   quantitatively compare the classification accuracy, latency and network
   bandwidth under various combinations of encryption parameters and
   learning hyper-parameters, in order to explore the impact of the
   configuration on the performances. Our results show that the computation
   and communication overhead of processing encrypted data increases by an
   order of magnitude of 5, and highly depends on the configuration of the
   encryption key and the number of nodes in the decision tree.}},
DOI = {{10.1145/3284028.3284032}},
ISBN = {{978-1-4503-6016-6}},
Unique-ID = {{ISI:000470974900004}},
}

@inproceedings{ ISI:000468499300098,
Author = {Pham, Anh T. and Xi, Jing},
Editor = {{Abe, N and Liu, H and Pu, C and Hu, X and Ahmed, N and Qiao, M and Song, Y and Kossmann, D and Liu, B and Lee, K and Tang, J and He, J and Saltz, J}},
Title = {{Differentially Private Semi-Supervised Learning With Known Class Priors}},
Booktitle = {{2018 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA)}},
Series = {{IEEE International Conference on Big Data}},
Year = {{2018}},
Pages = {{801-810}},
Note = {{IEEE International Conference on Big Data (Big Data), Seattle, WA, DEC
   10-13, 2018}},
Organization = {{IEEE; IEEE Comp Soc; Expedia Grp; Baidu; Squirrel AI Learning; Ankura;
   Springer}},
Abstract = {{Machine learning algorithms benefit greatly from a large amount of data
   in the big data regime, however, the labeling cost for all training
   samples is prohibitively high. Semi-supervised learning is a designed
   framework to overcome the labeling challenge. In particular, in
   semi-supervised learning, there are only a small number of labeled
   samples and unlabeled samples are used as side information to leverage
   the overall performance. Additionally, preserving the privacy of the
   data is another challenge in big data analysis. This paper addresses
   challenges by designing a privacy-preserving method for semi-supervised
   learning. Assuming that class priors are available, the proposed method
   can achieve the optimal classifier. Moreover, the privacy and utility of
   the proposed method are theoretically guaranteed. Experiments on real
   UCI datasets illustrate the effectiveness of our proposed method.}},
ISSN = {{2639-1589}},
ISBN = {{978-1-5386-5035-6}},
ResearcherID-Numbers = {{Pham, Anh T./J-5909-2015}},
Unique-ID = {{ISI:000468499300098}},
}

@inproceedings{ ISI:000468499301003,
Author = {Begoli, Edmon and Brown, Kris and Srinivas, Sudarshan and Tamang,
   Suzanne},
Editor = {{Abe, N and Liu, H and Pu, C and Hu, X and Ahmed, N and Qiao, M and Song, Y and Kossmann, D and Liu, B and Lee, K and Tang, J and He, J and Saltz, J}},
Title = {{SynthNotes: A Generator Framework for High-volume, High-fidelity
   Synthetic Mental Health Notes}},
Booktitle = {{2018 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA)}},
Series = {{IEEE International Conference on Big Data}},
Year = {{2018}},
Pages = {{951-958}},
Note = {{IEEE International Conference on Big Data (Big Data), Seattle, WA, DEC
   10-13, 2018}},
Organization = {{IEEE; IEEE Comp Soc; Expedia Grp; Baidu; Squirrel AI Learning; Ankura;
   Springer}},
Abstract = {{One of the key, emerging challenges that connects the ``Big Data{''} and
   the AI domain is the availability of sufficient volumes of training data
   for AI/Machine Learning tasks. SynthNotes is a framework for generating
   standards-compliant, realistic mental health progress report notes at
   the very large, population-level scale, and in a strict
   privacy-preserving manner. Our framework, inspired by the needs to
   explore, evaluate, and train computational methods for the emerging
   mental health crisis in the US, is useful for benchmarking,
   optimization, and training of biomedical natural language processing,
   information extraction, and machine learning systems intended to operate
   at ``Big Data{''} scale (billions of notes). The free text notes
   generated by SynthNotes are based on the literature and public
   statistical models allowing for realistic, natural language
   representation of a patient, and his or her mental health
   characteristics. Additionally, SynthNotes can partially simulate
   stylistic, grammatical, and expressive characteristics of a licensed
   mental health professional. SynthNotes is modular and flexible, allowing
   for representation of variety of conditions, incorporation of
   alternative foundational models, and parametrization of the variability
   of the structure, content, and size of the synthetically generated
   corpus. In this paper, we report on the initial use and performance
   characteristics of our SynthNotes framework and on the ongoing work for
   inclusion of content planning and deep learning-based generative methods
   trained on real data.}},
ISSN = {{2639-1589}},
ISBN = {{978-1-5386-5035-6}},
Unique-ID = {{ISI:000468499301003}},
}

@inproceedings{ ISI:000468499301034,
Author = {Chen, Xuhui and Ji, Jinlong and Luo, Changqing and Liao, Weixian and Li,
   Pan},
Editor = {{Abe, N and Liu, H and Pu, C and Hu, X and Ahmed, N and Qiao, M and Song, Y and Kossmann, D and Liu, B and Lee, K and Tang, J and He, J and Saltz, J}},
Title = {{When Machine Learning Meets Blockchain: A Decentralized,
   Privacy-preserving and Secure Design}},
Booktitle = {{2018 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA)}},
Series = {{IEEE International Conference on Big Data}},
Year = {{2018}},
Pages = {{1178-1187}},
Note = {{IEEE International Conference on Big Data (Big Data), Seattle, WA, DEC
   10-13, 2018}},
Organization = {{IEEE; IEEE Comp Soc; Expedia Grp; Baidu; Squirrel AI Learning; Ankura;
   Springer}},
Abstract = {{With the onset of the big data era, designing efficient and effective
   machine learning algorithms to analyze large-scale data is in dire need.
   In practice, data is typically generated by multiple parties and stored
   in a geographically distributed manner, which spurs the study of
   distributed machine learning. Traditional master-worker type of
   distributed machine learning algorithms assumes a trusted central server
   and focuses on the privacy issue in linear learning models, while
   privacy in nonlinear learning models and security issues are not well
   studied. To address these issues, in this paper, we explore the
   blockchain technique to propose a decentralized privacy-preserving and
   secure machine learning system, called LearningChain, by considering a
   general (linear or nonlinear) learning model and without a trusted
   central server. Specifically, we design a decentralized Stochastic
   Gradient Descent (SGD) algorithm to learn a general predictive model
   over the blockchain. In decentralized SGD, we develop differential
   privacy based schemes to protect each party's data privacy, and propose
   an l-nearest aggregation algorithm to protect the system from potential
   Byzantine attacks. We also conduct theoretical analysis on the privacy
   and security of the proposed LearningChain. Finally, we implement
   LearningChain on Etheurum and demonstrate its efficiency and
   effectiveness through extensive experiments.}},
ISSN = {{2639-1589}},
ISBN = {{978-1-5386-5035-6}},
Unique-ID = {{ISI:000468499301034}},
}

@inproceedings{ ISI:000468499302062,
Author = {Fritchman, Kyle and Saminathan, Keerthanaa and Dowsley, Rafael and
   Hughes, Tyler and De Cock, Martine and Nascimento, Anderson and
   Teredesai, Ankur},
Editor = {{Abe, N and Liu, H and Pu, C and Hu, X and Ahmed, N and Qiao, M and Song, Y and Kossmann, D and Liu, B and Lee, K and Tang, J and He, J and Saltz, J}},
Title = {{Privacy-Preserving Scoring of Tree Ensembles: A Novel Framework for AI
   in Healthcare}},
Booktitle = {{2018 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA)}},
Series = {{IEEE International Conference on Big Data}},
Year = {{2018}},
Pages = {{2413-2422}},
Note = {{IEEE International Conference on Big Data (Big Data), Seattle, WA, DEC
   10-13, 2018}},
Organization = {{IEEE; IEEE Comp Soc; Expedia Grp; Baidu; Squirrel AI Learning; Ankura;
   Springer}},
Abstract = {{Machine Learning (ML) techniques now impact a wide variety of domains.
   Highly regulated industries such as healthcare and finance have
   stringent compliance and data governance policies around data sharing.
   Advances in secure multiparty computation (SMC) for privacy-preserving
   machine learning (PPML) can help transform these regulated industries by
   allowing ML computations over encrypted data with personally
   identifiable information (PII). Yet very little of SMC-based PPML has
   been put into practice so far. In this paper we present the very first
   framework for privacy-preserving classification of tree ensembles with
   application in healthcare. We first describe the underlying
   cryptographic protocols that enable a healthcare organization to send
   encrypted data securely to a ML scoring service and obtain encrypted
   class labels without the scoring service actually seeing that input in
   the clear. We then describe the deployment challenges we solved to
   integrate these protocols in a cloud based scalable risk-prediction
   platform with multiple ML models for healthcare AI. Included are system
   internals, and evaluations of our deployment for supporting physicians
   to drive better clinical outcomes in an accurate, scalable, and provably
   secure manner. To the best of our knowledge, this is the first such
   applied framework with SMC-based privacy-preserving machine learning for
   healthcare.}},
ISSN = {{2639-1589}},
ISBN = {{978-1-5386-5035-6}},
Unique-ID = {{ISI:000468499302062}},
}

@inproceedings{ ISI:000465363900715,
Author = {Teixeira, Francisco and Abad, Alberto and Trancoso, Isabel},
Book-Group-Author = {{Int Speech Commun Assoc}},
Title = {{Patient Privacy in Paralinguistic Tasks}},
Booktitle = {{19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES}},
Series = {{Interspeech}},
Year = {{2018}},
Pages = {{3428-3432}},
Note = {{19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018),
   Hyderabad, INDIA, AUG 02-SEP 06, 2018}},
Organization = {{Int Speech Commun Assoc}},
Abstract = {{Recent developments in cryptography and, in particular in Fully
   Homomorphic Encryption (FHE), have allowed for the development of new
   privacy preserving machine learning schemes. In this paper, we show how
   these schemes can be applied to the automatic assessment of speech
   affected by medical conditions, allowing for patient privacy in
   diagnosis and monitoring scenarios. More specifically, we present
   results for the assessment of the degree of Parkinsons Disease, the
   detection of a Cold, and both the detection and assessment of the degree
   of Depression. To this end, we use a neural network in which all
   operations are performed in an FHE context. This implies replacing the
   activation functions by linear and second degree polynomials, as only
   additions and multiplications are viable. Furthermore, to guarantee that
   the inputs of these activation functions fall within the convergence
   interval of the approximation, a batch normalization layer is introduced
   before each activation function. After training the network with
   unencrypted data, the resulting model is then employed in an encrypted
   version of the network, to produce encrypted predictions. Our tests show
   that the use of this framework yields results with little to no
   performance degradation, in comparison to the baselines produced for the
   same datasets.}},
DOI = {{10.21437/Interspeech.2018-2186}},
ISSN = {{2308-457X}},
ISBN = {{978-1-5108-7221-9}},
ResearcherID-Numbers = {{Trancoso, Isabel/C-5965-2008}},
ORCID-Numbers = {{Trancoso, Isabel/0000-0001-5874-6313}},
Unique-ID = {{ISI:000465363900715}},
}

@inproceedings{ ISI:000468160600160,
Author = {Heyburn, Rachel and Bond, Raymond R. and Black, Michaela and Mulvenna,
   Maurice and Wallace, Jonathan and Rankin, Deborah and Cleland, Brian},
Editor = {{Liu, J and Lu, J and Xu, Y and Martinez, L and Kerre, EE}},
Title = {{Machine learning using synthetic and real data: Similarity of evaluation
   metrics for different healthcare datasets and for different algorithms}},
Booktitle = {{DATA SCIENCE AND KNOWLEDGE ENGINEERING FOR SENSING DECISION SUPPORT}},
Series = {{World Scientific Proceedings Series on Computer Engineering and
   Information Science}},
Year = {{2018}},
Volume = {{11}},
Pages = {{1281-1291}},
Note = {{13th International Conference on Fuzzy Logic and Intelligent
   Technologies in Nuclear Science (FLINS), Belfast, IRELAND, AUG 21-24,
   2018}},
Organization = {{Ulster Univ; Belfast City Council; Tourism NI; IEEE UK \& I Sect; IEEE
   Syst Man \& Cybernet Soc; SW Jiaotong Univ; Chinese Soc Log, Natl Assoc
   Non Class Log \& Computat; IEEE Syst Man \& Cybernet Soc Ireland
   Chapter; European Soc Fuzzy Log \& Technol; Int Fuzzy Syst Assoc}},
Abstract = {{Sharing data is often a risk in terms of security and privacy especially
   if the data is sensitive. Algorithms can be used to generate synthetic
   data from an original raw dataset in order to share data that are
   considered more `privacy preserving', and that increase the level of
   anonymity. In this paper, we carry out an experiment to study the
   validity of conducting machine learning on synthetic data. We compare
   the evaluation metrics produced from machine learning models that were
   trained using synthetic data with metrics yielded from machine learning
   models that were trained using the corresponding real data.}},
ISBN = {{978-981-3273-22-1}},
Unique-ID = {{ISI:000468160600160}},
}

@inproceedings{ ISI:000462968100271,
Author = {Alsulaimawi, Zahir and Kim, Jinsub and Thinh Nguyen},
Book-Group-Author = {{IEEE}},
Title = {{SEQUENTIAL GAME NETWORK (SEGANE) WITH APPLICATION TO ONLINE DATA
   SANITIZATION}},
Booktitle = {{2018 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING
   (GLOBALSIP 2018)}},
Series = {{IEEE Global Conference on Signal and Information Processing}},
Year = {{2018}},
Pages = {{1326-1330}},
Note = {{IEEE Global Conference on Signal and Information Processing (GlobalSIP),
   Anaheim, CA, NOV 26-29, 2018}},
Organization = {{Inst Elect \& Elect Engineers; IEEE Signal Proc Soc}},
Abstract = {{This paper proposes SEquential GAme NEtwork (SEGANE), a novel deep
   neural network (DNN) architecture for optimizing the performance of
   machine learning applications with multiple competing objectives.
   Specifically, SEGANE is evaluated in the context of data sanitization
   which aims to remove any pre-specified private information from the data
   in real time while keeping the relevant information used to improve the
   inference accuracy about the non-private information. In some settings,
   preserving private information and improving inference performance about
   non-private information are competing objectives. In such cases, SEGANE
   provides a sequential game framework and algorithmic tools to implement
   data sanitization schemes with flexible trade-off between these two
   objectives. We use two datasets: MNIST (hand-written digits) and IMDB
   (gender and age) to evaluate SEGANE. For MNIST, even numbers are
   considered private while numbers larger than 10 are considered
   nonprivate. For IMDB, in one setting, gender is considered private while
   age is non-private, and vice versa in another setting. Our experimental
   results on these datasets show that SEGANE is highly effective in
   removing private information from the dataset while allowing nonprivate
   data to be mined effectively.}},
ISSN = {{2376-4066}},
ISBN = {{978-1-7281-1295-4}},
Unique-ID = {{ISI:000462968100271}},
}

@inproceedings{ ISI:000462352800008,
Author = {Chen, Guoming and Chen, Qiang and Zhu, Xiongyong and Chen, Yiqun},
Editor = {{Wang, R and Luo, Z and Yin, B and Tan, J}},
Title = {{Encrypted Image Feature Extraction By Privacy-Preserving MFS}},
Booktitle = {{2018 7TH INTERNATIONAL CONFERENCE ON DIGITAL HOME (ICDH 2018)}},
Series = {{International Conference on Digital Home}},
Year = {{2018}},
Pages = {{42-45}},
Note = {{7th International Conference on Digital Home (ICDH), Guilin Univ Elect
   Technol, Guilin, PEOPLES R CHINA, NOV 30-DEC 01, 2018}},
Organization = {{IEEE Comp Soc; Dalian Univ Technol; Sun Yat Sen Univ; Beijing Univ
   Technol; Hefei Univ Technol; Natl Nat Sci Fdn China}},
Abstract = {{Privacy preserve machine learning is a hot topic in multimedia domain.
   In this paper, we propose a secure multifractal feature extraction and
   representation method in the encrypted domain. We first use chaotic
   sequence to scramble the image in a block wise way, then according to
   the characteristic of chaotic sequence which preserves locally the
   randomness and maintain special periodicity we propose a multifractal
   feature extraction method in the encrypted domain. Experimental results
   showed that multifractal feature has a good distinguish ability in the
   encrypted domain.}},
DOI = {{10.1109/ICDH.2018.00016}},
ISSN = {{2372-7160}},
ISBN = {{978-1-5386-9496-1}},
Unique-ID = {{ISI:000462352800008}},
}

@inproceedings{ ISI:000462706700024,
Author = {Pozi, Muhammad Syafiq Mohd and Abu Bakar, Asmidar and Ismail, Roslan and
   Yussof, Salman and Rahim, Fiza Abdul and Ramli, Ramona},
Book-Group-Author = {{IEEE}},
Title = {{Shifting Dataset To Preserve Data Privacy}},
Booktitle = {{2018 IEEE CONFERENCE ON E-LEARNING, E-MANAGEMENT AND E-SERVICES (IC3E)}},
Year = {{2018}},
Pages = {{134-139}},
Note = {{IEEE Conference on e-Learning, e-Management and e-Services (IC3e),
   Langkawi, MALAYSIA, NOV 21-22, 2018}},
Organization = {{IEEE; IEEE Comp Soc}},
Abstract = {{Data analytic is very valuable in any domain that produces large amount
   of data making demands on full datasets to be revealed for analytic
   purposes are rising. Regardless, the privacy of the released dataset
   should be preserved. New techniques using synthetic data as a mean to
   preserve the privacy has been identified as appropriate approach to
   fulfill the demand. In this paper, a privacy-preserving data synthetic
   framework for data analytic is proposed. Using a generative model that
   captures the density function of data attributes, the privacy-preserving
   synthetic data is produced. We performed classification task through
   various machine learning classifiers in measuring the data utility of
   the new privacy-preserving synthesized data.}},
ISBN = {{978-1-5386-7263-1}},
Unique-ID = {{ISI:000462706700024}},
}

@inproceedings{ ISI:000461852005005,
Author = {Frongillo, Rafael and Waggoner, Bo},
Editor = {{Bengio, S and Wallach, H and Larochelle, H and Grauman, K and CesaBianchi, N and Garnett, R}},
Title = {{Bounded-Loss Private Prediction Markets}},
Booktitle = {{ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)}},
Series = {{Advances in Neural Information Processing Systems}},
Year = {{2018}},
Volume = {{31}},
Note = {{32nd Conference on Neural Information Processing Systems (NIPS),
   Montreal, CANADA, DEC 02-08, 2018}},
Abstract = {{Prior work has investigated variations of prediction markets that
   preserve participants' (differential) privacy, which formed the basis of
   useful mechanisms for purchasing data for machine learning objectives.
   Such markets required potentially unlimited financial subsidy, however,
   making them impractical. In this work, we design an adaptively-growing
   prediction market with a bounded financial subsidy, while achieving
   privacy, incentives to produce accurate predictions, and precision in
   the sense that market prices are not heavily impacted by the added
   privacy-preserving noise. We briefly discuss how our mechanism can
   extend to the data-purchasing setting, and its relationship to
   traditional learning algorithms.}},
ISSN = {{1049-5258}},
Unique-ID = {{ISI:000461852005005}},
}

@inproceedings{ ISI:000462054900013,
Author = {Wang, Sen and Chang, J. Morris},
Book-Group-Author = {{IEEE}},
Title = {{Differentially Private Principal Component Analysis Over Horizontally
   Partitioned Data}},
Booktitle = {{2018 IEEE CONFERENCE ON DEPENDABLE AND SECURE COMPUTING (DSC)}},
Year = {{2018}},
Pages = {{114-121}},
Note = {{IEEE Conference on Dependable and Secure Computing (DSC), Kaohsiung,
   TAIWAN, DEC 10-13, 2018}},
Organization = {{IEEE; Minist Sci \& Technol, Sci Commun; Natl Chiao Tung Univ, Taiwan
   Informat Secur Ctr; Hacker Coll; Bur Foreign Trade; Minist Educ; Natl
   Sun Yat Sen Univ}},
Abstract = {{Principal Component Analysis (PCA) is widely adopted in various data
   mining and machine learning applications, it computes a low dimension
   subspace that captures the most variances of the underlying data. The
   area of distributed computing provides a promising domain for PCA, where
   it has been studied in many fields. In big data era, large volume and
   high dimensional data are generated at all times. For instance, mobile
   devices become the important producer and carrier for personal
   information, which can provide a considerable social utility. However,
   the current distributed PCA protocol cannot provide the efficiency and
   scalability with respect to such large amounts of data. Furthermore, the
   privacy issue arises when data contains sensitive information. The data
   owner would not prefer to sharing the data in cleartext, and the
   inference from PCA should also be prevented. Motivated to resolve these
   challenges, in this paper, we design and implement a highly efficient
   and largely scalable privacy preserving distributed PCA protocol, in
   which the (epsilon;delta)-Differential Privacy is guaranteed. In the
   experiments, we evaluate the protocol in terms of efficiency and
   utility, and shows that it maintains a high data utility while
   preserving the privacy.}},
ISBN = {{978-1-5386-5790-4}},
Unique-ID = {{ISI:000462054900013}},
}

@inproceedings{ ISI:000461315900004,
Author = {Mohassel, Payman and Rindal, Peter},
Book-Group-Author = {{ACM}},
Title = {{ABY(3): A Mixed Protocol Framework for Machine Learning}},
Booktitle = {{PROCEEDINGS OF THE 2018 ACM SIGSAC CONFERENCE ON COMPUTER AND
   COMMUNICATIONS SECURITY (CCS'18)}},
Year = {{2018}},
Pages = {{35-52}},
Note = {{ACM SIGSAC Conference on Computer and Communications Security (CCS),
   Toronto, CANADA, OCT 15-19, 2018}},
Organization = {{Assoc Comp Machinery; ACM SIGSAC; NSF; ANT Financil; Baidu; Cisco;
   Samsung Res; BlackBerry; Facebook; IBM Res; Ledger; SAP; Visa Res}},
Abstract = {{Machine learning is widely used to produce models for a range of
   applications and is increasingly offered as a service by major
   technology companies. However, the required massive data collection
   raises privacy concerns during both training and prediction stages.
   In this paper, we design and implement a general framework for
   privacy-preserving machine learning and use it to obtain new solutions
   for training linear regression, logistic regression and neural network
   models. Our protocols are in a three-server model wherein data owners
   secret share their data among three servers who train and evaluate
   models on the joint data using three-party computation (3PC).
   Our main contribution is a new and complete framework (ABY(3)) for
   efficiently switching back and forth between arithmetic, binary, and Yao
   3PC which is of independent interest. Many of the conversions are based
   on new techniques that are designed and optimized for the first time in
   this paper. We also propose new techniques for fixed-point
   multiplication of shared decimal values that extends beyond the
   three-party case, and customized protocols for evaluating piecewise
   polynomial functions. We design variants of each building block that is
   secure against malicious adversaries who deviate arbitrarily. We
   implement our system in C++. Our protocols are up to four orders of
   magnitude faster than the best prior work, hence significantly reducing
   the gap between privacy-preserving and plaintext training.}},
DOI = {{10.1145/3243734.3243760}},
ISBN = {{978-1-4503-5693-0}},
Unique-ID = {{ISI:000461315900004}},
}

@inproceedings{ ISI:000461315900138,
Author = {Bayatbabolghani, Fattaneh and Blanton, Marina},
Book-Group-Author = {{ACM}},
Title = {{Secure Multi-Party Computation}},
Booktitle = {{PROCEEDINGS OF THE 2018 ACM SIGSAC CONFERENCE ON COMPUTER AND
   COMMUNICATIONS SECURITY (CCS'18)}},
Year = {{2018}},
Pages = {{2157-2159}},
Note = {{ACM SIGSAC Conference on Computer and Communications Security (CCS),
   Toronto, CANADA, OCT 15-19, 2018}},
Organization = {{Assoc Comp Machinery; ACM SIGSAC; NSF; ANT Financil; Baidu; Cisco;
   Samsung Res; BlackBerry; Facebook; IBM Res; Ledger; SAP; Visa Res}},
Abstract = {{Secure multi-party computation (SMC) is an emerging topic which has been
   drawing growing attention during recent decades. There are many examples
   which show importance of SMC constructions in practice, such as
   privacy-preserving decision making and machine learning, auctions,
   private set intersection, and others. In this tutorial, we provide a
   comprehensive coverage of SMC techniques, starting from precise
   definitions and fundamental techniques. Consequently, a significant
   portion of the tutorial focuses on recent advances in general SMC
   constructions. We cover garbled circuit evaluation (GCE) and linear
   secret sharing (LSS) which are commonly used for secure two-party and
   multi-party computation, respectively. The coverage includes both
   standard adversarial models: semi-honest and malicious.
   For GCE, we start with the original Yao's garbled circuits construction
   {[}30] for semi-honest adversaries and consequently cover its recent
   optimizations such as the ``free XOR,{''} the garbled row reduction, the
   half-gates optimization, and the use of AES NI techniques. We follow
   with a discussion of techniques for making GCE resilient to malicious
   behavior, which includes the cut-and-choose approach and additional
   techniques to deter known attacks in the presence of malicious
   participants. In addition, we include the-state-of-the-art protocols for
   oblivious transfer (OT) and OT extension in the presence of semi-honest
   and malicious users.
   For LSS, we start from standard solutions for the semi-honest
   adversarial model including {[}5, 28] and consequently move to recent
   efficient constructions for semi-honest and malicious adversarial
   models. The coverage includes different types of corruption thresholds
   (with and without honest majority), which imply different guarantees
   with respect to abort.}},
DOI = {{10.1145/3243734.3264419}},
ISBN = {{978-1-4503-5693-0}},
Unique-ID = {{ISI:000461315900138}},
}

@inproceedings{ ISI:000461315900177,
Author = {Pejo, Balazs and Tang, Qiang and Biczok, Gergely},
Book-Group-Author = {{ACM}},
Title = {{The Price of Privacy in Collaborative Learning}},
Booktitle = {{PROCEEDINGS OF THE 2018 ACM SIGSAC CONFERENCE ON COMPUTER AND
   COMMUNICATIONS SECURITY (CCS'18)}},
Year = {{2018}},
Pages = {{2261-2263}},
Note = {{ACM SIGSAC Conference on Computer and Communications Security (CCS),
   Toronto, CANADA, OCT 15-19, 2018}},
Organization = {{Assoc Comp Machinery; ACM SIGSAC; NSF; ANT Financil; Baidu; Cisco;
   Samsung Res; BlackBerry; Facebook; IBM Res; Ledger; SAP; Visa Res}},
Abstract = {{Machine learning algorithms have reached mainstream status and are
   widely deployed in many applications. The accuracy of such algorithms
   depends significantly on the size of the underlying training dataset; in
   reality a small or medium sized organization often does not have enough
   data to train a reasonably accurate model. For such organizations, a
   realistic solution is to train machine learning models based on a joint
   dataset (which is a union of the individual ones). Unfortunately,
   privacy concerns prevent them from straightforwardly doing so. While a
   number of privacy-preserving solutions exist for collaborating
   organizations to securely aggregate the parameters in the process of
   training the models, we are not aware of any work that provides a
   rational framework for the participants to precisely balance the privacy
   loss and accuracy gain in their collaboration.
   In this paper, we model the collaborative training process as a
   two-player game where each player aims to achieve higher accuracy while
   preserving the privacy of its own dataset. We introduce the notion of
   Price of Privacy, a novel approach for measuring the impact of privacy
   protection on the accuracy in the proposed framework. Furthermore, we
   develop a game-theoretical model for different player types, and then
   either find or prove the existence of a Nash Equilibrium with regard to
   the strength of privacy protection for each player.}},
DOI = {{10.1145/3243734.3278525}},
ISBN = {{978-1-4503-5693-0}},
Unique-ID = {{ISI:000461315900177}},
}

@inproceedings{ ISI:000461315900192,
Author = {Wang, Jun and Arriaga, Afonso and Tang, Qiang and Ryan, Peter Y. A.},
Book-Group-Author = {{ACM}},
Title = {{Facilitating Privacy-preserving Recommendation-as-a-Service with Machine
   Learning}},
Booktitle = {{PROCEEDINGS OF THE 2018 ACM SIGSAC CONFERENCE ON COMPUTER AND
   COMMUNICATIONS SECURITY (CCS'18)}},
Year = {{2018}},
Pages = {{2306-2308}},
Note = {{ACM SIGSAC Conference on Computer and Communications Security (CCS),
   Toronto, CANADA, OCT 15-19, 2018}},
Organization = {{Assoc Comp Machinery; ACM SIGSAC; NSF; ANT Financil; Baidu; Cisco;
   Samsung Res; BlackBerry; Facebook; IBM Res; Ledger; SAP; Visa Res}},
Abstract = {{Machine-Learning-as-a-Service has become increasingly popular, with
   Recommendation-as-a-Service as one of the representative examples. In
   such services, providing privacy protection for the users is an
   important topic. Reviewing privacy-preserving solutions which were
   proposed in the past decade, privacy and machine learning are often seen
   as two competing goals at stake. Though improving cryptographic
   primitives (e.g., secure multi-party computation (SMC) or homomorphic
   encryption (HE)) or devising sophisticated secure protocols has made a
   remarkable achievement, but in conjunction with state-of-the-art
   recommender systems often yields far-from-practical solutions.
   We tackle this problem from the direction of machine learning. We aim to
   design crypto-friendly recommendation algorithms, thus to obtain
   efficient solutions by directly using existing cryptographic tools. In
   particular, we propose an HE-friendly recommender system, referred to as
   CryptoRec, which (1) decouples user features from latent feature space,
   avoiding training the recommendation model on encrypted data; (2) only
   relies on addition and multiplication operations, making the model
   straightforwardly compatible with HE schemes. The properties turn
   recommendation computations into a simple matrix-multiplication
   operation. To further improve efficiency, we introduce a
   sparse-quantization-reuse method which reduces the
   recommendation-computation time by 9x (compared to using CryptoRec
   directly), without compromising the accuracy.
   We demonstrate the efficiency and accuracy of CryptoRec on three
   real-world datasets. CryptoRec allows a server to estimate a user's
   preferences on thousands of items within a few seconds on a single PC,
   with the user's data homomorphically encrypted, while its prediction
   accuracy is still competitive with state-of-the-art recommender systems
   computing over clear data. Our solution enables
   Recommendation-as-a-Service on large datasets in a nearly real-time
   (seconds) level.}},
DOI = {{10.1145/3243734.3278504}},
ISBN = {{978-1-4503-5693-0}},
Unique-ID = {{ISI:000461315900192}},
}

@inproceedings{ ISI:000446034500015,
Author = {Hussain, Siam U. and Rouhani, Bita Darvish and Ghasemzadeh, Mohammad and
   Koushanfar, Farinaz},
Book-Group-Author = {{IEEE}},
Title = {{MAXelerator: FPGA Accelerator for Privacy Preserving Multiply-Accumulate
   (MAC) on Cloud Servers}},
Booktitle = {{2018 55TH ACM/ESDA/IEEE DESIGN AUTOMATION CONFERENCE (DAC)}},
Year = {{2018}},
Note = {{55th ACM/ESDA/IEEE Design Automation Conference (DAC), San Francisco,
   CA, JUN 24-28, 2018}},
Organization = {{IEEE; ACM; ESDA}},
Abstract = {{This paper presents MAXelerator, the first hardware accelerator for
   privacy-preserving machine learning (ML) on cloud servers. Cloud-based
   ML is being increasingly employed in various data sensitive scenarios.
   While it enhances both efficiency and quality of the service, it also
   raises concern about privacy of the users' data. We create a practical
   privacy-preserving solution for matrix-based ML on cloud servers. We
   show that for the majority of the ML applications, the privacy-sensitive
   computation boils down to either matrix multiplication, which is a
   repetition of Multiply-Accumulate (MAC) or the MAC itself We design an
   FPGA architecture for privacy-preserving MAC to accelerate the ML
   computation based on the well known Secure Function Evaluation protocol
   named Yao's Garbled Circuit. MAXelerator demonstrates up to 57x
   improvement in throughput per core compared to the fastest existing GC
   framework. We corroborate the effectiveness of the accelerator with
   real-world case studies in privacy-sensitive scenarios.}},
DOI = {{10.1145/3195970.3196074}},
ISBN = {{978-1-4503-5700-5}},
Unique-ID = {{ISI:000446034500015}},
}

@inproceedings{ ISI:000458654000195,
Author = {Lee, Sujee and Nemati, Ebrahim and Kuang, Jilong},
Editor = {{Zheng, H and Callejas, Z and Griol, D and Wang, H and Hu, X and Schmidt, H and Baumbach, J and Dickerson, J and Zhang, L}},
Title = {{Configurable Pulmonary-Tuned Privacy Preservation Algorithm for Mobile
   Devices}},
Booktitle = {{PROCEEDINGS 2018 IEEE INTERNATIONAL CONFERENCE ON BIOINFORMATICS AND
   BIOMEDICINE (BIBM)}},
Series = {{IEEE International Conference on Bioinformatics and Biomedicine-BIBM}},
Year = {{2018}},
Pages = {{1107-1112}},
Note = {{IEEE International Conference on Bioinformatics and Biomedicine (BIBM),
   Madrid, SPAIN, DEC 03-06, 2018}},
Organization = {{Ulster Univ; Univ Granada; Univ Carlos III Madrid; TCCLS; IEEE; IEEE
   Comp Soc; Natl Sci Fdn; Syst Med; Mary Ann Liebert Inc publishers}},
Abstract = {{Audio-based automated pulmonary symptom detection has the potential to
   offer accurate and continuous assessment of patients with lung disease.
   However, privacy preservation becomes a significant issue when it comes
   to continuous passive audio recording. Various techniques have been
   employed to obfuscate the speech within audio in these applications.
   However, that penalizes the accuracy of detection by affecting
   of-interest, non-speech audio parts. This is inevitably undesirable as
   it contradicts the notion of sensing. In this paper, we propose a novel
   algorithm to achieve the goal by employing a machine-learning-based
   vowel detection algorithm. The algorithm is implemented in a
   configurable manner to address many different scenarios of data
   collection. Logistic regression has been utilized to make the algorithm
   feasible for on-device implementation. We have shown that our algorithm
   achieves the goals in that the obfuscated speech is unrecognizable while
   cough sounds are identifiable by symptom detection models as well as a
   human ear.}},
ISSN = {{2156-1125}},
EISSN = {{2156-1133}},
ISBN = {{978-1-5386-5488-0}},
Unique-ID = {{ISI:000458654000195}},
}

@inproceedings{ ISI:000458665600055,
Author = {Nakachi, Takayuki and Ishihara, Hiroyuki and Kiya, Hitoshi},
Editor = {{Wysocki, TA and Wysocki, BJ}},
Title = {{Privacy-preserving Network BMI Decoding of Covert Spatial Attention}},
Booktitle = {{2018 12TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND
   COMMUNICATION SYSTEMS (ICSPCS)}},
Year = {{2018}},
Note = {{12th International Conference on Signal Processing and Communication
   Systems (ICSPCS), Cairns, AUSTRALIA, DEC 17-19, 2018}},
Organization = {{IEEE; IEEE Commun Soc; Univ Nebraskas, Peter Keiwit Inst}},
Abstract = {{The brain-machine interface (BMI) has attracted much attention in the
   fields of biomedical engineering and ICT human communications. Of
   particular interest, neural decoding methods have rapidly developed over
   the last decade in neuroscience, allowing us to estimate the contents of
   human perception and subjective mental states by capturing brain
   activity patterns. However, the development of neural decoding will
   generate significant concern about privacy violation. In this
   manuscript, we propose a secure network BMI decoding method based on
   sparse coding for a covert spatial attention task. It is shown that
   secure sparse coding enables us to not only protect observed EEG
   signals, but also achieve the same estimation performance as that
   offered by sparse coding with unprotected observed signals.}},
ISBN = {{978-1-5386-5602-0}},
Unique-ID = {{ISI:000458665600055}},
}

@inproceedings{ ISI:000458674900008,
Author = {Gustavo Esquivel-Quiros, Luis and Gabriela Barrantes, Elena and Esponda
   Darlington, Fernando},
Editor = {{Mata, MAM and Miranda, JM and Pereznegron, AP and Alorhernandez, G and Carrillo, AYQ}},
Title = {{Measuring data privacy preserving and machine learning}},
Booktitle = {{2018 7TH INTERNATIONAL CONFERENCE ON SOFTWARE PROCESS IMPROVEMENT
   (CIMPS): APPLICATIONS IN SOFTWARE ENGINEERING}},
Year = {{2018}},
Pages = {{85-94}},
Note = {{7th International Conference on Software Process Improvement (CIMPS),
   CUCEI, Guadalajara, MEXICO, OCT 17-19, 2018}},
Organization = {{IEEE; Centro Investigacion Matematicas A C}},
Abstract = {{The increasing publication of large amounts of data, theoretically
   anonymous, can lead to a number of attacks on the privacy of people. The
   publication of sensitive data without exposing the data owners is
   generally not part of the software developers concerns. The regulations
   for the data privacy-preserving create an appropriate scenario to focus
   on privacy from the perspective of the use or data exploration that
   takes place in an organization. The increasing number of sanctions for
   privacy violations motivates the systematic comparison of three known
   machine learning algorithms in order to measure the usefulness of the
   data privacy preserving. The scope of the evaluation is extended by
   comparing them with a known privacy preservation metric. Different
   parameter scenarios and privacy levels are used. The use of publicly
   available implementations, the presentation of the methodology,
   explanation of the experiments and the analysis allow providing a
   framework of work on the problem of the preservation of privacy.
   Problems are shown in the measurement of the usefulness of the data and
   its relationship with the privacy preserving. The findings motivate the
   need to create optimized metrics on the privacy preferences of the
   owners of the data since the risks of predicting sensitive attributes by
   means of machine learning techniques are not usually eliminated. In
   addition, it is shown that there may be a hundred percent, but it cannot
   be measured. As well as ensuring adequate performance of machine
   learning models that are of interest to the organization that data
   publisher.}},
ISBN = {{978-1-7281-0158-3}},
ORCID-Numbers = {{Esquivel-Quiros, Luis Gustavo/0000-0003-1536-6710}},
Unique-ID = {{ISI:000458674900008}},
}

@inproceedings{ ISI:000457600600075,
Author = {Liu, Cong and Xiao, Mingjun and Zhao, Yaxiong},
Book-Group-Author = {{IEEE}},
Title = {{Scalable and Privacy Preserving Routing in Mobile Social Networks}},
Booktitle = {{2018 IEEE 15TH INTERNATIONAL CONFERENCE ON MOBILE AD HOC AND SENSOR
   SYSTEMS (MASS)}},
Series = {{IEEE International Conference on Mobile Ad-hoc and Sensor Systems}},
Year = {{2018}},
Pages = {{559-564}},
Note = {{15th IEEE International Conference on Mobile Ad Hoc and Sensor Systems
   (MASS), Chengdu, PEOPLES R CHINA, OCT 09-12, 2018}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Tech Comm Distributed Proc; Tech Comm Simulat;
   Natl Sci Fdn; Journal Sensor \& Actuator Networks}},
Abstract = {{Mobility-assisted opportunistic routing in mobile social network is an
   interesting research topic with real world applications. It allows users
   to exchange large chunks of data without relying on stationary network
   infrastructures. This paper proposes a scalability and privacy
   preserving routing algorithm. The core of a multi-hop routing scheme is
   the prediction of the next relay router, which ideally is the next hop
   on the shortest path to the destination of the message. The proposed
   routing algorithm contains a machine learning based prediction model
   that is trained on a trace of nodes connection events collected during a
   warm up period in a network where the mobility is assumed to exhibit a
   certain degree of regularity as in general human social networks. The
   algorithm is scalable since the prediction model, i.e. the control plane
   of the algorithm, only needs to maintain and broadcast the trained model
   that implicitly encodes the possible mobility locations and patterns
   within the mobility area, which is a constant to the number of nodes in
   the network. The algorithm is privacy preserving because that it does
   not require the nodes to disclose to other nodes any explicit
   information about its previous mobility or its personal preferences.
   Privacy preserving is achieved by using distributed node representations
   that jointly encode the mobility pattern of the node together with the
   prediction model, instead of using explicit statistical information to
   represent previous connectivity patterns as in prior work. This on one
   hand make attacker hard to find out any exact information about any
   particular node, and on the other hand enables the routing algorithm to
   extrapolate to the prediction of nodes of unseen mobility pattern, which
   is difficult if hand designed statistical mobility information is used.
   To perform large scale experiments, we collect very large synthetic
   mobility trace from university course registration information, this
   trace contain a total of about 35,000 nodes and 300 million message
   forwarding links. Experiments are conduct to examine several prediction
   accuracies of our algorithm.}},
DOI = {{10.1109/MASS.2018.00087}},
ISSN = {{2155-6806}},
ISBN = {{978-1-5386-5580-1}},
Unique-ID = {{ISI:000457600600075}},
}

@inproceedings{ ISI:000455774600056,
Author = {Wood, Alexander and Shpilrain, Vladimir and Najarian, Kayvan and
   Mostashari, Ali and Kahrobaei, Delaram},
Editor = {{Davenport, JH and Kauers, M and Labahn, G and Urban, J}},
Title = {{Private-Key Fully Homomorphic Encryption for Private Classification}},
Booktitle = {{MATHEMATICAL SOFTWARE - ICMS 2018}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2018}},
Volume = {{10931}},
Pages = {{475-481}},
Note = {{6th International Conference on Mathematical Software (ICMS), Univ Notre
   Dame, Dept Appl \& Computat Math \& Stat, South Bend, IN, JUL 24-27,
   2018}},
Abstract = {{Fully homomophic encryption enables private computation over sensitive
   data, such as medical data, via potentially quantum-safe primitives. In
   this extended abstract we provide an overview of an implementation of a
   private-key fully homomorphic encryption scheme in a protocol for
   private Naive Bayes classification. This protocol allows a data owner to
   privately classify her data point without direct access to the learned
   model. We implement this protocol by performing privacy-preserving
   classification of breast cancer data as benign or malignant.}},
DOI = {{10.1007/978-3-319-96418-8\_56}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-96418-8}},
ORCID-Numbers = {{Wood, Alexander/0000-0003-1729-2962}},
Unique-ID = {{ISI:000455774600056}},
}

@inproceedings{ ISI:000455808300017,
Author = {Jourdan, Theo and Boutet, Antoine and Frindel, Carole},
Book-Group-Author = {{Assoc Comp Machinery}},
Title = {{Toward privacy in IoT mobile devices for activity recognition}},
Booktitle = {{PROCEEDINGS OF THE 15TH EAI INTERNATIONAL CONFERENCE ON MOBILE AND
   UBIQUITOUS SYSTEMS: COMPUTING, NETWORKING AND SERVICES (MOBIQUITOUS
   2018)}},
Year = {{2018}},
Pages = {{155-165}},
Note = {{15th EAI International Conference on Mobile and Ubiquitous Systems -
   Computing, Networking and Services (Mobiquitous), New York City, NY, NOV
   05-07, 2018}},
Organization = {{EAI}},
Abstract = {{Recent advances in wireless sensors for personal healthcare allow to
   recognise human real-time activities with mobile devices. While the
   analysis of those datastream can have many benefits from a health point
   of view, it can also lead to privacy threats by exposing highly
   sensitive information. In this paper, we propose a privacy-preserving
   framework for activity recognition. This framework relies on a machine
   learning technique to efficiently recognise the user activity pattern,
   useful for personal healthcare monitoring, while limiting the risk of
   re-identification of users from biometric patterns that characterizes
   each individual. To achieve that, we first deeply analysed different
   features extraction schemes in both temporal and frequency domain. We
   show that features in temporal domain are useful to discriminate user
   activity while features in frequency domain lead to distinguish the user
   identity. On the basis of this observation, we second design a novel
   protection mechanism that processes the raw signal on the user's
   smartphone and transfers to the application server only the relevant
   features unlinked to the identity of users. In addition, a
   generalisation-based approach is also applied on features in frequency
   domain before to be transmitted to the server in order to limit the risk
   of re-identification. We extensively evaluate our framework with a
   reference dataset: results show an accurate activity recognition (87\%)
   while limiting the re-identifation rate (33\%). This represents a
   slightly decrease of utility (9\%) against a large privacy improvement
   (53\%) compared to state-of-the-art baselines.}},
DOI = {{10.1145/3286978.3287009}},
ISBN = {{978-1-4503-6093-7}},
ResearcherID-Numbers = {{Frindel, Carole/H-6484-2014}},
ORCID-Numbers = {{Frindel, Carole/0000-0003-4570-0994}},
Unique-ID = {{ISI:000455808300017}},
}

@inproceedings{ ISI:000455346400013,
Author = {Chen, Xi and Liu, Yiqun and Zhang, Liang and Kenthapadi, Krishnaram},
Book-Group-Author = {{ACM}},
Title = {{How LinkedIn Economic Graph Bonds Information and Product: Applications
   in LinkedIn Salary}},
Booktitle = {{KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON
   KNOWLEDGE DISCOVERY \& DATA MINING}},
Year = {{2018}},
Pages = {{120-129}},
Note = {{24th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD),
   London, ENGLAND, AUG 19-23, 2018}},
Organization = {{Assoc Comp Machinery; Assoc Comp Machinery SIGKDD; Assoc Comp Machinery
   SIGMOD}},
Abstract = {{The LinkedIn Salary product was launched in late 2016 with the goal of
   providing insights on compensation distribution to job seekers, so that
   they can make more informed decisions when discovering and assessing
   career opportunities. The compensation insights are provided based on
   data collected from LinkedIn members and aggregated in a
   privacy-preserving manner. Given the simultaneous desire for computing
   robust, reliable insights and for having insights to satisfy as many job
   seekers as possible, a key challenge is to reliably infer the insights
   at the company level when there is limited or no data at all. We propose
   a two-step framework that utilizes a novel, semantic representation of
   companies (Company2vec) and a Bayesian statistical model to address this
   problem. Our approach makes use of the rich information present in the
   LinkedIn Economic Graph, and in particular, uses the intuition that two
   companies are likely to be similar if employees are very likely to
   transition from one company to the other and vice versa. We compute
   embeddings for companies by analyzing the LinkedIn members' company
   transition data using machine learning algorithms, then compute pairwise
   similarities between companies based on these embeddings, and finally
   incorporate company similarities in the form of peer company groups as
   part of the proposed Bayesian statistical model to predict insights at
   the company level. We perform extensive validation using several
   different evaluation techniques, and show that we can significantly
   increase the coverage of insights while, in fact, even slightly
   improving the quality of the obtained insights. For example, we were
   able to compute salary insights for 35 times as many
   title-region-company combinations in the U.S. as compared to previous
   work, corresponding to 4.9 times as many monthly active users. Finally,
   we highlight the lessons learned from practical deployment of our
   system.}},
DOI = {{10.1145/3219819.3219921}},
ISBN = {{978-1-4503-5552-0}},
Unique-ID = {{ISI:000455346400013}},
}

@inproceedings{ ISI:000455001500070,
Author = {Huang, Kai and Zhou, Di and Xu, Wen and Cong, Yingna and Wang, Wei and
   Que, Zhiguo},
Book-Group-Author = {{IEEE}},
Title = {{Distributed Apriori on Cloud Plateform without Exposing Individual
   Transaction Data}},
Booktitle = {{PROCEEDINGS OF 2018 TENTH INTERNATIONAL CONFERENCE ON ADVANCED
   COMPUTATIONAL INTELLIGENCE (ICACI)}},
Year = {{2018}},
Pages = {{397-401}},
Note = {{10th International Conference on Advanced Computational Intelligence
   (ICACI), xiamen, PEOPLES R CHINA, MAR 29-31, 2018}},
Organization = {{IEEE Computat Intelligence Soc; Xiamen Univ; Fujian Prov Assoc
   Artificial Intelligence; IEEE Systems Man \& Cybernet Soc}},
Abstract = {{Privacy preserving and security issue becomes more and more important in
   data mining problems, especially in cloud computing environment. On
   cloud computing platform, data always distributed over different
   servers, It is not secure for each server to share data with other
   servers. Association rule mining is a core problem in machine learning
   domain, the widely used Apriori algorithm as a useful tool for
   personalized recommendation must address the privacy security problem in
   cloud computing environment. This paper addresses association rule
   mining issue with distributed data. Each server node is with some part
   of each transaction. Servers should corporate to get global association
   rules. In this process, no individual information should be revealed.
   One algorithm for finding frequent itemsets without revealing privacy
   data is proposed in this paper. In addition, one association rule mining
   algorithm in Hadoop which preserves privacy information has been
   proposed. The experiment presents the effectiveness of our method.}},
ISBN = {{978-1-5386-4362-4}},
Unique-ID = {{ISI:000455001500070}},
}

@article{ ISI:000454735500001,
Author = {Zhang, Xinyue and Wang, Jingyi and Shu, Minglei and Wang, Yinglong and
   Pan, Miao and Han, Zhu},
Title = {{TPP: Trajectory Privacy Preservation Against Tensor Voting Based
   Inference Attacks}},
Journal = {{IEEE ACCESS}},
Year = {{2018}},
Volume = {{6}},
Pages = {{77975-77985}},
Abstract = {{The popularity of mobile devices with global positioning system (GPS)
   has boosted various wireless location-based services (LBSs). Certain
   honest-but-curious or even dishonest LBS servers may learn the users'
   trajectories from location trace files, and the users' privacy can be
   compromised. In this paper, we propose a quantitative approach to model
   trajectory inference attacks via tensor voting, which can be widely
   applied in computer vision and machine learning as a perceptual
   organization. To counter the tensor voting based attacks, we propose a
   novel trajectory privacy preservation TPP scheme, in which LBS users
   will intentionally generate dummy trajectories to obfuscate LBS servers.
   Meanwhile, the LBS users have the option to disclose their trajectories
   to trustworthy parties (e.g., users' parents) by sending those parties a
   few more encrypted locations. Considering the power constraint of
   hand-held mobile devices, we mathematically formulate the trajectory
   privacy preservation problem into a mixed integer linear programming
   optimization problem and propose the algorithms for optimizing
   solutions. Through simulations and analysis, we show that the proposed
   scheme can effectively preserve LBS users' trajectory privacy against
   tensor voting-based inference attacks with limited power consumption.}},
DOI = {{10.1109/ACCESS.2018.2884516}},
ISSN = {{2169-3536}},
Unique-ID = {{ISI:000454735500001}},
}

@inproceedings{ ISI:000454683600008,
Author = {Senavirathne, Navoda and Torra, Vicenc},
Editor = {{McLaughlin, K and Ghorbani, A and Sezer, S and Lu, R and Chen, L and Deng, RH and Miller, P and Marsh, S and Nurse, J}},
Title = {{Approximating Robust Linear Regression With An Integral Privacy
   Guarantee}},
Booktitle = {{2018 16TH ANNUAL CONFERENCE ON PRIVACY, SECURITY AND TRUST (PST)}},
Series = {{Annual Conference on Privacy Security and Trust-PST}},
Year = {{2018}},
Pages = {{85-94}},
Note = {{16th Annual Conference on Privacy, Security and Trust (PST), Belfast,
   NORTH IRELAND, AUG 28-30, 2018}},
Organization = {{IEEE; Proofpoint; Invest NI; NCSC; CSIT; Allstate; Titan IC; Carson
   McDowell; RITICS; RISE; Hays Recruitment}},
Abstract = {{Most of the privacy-preserving techniques suffer from an inevitable
   utility loss due to different perturbations carried out on the input
   data or the models in order to gain privacy. When it comes to machine
   learning (ML) based prediction models, accuracy is the key criterion for
   model selection. Thus, an accuracy loss due to privacy implementations
   is undesirable.
   The motivation of this work, is to implement the privacy model
   ``integral privacy{''} and to evaluate its eligibility as a technique
   for machine learning model selection while preserving model utility. In
   this paper, a linear regression approximation method is implemented
   based on integral privacy which ensures high accuracy and robustness
   while maintaining a degree of privacy for ML models. The proposed method
   uses a re-sampling based estimator to construct linear regression model
   which is coupled with a rounding based data discretization method to
   support integral privacy principles. The implementation is evaluated in
   comparison with differential privacy in terms of privacy, accuracy and
   robustness of the output ML models. In comparison, integral privacy
   based solution provides a better solution with respect to the above
   criteria.}},
ISSN = {{1712-364X}},
ISBN = {{978-1-5386-7493-2}},
Unique-ID = {{ISI:000454683600008}},
}

@inproceedings{ ISI:000454722500003,
Author = {Voegel, Hans-Joerg and Suess, Christian and Hubregtsen, Thomas and
   Ghaderi, Viviane and Chadowitz, Ronee and Andre, Elisabeth and Cummins,
   Nicholas and Schuller, Bjoern and Harri, Jerome and Troncy, Raphael and
   Huet, Benoit and Onen, Melek and Ksentini, Adlen and Conradt, Joerg and
   Adi, Asaf and Zadorojniy, Alexander and Terken, Jacques and Beskow,
   Jonas and Morrison, Ann and Eng, Kynan and Eyben, Florian and Al
   Moubayed, Samer and Mueller, Susanne},
Book-Group-Author = {{IEEE}},
Title = {{Emotion-awareness for intelligent Vehicle Assistants: a research agenda}},
Booktitle = {{PROCEEDINGS 2018 IEEE/ACM 1ST INTERNATIONAL WORKSHOP ON SOFTWARE
   ENGINEERING FOR AI IN AUTONOMOUS SYSTEMS (SEFAIAS)}},
Year = {{2018}},
Pages = {{11-15}},
Note = {{1st IEEE/ACM International Workshop on Software Engineering for AI in
   Autonomous Systems (SEFAIAS), Gothenburg, SWEDEN, MAY 28, 2018}},
Organization = {{Assoc Comp Machinery; IEEE Comp Soc; SIGSOFT; IEEE Tech Council Software
   Engn}},
Abstract = {{EVA(1) is describing a new class of emotion-aware autonomous systems
   delivering intelligent personal assistant functionalities. EVA requires
   a multi-disciplinary approach, combining a number of critical building
   blocks into a cybernetics systems/software architecture: emotion aware
   systems and algorithms, multimodal interaction design, cognitive
   modelling, decision making and re-commender systems, emotion sensing as
   feedback for learning, and distributed (edge) computing delivering
   cognitive services.}},
DOI = {{10.1145/3194085.3194094}},
ISBN = {{978-1-4503-5739-5}},
Unique-ID = {{ISI:000454722500003}},
}

@inproceedings{ ISI:000454452800004,
Author = {AlAhmadi, Bushra A. and Martinovic, Ivan},
Book-Group-Author = {{IEEE}},
Title = {{MalClassifier: Malware Family Classification Using Network Flow Sequence
   Behaviour}},
Booktitle = {{PROCEEDINGS OF THE 2018 APWG SYMPOSIUM ON ELECTRONIC CRIME RESEARCH
   (ECRIME)}},
Year = {{2018}},
Pages = {{79-91}},
Note = {{APWG Symposium on Electronic Crime Research (eCrime), San Diego, CA, MAY
   15-17, 2018}},
Abstract = {{Anti-malware vendors receive daily thousands of potentially malicious
   binaries to analyse and categorise before deploying the appropriate
   defence measure. Considering the limitations of existing malware
   analysis and classification methods, we present MalClassifier, a novel
   privacy-preserving system for the automatic analysis and classification
   of malware using network flow sequence mining. MalClassifier allows
   identifying the malware family behind detected malicious network
   activity without requiring access to the infected host or malicious
   executable reducing overall response time. MalClassifier abstracts the
   malware families' network flow sequence order and semantics behaviour as
   an n-flow. By mining and extracting the distinctive n-flows for each
   malware family, it automatically generates network flow sequence
   behaviour profiles. These profiles are used as features to build
   supervised machine learning classifiers (K-Nearest Neighbour and Random
   Forest) for malware family classification. We compute the degree of
   similarity between a flow sequence and the extracted profiles using a
   novel fuzzy similarity measure that computes the similarity between
   flows attributes and the similarity between the order of the flow
   sequences. For classifier performance evaluation, we use network traffic
   datasets of ransomware and botnets obtaining 96\% F-measure for family
   classification. MalClassifier is resilient to malware evasion through
   flow sequence manipulation, maintaining the classifier's high accuracy.
   Our results demonstrate that this type of network flow-level sequence
   analysis is highly effective in malware family classification, providing
   insights on reoccurring malware network flow patterns.}},
ISBN = {{978-1-5386-4922-0}},
Unique-ID = {{ISI:000454452800004}},
}

@article{ ISI:000454306700001,
Author = {Tsou, Yao-Tung and Lin, Bo-Cheng},
Title = {{PPDCA: Privacy-Preserving Crowdsourcing Data Collection and Analysis
   With Randomized Response}},
Journal = {{IEEE ACCESS}},
Year = {{2018}},
Volume = {{6}},
Pages = {{76970-76983}},
Abstract = {{Randomized response mechanisms for guaranteeing crowdsourcing data
   privacy have attracted scholarly attention; aggregators can ensure
   privacy by collecting only randomized data, and individuals can have
   plausible deniability regarding their responses. With these mechanisms,
   analysts employed by organizations can still make predictions and
   conduct analyses using the randomized data. Existing randomized
   response-based data collection solutions have severely restricted
   functionality and usability, resulting in impractical and inefficient
   systems. Therefore, we developed a randomized response-based
   privacy-preserving crowdsourcing data collection and analysis mechanism.
   We designed a complementary randomized response (C-RR) method to
   guarantee individuals' data privacy and to preserve features from the
   original data for analysis. We formalized a machine learning framework;
   our proposed method uses randomized data in the form of binary vectors
   to generate a learning network. Extensive experiments on real-world data
   sets demonstrated that our heavy-hitters estimation scheme, which
   applies C-RR and our data learning model, significantly outperformed
   existing estimation schemes in terms of data analysis.}},
DOI = {{10.1109/ACCESS.2018.2884511}},
ISSN = {{2169-3536}},
Unique-ID = {{ISI:000454306700001}},
}

@article{ ISI:000445407800001,
Author = {Cheon, Jung Hee and Kim, Duhyeong and Kim, Yongdai and Song, Yongsoo},
Title = {{Ensemble Method for Privacy-Preserving Logistic Regression Based on
   Homomorphic Encryption}},
Journal = {{IEEE ACCESS}},
Year = {{2018}},
Volume = {{6}},
Pages = {{46938-46948}},
Abstract = {{Homomorphic encryption (HE) is one of promising cryptographic candidates
   resolving privacy issues in machine learning on sensitive data such as
   biomedical data and financial data. However, HE-based solutions commonly
   suffer from relatively high computational costs due to a large number of
   iterations in the optimization algorithms such as gradient descent (GD)
   for the learning phase. In this paper, we propose a new method called
   ensemble GD for logistic regression, a commonly used machine learning
   technique for binary classification. Our ensemble method reduces the
   number of iterations of GD, which results in substantial improvement on
   the performance of logistic regression based on HE in terms of speed and
   memory. The convergence of ensemble GD based on HE is guaranteed by our
   theoretical analysis on the erroneous variant of ensemble GD. We
   implemented ensemble GD for the logistic regression based on an
   approximate HE scheme HEAAN on MNIST data set and Credit data set from
   UCI machine learning repository. Compared to the standard GD for
   logistic regression, our ensemble method requires only about 60\% number
   of iterations, which results in 60-70\% reduction on the running time of
   total learning procedure in encrypted state, and 30-40\% reduction on
   the storage of encrypted data set.}},
DOI = {{10.1109/ACCESS.2018.2866697}},
ISSN = {{2169-3536}},
ORCID-Numbers = {{Kim, Duhyeong/0000-0002-4766-3456
   Song, Yongsoo/0000-0002-0496-9789}},
Unique-ID = {{ISI:000445407800001}},
}

@article{ ISI:000430620000003,
Author = {Catak, Ferhat Ozgur and Mustacoglu, Ahmet Fatih},
Title = {{CPP-ELM: Cryptographically Privacy-Preserving Extreme Learning Machine
   for Cloud Systems}},
Journal = {{INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS}},
Year = {{2018}},
Volume = {{11}},
Number = {{1}},
Pages = {{33-44}},
Abstract = {{The training techniques of the distributed machine learning approach
   replace the traditional methods with a cloud computing infrastructure
   and provide flexible computing services to clients. Moreover, machine
   learning-based classification methods are used in many diverse
   applications such as medical predictions, speech/face recognition, and
   financial applications. Most of the application areas require security
   and confidentiality for both the data and the classifier model. In order
   to prevent the risk of confidential data disclosure while outsourcing
   the data analysis, we propose a privacy-preserving protocol approach for
   the extreme learning machine algorithm and give private classification
   protocols. The proposed protocols compute the hidden layer output matrix
   H in an encrypted form by using a distributed multi-party computation
   (or cloud computing model) approach. This paper shows how to build a
   privacy-preserving classification model from encrypted data.}},
ISSN = {{1875-6891}},
EISSN = {{1875-6883}},
ResearcherID-Numbers = {{Catak, Ferhat Ozgur/C-1810-2016}},
ORCID-Numbers = {{Catak, Ferhat Ozgur/0000-0002-2434-9966}},
Unique-ID = {{ISI:000430620000003}},
}

@inproceedings{ ISI:000452447900033,
Author = {Diraco, Giovanni and Leone, Alessandro and Siciliano, Pietro},
Editor = {{Leone, A and Forleo, A and Francioso, L and Capone, S and Siciliano, P and DiNatale, C}},
Title = {{Radar-Based Fall Detection Using Deep Machine Learning: System
   Configuration and Performance}},
Booktitle = {{SENSORS AND MICROSYSTEMS}},
Series = {{Lecture Notes in Electrical Engineering}},
Year = {{2018}},
Volume = {{457}},
Pages = {{257-268}},
Note = {{19th Associazione-Italiana-Sensori-e- Microsistemi (AISEM) National
   Conference on Sensors and Microsystems, Lecce, ITALY, FEB 21-23, 2017}},
Organization = {{Assoc Italiana Sensori \& Microsistemi; Natl Res Council Italy, Inst
   Microelectron \& Microsystems}},
Abstract = {{Automatic fall-detection systems, saving time for the arrival of medical
   assistance, have the potential to reduce the risk of adverse health
   consequences. Fall-detection technologies are under continuous
   improvements in terms of both acceptability and performance.
   Ultra-wideband radar sensing is an interesting technology able to
   provide rich information in a privacy-preserving way, and thus well
   acceptable by end-users. In this study, a radar sensor compound of two
   ultra-wideband monostatic units in two different configurations (i.e.,
   vertical and horizontal baseline) has been investigated in order to
   provide sensor data from which robust features can be automatically
   extracted by using deep learning. The achieved results show the
   potential of the suggested sensor data representation and the
   superiority of the double-unit vertical-baseline configuration. Indeed,
   while the horizontal configuration allows to discriminate the body's
   position around the radar system, the vertical one discriminates the
   body's height that is more important for fall detection.}},
DOI = {{10.1007/978-3-319-66802-4\_33}},
ISSN = {{1876-1100}},
EISSN = {{1876-1119}},
ISBN = {{978-3-319-66802-4; 978-3-319-66801-7}},
Unique-ID = {{ISI:000452447900033}},
}

@inproceedings{ ISI:000450834400007,
Author = {Rodda, Sireesha and Erothi, Uma Shankar Rao},
Editor = {{Chaki, N and Cortesi, A and Devarakonda, N}},
Title = {{Network Intrusion Detection System to Preserve User Privacy}},
Booktitle = {{PROCEEDINGS OF INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE
   AND DATA ENGINEERING}},
Series = {{Lecture Notes on Data Engineering and Communications Technologies}},
Year = {{2018}},
Volume = {{9}},
Pages = {{85-94}},
Note = {{1st International Conference on Computational Intelligence \& Data
   Engineering (ICCIDE), Lakireddy Bali Reddy Coll Engn, Mylavaram, INDIA,
   JUL 14-15, 2017}},
Abstract = {{A wide range of malicious attacks and threats are increasing day by day
   with the growth and development of internet and network technologies.
   Enforcing network security is important to protect data or information
   in the computer network against attacks from intruders. The right of
   privacy of the user must be respected even on the network-resident data.
   This paper evaluates the performance of four different classifiers on a
   standard network intrusion detection dataset. The original values in the
   dataset are anonymized in order to protect the user's privacy. All the
   experiments were performed on IBM SPSS Premium Modeler. The
   effectiveness of the techniques is tested using different evaluation
   measures.}},
DOI = {{10.1007/978-981-10-6319-0\_7}},
ISSN = {{2367-4512}},
ISBN = {{978-981-10-6319-0; 978-981-10-6318-3}},
ORCID-Numbers = {{Erothi, uma shankar rao/0000-0001-6446-0346}},
Unique-ID = {{ISI:000450834400007}},
}

@inproceedings{ ISI:000449531900053,
Author = {Guo, Yuanxiong and Gong, Yanmin},
Book-Group-Author = {{IEEE}},
Title = {{Practical Collaborative Learning for Crowdsensing in the Internet of
   Things with Differential Privacy}},
Booktitle = {{2018 IEEE CONFERENCE ON COMMUNICATIONS AND NETWORK SECURITY (CNS)}},
Series = {{IEEE Conference on Communications and Network Security}},
Year = {{2018}},
Note = {{6th IEEE Conference on Communications and Network Security (CNS),
   Beijing, PEOPLES R CHINA, MAY 30-JUN 01, 2018}},
Organization = {{IEEE; Natl Sci Fdn; Chinese Acad Sci; Chinese Acad Sci, Inst Informat
   Engn}},
Abstract = {{Machine learning is increasingly used to produce predictive models for
   crowdsensing applications such as health monitoring and query
   suggestion. These models are more accurate when trained on large amount
   of data collected from different sources. However, such massive data
   collection presents serious privacy concerns. The personal crowdsensing
   data such as photos, voice records, and locations is often highly
   sensitive, and once being sent out to the collecting companies, falls
   out of the control of the crowdsensing users who own it. This may
   preclude the practice of transmitting all user data to a central
   location and training there using conventional machine learning
   approaches.
   In this paper, we advocate an alternative approach that leaves data
   stored on the user side and learns a shared model by coordinating local
   training of crowdsensing users in an iterative process. Specifically, we
   focus on regularized empirical risk minimization and propose an
   efficient scheme based on decomposition that enables multiple
   crowdsensing users to jointly learn an accurate learning model for a
   given learning objective without sharing their private crowdsensing
   data. We exploit the fact that the optimization problems used in many
   learning tasks are decomposable and can be solved in a parallel and
   distributed way by the alternating direction method of multipliers
   (ADMM). Considering the heterogeneity of different user devices in
   practice, we propose an asynchronous ADMM algorithm to speed up the
   training process. Our scheme lets users train independently on their own
   crowdsensing data and only share some updated model parameters instead
   of raw data. Moreover, secure computation and distributed noise
   generation are novelly integrated in our scheme to guarantee
   differential privacy of the shared parameters in the execution of the
   asynchronous ADMM algorithm. We analyze the privacy guarantee and
   demonstrate the privacy-utility trade-off of our privacy-preserving
   collaborative learning scheme empirically based on real-world data.}},
ISSN = {{2474-025X}},
ISBN = {{978-1-5386-4586-4}},
Unique-ID = {{ISI:000449531900053}},
}

@inproceedings{ ISI:000449531900032,
Author = {Yang, Lei and Li, Fengjun},
Book-Group-Author = {{IEEE}},
Title = {{Cloud-Assisted Privacy-Preserving Classification for IoT Applications}},
Booktitle = {{2018 IEEE CONFERENCE ON COMMUNICATIONS AND NETWORK SECURITY (CNS)}},
Series = {{IEEE Conference on Communications and Network Security}},
Year = {{2018}},
Note = {{6th IEEE Conference on Communications and Network Security (CNS),
   Beijing, PEOPLES R CHINA, MAY 30-JUN 01, 2018}},
Organization = {{IEEE; Natl Sci Fdn; Chinese Acad Sci; Chinese Acad Sci, Inst Informat
   Engn}},
Abstract = {{The explosive proliferation of Internet of Things (IoT) devices is
   generating an incomprehensible amount of data. Machine learning plays an
   imperative role in aggregating this data and extracting valuable
   information for improving operational and decision-making processes. In
   particular, emerging machine intelligence platforms that host
   pre-trained machine learning models are opening up new opportunities for
   IoT industries. While those platforms facilitate customers to analyze
   IoT data and deliver faster and accurate insights, end users and machine
   learning service providers (MLSPs) have raised concerns regarding
   security and privacy of IoT data as well as the pre-trained machine
   learning models for certain applications such as healthcare, smart
   energy, etc. In this paper, we propose a cloud-assisted,
   privacy-preserving machine learning classification scheme over encrypted
   data for IoT devices. Our scheme is based on a three-party model coupled
   with a two-stage decryption Paillier-based cryptosystem, which allows a
   cloud server to interact with MLSPs on behalf of the
   resource-constrained IoT devices in a privacy-preserving manner, and
   shift load of computation-intensive classification operations from them.
   The detailed security analysis and the extensive simulations with
   different key lengths and number of features and classes demonstrate
   that our scheme can effectively reduce the overhead for IoT devices in
   machine learning classification applications.}},
ISSN = {{2474-025X}},
ISBN = {{978-1-5386-4586-4}},
Unique-ID = {{ISI:000449531900032}},
}

@inproceedings{ ISI:000446384602048,
Author = {Dias, Miguel and Abad, Alberto and Trancoso, Isabel},
Book-Group-Author = {{IEEE}},
Title = {{EXPLORING HASHING AND CRYPTONET BASED APPROACHES FOR PRIVACY-PRESERVING
   SPEECH EMOTION RECOGNITION}},
Booktitle = {{2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)}},
Year = {{2018}},
Pages = {{2057-2061}},
Note = {{IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP), Calgary, CANADA, APR 15-20, 2018}},
Organization = {{Inst Elect \& Elect Engineers; Inst Elect \& Elect Engineers Signal Proc
   Soc}},
Abstract = {{The outsourcing of machine learning classification and data mining tasks
   can be an effective solution for those parties that need machine
   learning services, but lack the appropriate resources, knowledge and/or
   tools to carry them out, in their own premises. This solution, however,
   raises major privacy concerns, in particular, when irrevocable biometric
   data such as speech is involved. In this work, we focus on the
   development of privacy-preserving schemes in a speech emotion
   recognition task, as a proof of concept that could be extended to other
   speech analytics tasks. Our aim is to prove that the implementation of
   privacy-preserving speech mining schemes in challenging tasks involving
   paralinguistic features is not only feasible, but also accurate. Using
   distance-preserving hashing techniques in a first approach, and
   homomorphic encryption in a second approach, we successfully protect
   sensitive data with little degradation costs regarding the accuracy of
   the predictive models.}},
ISBN = {{978-1-5386-4658-8}},
ResearcherID-Numbers = {{Abad, Alberto/O-1351-2019
   Trancoso, Isabel/C-5965-2008}},
ORCID-Numbers = {{Abad, Alberto/0000-0003-2122-5148
   Trancoso, Isabel/0000-0001-5874-6313}},
Unique-ID = {{ISI:000446384602048}},
}

@inproceedings{ ISI:000446384602051,
Author = {Zhang, Ruochi and Venkitasubramaniam, Parv},
Book-Group-Author = {{IEEE}},
Title = {{MUTUAL-INFORMATION-PRIVATE ONLINE GRADIENT DESCENT ALGORITHM}},
Booktitle = {{2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)}},
Year = {{2018}},
Pages = {{2077-2081}},
Note = {{IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP), Calgary, CANADA, APR 15-20, 2018}},
Organization = {{Inst Elect \& Elect Engineers; Inst Elect \& Elect Engineers Signal Proc
   Soc}},
Abstract = {{A user implemented privacy preservation mechanism is proposed for the
   online gradient descent (OGD) algorithm. Privacy is measured through the
   information leakage as quantified by the mutual information between the
   users outputs and learners inputs. The input perturbation mechanism
   proposed can be implemented by individual users with a space and time
   complexity that is independent of the horizon T. For the proposed
   mechanism, the information leakage is shown to be bounded by the
   Gaussian channel capacity in the full information setting. The regret
   bound of the privacy preserving learning mechanism is identical to the
   non private OGD with only differing in constant factors.}},
ISBN = {{978-1-5386-4658-8}},
Unique-ID = {{ISI:000446384602051}},
}

@inproceedings{ ISI:000446384602076,
Author = {Imtiaz, Hafiz and Sarwate, Anand D.},
Book-Group-Author = {{IEEE}},
Title = {{IMPROVED ALGORITHMS FOR DIFFERENTIALLY PRIVATE ORTHOGONAL TENSOR
   DECOMPOSITION}},
Booktitle = {{2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)}},
Year = {{2018}},
Pages = {{2201-2205}},
Note = {{IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP), Calgary, CANADA, APR 15-20, 2018}},
Organization = {{Inst Elect \& Elect Engineers; Inst Elect \& Elect Engineers Signal Proc
   Soc}},
Abstract = {{Tensor decompositions have applications in many areas including signal
   processing, machine learning, computer vision and neuroscience. In this
   paper, we propose two new differentially private algorithms for
   orthogonal decomposition of symmetric tensors from private or sensitive
   data; these arise in applications such as latent variable models.
   Differential privacy is a formal privacy framework that guarantees
   protections against adversarial inference. We investigate the
   performance of these algorithms with varying privacy and database
   parameters and compare against another recently proposed
   privacy-preserving algorithm. Our experiments show that the proposed
   algorithms provide very good utility even while preserving strict
   privacy guarantees.}},
ISBN = {{978-1-5386-4658-8}},
Unique-ID = {{ISI:000446384602076}},
}

@inproceedings{ ISI:000446384603012,
Author = {Al, Mert and Chanyaswad, Thee and Kung, Sun-Yuan},
Book-Group-Author = {{IEEE}},
Title = {{MULTI-KERNEL, DEEP NEURAL NETWORK AND HYBRID MODELS FOR PRIVACY
   PRESERVING MACHINE LEARNING}},
Booktitle = {{2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)}},
Year = {{2018}},
Pages = {{2891-2895}},
Note = {{IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP), Calgary, CANADA, APR 15-20, 2018}},
Organization = {{Inst Elect \& Elect Engineers; Inst Elect \& Elect Engineers Signal Proc
   Soc}},
Abstract = {{The rapid rise of IoT and Big Data can facilitate the use of data to
   enhance our quality of life. However, the omnipresent and sensitive
   nature of data can simultaneously generate privacy concerns. Hence,
   there is a strong need to develop techniques that ensure the data serve
   the intended purposes, but not for prying into one's sensitive
   information. We address this challenge via utility maximizing lossy
   compression of data. Our techniques combine the mathematical rigor of
   Kernel Learning models with the structural richness of Deep Neural
   Networks, and lead to the novel Multi-Kernel Learning and Hybrid
   Learning models. We systematically construct the proposed models in
   progressive stages, as motivated by the cumulative improvement in the
   experimental results from the two previously non-intersecting regimes,
   namely, Kernel Learning and Deep Neural Networks. The final experimental
   results of the three proposed models on three mobile sensing datasets
   show that, not only are our methods able to improve the utility
   prediction accuracies, but they can also cause sensitive predictions to
   perform nearly as bad as random guessing, resulting in a win-win
   situation in terms of utility and privacy.}},
ISBN = {{978-1-5386-4658-8}},
Unique-ID = {{ISI:000446384603012}},
}

@article{ ISI:000444876900001,
Author = {Dai, Hua and Ren, Hui and Chen, Zhiye and Yang, Geng and Yi, Xun},
Title = {{Privacy-Preserving Sorting Algorithms Based on Logistic Map for Clouds}},
Journal = {{SECURITY AND COMMUNICATION NETWORKS}},
Year = {{2018}},
Abstract = {{Outsourcing data in clouds is adopted by more and more companies and
   individuals due to the profits from data sharing and parallel, elastic,
   and on-demand computing. However, it forces data owners to lose control
   of their own data, which causes privacy-preserving problems on sensitive
   data. Sorting is a common operation in many areas, such as machine
   learning, service recommendation, and data query. It is a challenge to
   implement privacy-preserving sorting over encrypted data without leaking
   privacy of sensitive data. In this paper, we propose privacy-preserving
   sorting algorithms which are on the basis of the logistic map. Secure
   comparable codes are constructed by logistic map functions, which can be
   utilized to compare the corresponding encrypted data items even without
   knowing their plaintext values. Data owners firstly encrypt their data
   and generate the corresponding comparable codes and then outsource them
   to clouds. Cloud servers are capable of sorting the outsourced encrypted
   data in accordance with their corresponding comparable codes by the
   proposed privacy-preserving sorting algorithms. Security analysis and
   experimental results show that the proposed algorithms can protect data
   privacy, while providing efficient sorting on encrypted data.}},
DOI = {{10.1155/2018/2373545}},
Article-Number = {{UNSP 2373545}},
ISSN = {{1939-0114}},
EISSN = {{1939-0122}},
ORCID-Numbers = {{Dai, Hua/0000-0003-2465-8977}},
Unique-ID = {{ISI:000444876900001}},
}

@article{ ISI:000433947600015,
Author = {Sadat, Md Nazmus and Jiang, Xiaoqian and Al Aziz, Md Momin and Wang,
   Shuang and Mohammed, Noman},
Title = {{Secure and Efficient Regression Analysis Using a Hybrid Cryptographic
   Framework: Development and Evaluation}},
Journal = {{JMIR MEDICAL INFORMATICS}},
Year = {{2018}},
Volume = {{6}},
Number = {{1}},
Month = {{JAN-MAR}},
Abstract = {{Background: Machine learning is an effective data-driven tool that is
   being widely used to extract valuable patterns and insights from data.
   Specifically, predictive machine learning models are very important in
   health care for clinical data analysis. The machine learning algorithms
   that generate predictive models often require pooling data from
   different sources to discover statistical patterns or correlations among
   different attributes of the input data. The primary challenge is to
   fulfill one major objective: preserving the privacy of individuals while
   discovering knowledge from data.
   Objective: Our objective was to develop a hybrid cryptographic framework
   for performing regression analysis over distributed data in a secure and
   efficient way.
   Methods: Existing secure computation schemes are not suitable for
   processing the large-scale data that are used in cutting-edge machine
   learning applications. We designed, developed, and evaluated a hybrid
   cryptographic framework, which can securely perform regression analysis,
   a fundamental machine learning algorithm using somewhat homomorphic
   encryption and a newly introduced secure hardware component of Intel
   Software Guard Extensions (Intel SGX) to ensure both privacy and
   efficiency at the same time.
   Results: Experimental results demonstrate that our proposed method
   provides a better trade-off in terms of security and efficiency than
   solely secure hardware-based methods. Besides, there is no approximation
   error. Computed model parameters are exactly similar to plaintext
   results.
   Conclusions: To the best of our knowledge, this kind of secure
   computation model using a hybrid cryptographic framework, which
   leverages both somewhat homomorphic encryption and Intel SGX, is not
   proposed or evaluated to this date. Our proposed framework ensures data
   security and computational efficiency at the same time.}},
DOI = {{10.2196/medinform.8286}},
Article-Number = {{e14}},
EISSN = {{2291-9694}},
ORCID-Numbers = {{Jiang, Xiaoqian/0000-0001-9933-2205
   Aziz, Md Momin Al/0000-0001-6161-8275}},
Unique-ID = {{ISI:000433947600015}},
}

@inproceedings{ ISI:000435009500047,
Author = {Diraco, Giovanni and Leone, Alessandro and Siciliano, Pietro},
Editor = {{Ando, B and Baldini, F and DiNatale, C and Marrazza, G and Siciliano, P}},
Title = {{A Fall Detector Based on Ultra-Wideband Radar Sensing}},
Booktitle = {{SENSORS}},
Series = {{Lecture Notes in Electrical Engineering}},
Year = {{2018}},
Volume = {{431}},
Pages = {{373-382}},
Note = {{3rd National Conference on Sensors, Rome, ITALY, FEB 23-25, 2016}},
Abstract = {{Falls in the elderly have been recognized worldwide as a major public
   health problem. Nevertheless, falls cannot be detected efficiently yet,
   due to open issues on both sensing and processing sides. The most
   promising sensing approaches raise concerns for privacy issues (e.g.,
   video-based approaches) or low acceptability rate (e.g., wearable
   approaches); whereas on the processing side, the commonly used
   methodologies are based on supervised techniques trained with both
   positive (falls) and negative (ADL-Activity of Daily Living) samples,
   both simulated by healthy young subjects. As a result of such a training
   protocol, fall detectors inevitably exhibit lower performance when used
   in real-world situations, in which monitored subjects are older adults.
   The aim of this study is to investigate a fully privacy-preserving and
   high-acceptance sensing technology, i.e. ultra-wideband radar sensor,
   together with a novelty detection methodology based exclusively on real
   ADL data from monitored elderly subject. The use of the UWB novelty
   detection methodology allowed to significantly improve detection
   performance in comparison to traditional supervised approaches.}},
DOI = {{10.1007/978-3-319-55077-0\_47}},
ISSN = {{1876-1100}},
EISSN = {{1876-1119}},
ISBN = {{978-3-319-55077-0; 978-3-319-55076-3}},
ResearcherID-Numbers = {{Diraco, Giovanni/Q-5776-2018}},
ORCID-Numbers = {{Diraco, Giovanni/0000-0002-9737-3721}},
Unique-ID = {{ISI:000435009500047}},
}

@article{ ISI:000433306100001,
Author = {Danner, Gabor and Berta, Arpad and Hegeds, Istvan and Jelasity, Mark},
Title = {{Robust Fully Distributed Minibatch Gradient Descent with Privacy
   Preservation}},
Journal = {{SECURITY AND COMMUNICATION NETWORKS}},
Year = {{2018}},
Abstract = {{Privacy and security are among the highest priorities in data mining
   approaches over data collected from mobile devices. Fully distributed
   machine learning is a promising direction in this context. However, it
   is a hard problem to design protocols that are efficient yet provide
   sufficient levels of privacy and security. In fully distributed
   environments, secure multiparty computation (MPC) is often applied to
   solve these problems. However, in our dynamic and unreliable application
   domain, known MPC algorithms are not scalable or not robust enough. We
   propose a light-weight protocol to quickly and securely compute the sum
   query over a subset of participants assuming a semihonest adversary.
   During the computation the participants learn no individual values. We
   apply this protocol to efficiently calculate the sum of gradients as
   part of a fully distributed minibatch stochastic gradient descent
   algorithm. The protocol achieves scalability and robustness by
   exploiting the fact that in this application domain a ``quick and
   dirty{''} sum computation is acceptable. We utilize the Paillier
   homomorphic cryptosystem as part of our solution combined with extreme
   lossy gradient compression to make the cost of the cryptographic
   algorithms affordable. We demonstrate both theoretically and
   experimentally, based on churn statistics from a real smartphone trace,
   that the protocol is indeed practically viable.}},
DOI = {{10.1155/2018/6728020}},
Article-Number = {{UNSP 6728020}},
ISSN = {{1939-0114}},
EISSN = {{1939-0122}},
ResearcherID-Numbers = {{jelasity, mark/F-3798-2011
   }},
ORCID-Numbers = {{jelasity, mark/0000-0001-9363-1482
   Danner, Gabor/0000-0002-9983-1060}},
Unique-ID = {{ISI:000433306100001}},
}

@article{ ISI:000433202300006,
Author = {Wang, Jingjing and Zhang, Xiaoyu and Tao, Xiaoling and Wang, Jianfeng},
Title = {{EPSLP: Efficient and privacy-preserving single-layer perceptron learning
   in cloud computing}},
Journal = {{JOURNAL OF HIGH SPEED NETWORKS}},
Year = {{2018}},
Volume = {{24}},
Number = {{3}},
Pages = {{259-279}},
Abstract = {{With the synchronous development of both cloud computing and machine
   learning techniques, the resource constrained clients are preferring to
   outsource the tasks of data storage and computation to the cloud server.
   However, in this outsourcing paradigm, since the data owners lose the
   control of their data, it is of vital significance to address the
   privacy concern of data stored on the cloud server. Hence, many
   researchers have been focusing on preserving the privacy of training
   data in learning model. Recently, Wang et al. presented a privacy
   protection single-layer perceptron learning for e-healthcare (PSLP) by
   using Paillier cryptosystem. In this paper, we present that the cloud
   server can obtain the sensitive training data and weight vector in the
   PSLP scheme. Besides, based on a symmetric homomorphic encryption
   algorithm, we propose an efficient and privacy-preserving single-layer
   perceptron learning scheme in cloud computing, named EPSLP. Security
   analysis shows that the proposed EPSLP can protect the privacy of
   training data, intermediate results and the optimal single-layer
   perceptron predictive models. Finally, we implement the EPSLP scheme and
   PSLP scheme, and extensive experiments indicate that the EPSLP is
   efficient in data encryption phase and the training phase of predictive
   model.}},
DOI = {{10.3233/JHS-180594}},
ISSN = {{0926-6801}},
EISSN = {{1875-8940}},
Unique-ID = {{ISI:000433202300006}},
}

@article{ ISI:000427552600027,
Author = {Var, Esra and Inan, Ali},
Title = {{Differentially private attribute selection for classification}},
Journal = {{JOURNAL OF THE FACULTY OF ENGINEERING AND ARCHITECTURE OF GAZI
   UNIVERSITY}},
Year = {{2018}},
Volume = {{33}},
Number = {{1}},
Pages = {{323-336}},
Abstract = {{Selecting a relevant subset of attributes is one of the most important
   data preprocessing steps of data mining and machine learning solutions.
   For the classification task, selection is based on the correlation
   between an attribute and the class attribute. There are various studies
   on privacy preserving classification. However, there is no attribute
   selection solution for such work in the literature. In this study, novel
   attribute selection methods based on the state of the art solution in
   statistical database security, known as differential privacy, are
   proposed. The proposed solutions are implemented with the popular data
   mining library WEKA and experimental results confirm the positive
   effects of the proposed solutions on classification accuracy.}},
DOI = {{10.17341/gazimmfd.406804}},
ISSN = {{1300-1884}},
EISSN = {{1304-4915}},
ResearcherID-Numbers = {{Inan, Ali/D-6172-2015}},
ORCID-Numbers = {{Inan, Ali/0000-0002-3149-1565}},
Unique-ID = {{ISI:000427552600027}},
}

@article{ ISI:000418581900037,
Author = {Fan, Jingyuan and Guan, Chaowen and Ren, Kui and Cui, Yong and Qiao,
   Chunming},
Title = {{SPABox: Safeguarding Privacy During Deep Packet Inspection at a
   MiddleBox}},
Journal = {{IEEE-ACM TRANSACTIONS ON NETWORKING}},
Year = {{2017}},
Volume = {{25}},
Number = {{6}},
Pages = {{3753-3766}},
Month = {{DEC}},
Abstract = {{Widely used over the Internet to encrypt traffic, HTTPS provides secure
   and private data communication between clients and servers. However, to
   cope with rapidly changing and sophisticated security attacks, network
   operators often deploy middleboxes to perform deep packet inspection
   (DPI) to detect attacks and potential security breaches, using
   techniques ranging from simple keyword matching to more advanced machine
   learning and data mining analysis. But this creates a problem: how can
   middleboxes, which employ DPI, work over HTTPS connections with
   encrypted traffic while preserving privacy? In this paper, we present
   SPABox, a middlebox-based system that supports both keyword-based and
   data analysis-based DPI functions over encrypted traffic. SPABox
   preserves privacy by using a novel protocol with a limited connection
   setup overhead. We implement SPABox on a standard server and show that
   SPABox is practical for both long-lived and short-lived connection.
   Compared with the state-of-the-art Blindbox system, SPABox is more than
   five orders of magnitude faster and requires seven orders of magnitude
   less bandwidth for connection setup while SPABox can achieve a higher
   security level.}},
DOI = {{10.1109/TNET.2017.2753044}},
ISSN = {{1063-6692}},
EISSN = {{1558-2566}},
Unique-ID = {{ISI:000418581900037}},
}

@article{ ISI:000415946200001,
Author = {Veale, Michael and Binns, Reuben},
Title = {{Fairer machine learning in the real world: Mitigating discrimination
   without collecting sensitive data}},
Journal = {{BIG DATA \& SOCIETY}},
Year = {{2017}},
Volume = {{4}},
Number = {{2}},
Month = {{NOV 20}},
Abstract = {{Decisions based on algorithmic, machine learning models can be unfair,
   reproducing biases in historical data used to train them. While
   computational techniques are emerging to address aspects of these
   concerns through communities such as discrimination-aware data mining
   (DADM) and fairness, accountability and transparency machine learning
   (FATML), their practical implementation faces real-world challenges. For
   legal, institutional or commercial reasons, organisations might not hold
   the data on sensitive attributes such as gender, ethnicity, sexuality or
   disability needed to diagnose and mitigate emergent indirect
   discrimination-by-proxy, such as redlining. Such organisations might
   also lack the knowledge and capacity to identify and manage fairness
   issues that are emergent properties of complex sociotechnical systems.
   This paper presents and discusses three potential approaches to deal
   with such knowledge and information deficits in the context of fairer
   machine learning. Trusted third parties could selectively store data
   necessary for performing discrimination discovery and incorporating
   fairness constraints into model-building in a privacy-preserving manner.
   Collaborative online platforms would allow diverse organisations to
   record, share and access contextual and experiential knowledge to
   promote fairness in machine learning systems. Finally, unsupervised
   learning and pedagogically interpretable algorithms might allow fairness
   hypotheses to be built for further selective testing and exploration.
   Real-world fairness challenges in machine learning are not abstract,
   constrained optimisation problems, but are institutionally and
   contextually grounded. Computational fairness tools are useful, but must
   be researched and developed in and with the messy contexts that will
   shape their deployment, rather than just for imagined situations. Not
   doing so risks real, near-term algorithmic harm.}},
DOI = {{10.1177/2053951717743530}},
ISSN = {{2053-9517}},
Unique-ID = {{ISI:000415946200001}},
}

@article{ ISI:000409541400015,
Author = {Le, Trang T. and Simmons, W. Kyle and Misaki, Masaya and Bodurka, Jerzy
   and White, Bill C. and Savitz, Jonathan and McKinney, Brett A.},
Title = {{Differential privacy-based evaporative cooling feature selection and
   classification with relief-F and random forests}},
Journal = {{BIOINFORMATICS}},
Year = {{2017}},
Volume = {{33}},
Number = {{18}},
Pages = {{2906-2913}},
Month = {{SEP 15}},
Abstract = {{Motivation: Classification of individuals into disease or clinical
   categories from high-dimensional biological data with low prediction
   error is an important challenge of statistical learning in
   bioinformatics. Feature selection can improve classification accuracy
   but must be incorporated carefully into cross-validation to avoid
   overfitting. Recently, feature selection Methods based on differential
   privacy, such as differentially private random forests and reusable
   holdout sets, have been proposed. However, for domains such as
   bioinformatics, where the number of features is much larger than the
   number of observations p >> n, these differential privacy methods are
   susceptible to overfitting.
   Methods: We introduce private Evaporative Cooling, a stochastic
   privacy-preserving machine learning algorithm that uses Relief-F for
   feature selection and random forest for privacy preserving
   classification that also prevents overfitting. We relate the
   privacy-preserving threshold mechanism to a thermodynamic
   Maxwell-Boltzmann distribution, where the temperature represents the
   privacy threshold. We use the thermal statistical physics concept of
   Evaporative Cooling of atomic gases to perform backward stepwise
   privacy-preserving feature selection.
   Results: On simulated data with main effects and statistical
   interactions, we compare accuracies on holdout and validation sets for
   three privacy-preserving methods: the reusable holdout, reusable holdout
   with random forest, and private Evaporative Cooling, which uses Relief-F
   feature selection and random forest classification. In simulations where
   interactions exist between attributes, private Evaporative Cooling
   provides higher classification accuracy without overfitting based on an
   independent validation set. In simulations without interactions,
   thresholdout with random forest and private Evaporative Cooling give
   comparable accuracies. We also apply these privacy methods to human
   brain resting-state fMRI data from a study of major depressive disorder.
   Availability and implementation: Code available at
   http://insilico.utulsa.edu/software/privateEC.
   Contact: brett-mckinney@utulsa.edu
   Supplementary information: Supplementary data are available at
   Bioinformatics online.}},
DOI = {{10.1093/bioinformatics/btx298}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
ResearcherID-Numbers = {{Savitz, Jonathan/C-3088-2009
   Le, Trang/L-7344-2019
   Misaki, Masaya/H-3042-2013}},
ORCID-Numbers = {{Savitz, Jonathan/0000-0001-8143-182X
   Le, Trang/0000-0003-3737-6565
   Misaki, Masaya/0000-0002-6829-4311}},
Unique-ID = {{ISI:000409541400015}},
}

@article{ ISI:000412220600006,
Author = {Wang, Jingjun and Hu, Shengshan and Wang, Qian and Ma, Yutao},
Title = {{Privacy-Preserving Outsourced Feature Extractions in the Cloud: A Survey}},
Journal = {{IEEE NETWORK}},
Year = {{2017}},
Volume = {{31}},
Number = {{5}},
Pages = {{36-41}},
Month = {{SEP-OCT}},
Abstract = {{Large-scale multimedia data generated in our daily life have
   increasingly motivated companies and researchers to discover valuable
   knowledge and hidden information by employing machine learning
   techniques. It is well known that this is an inherently time consuming
   job to deal with massive multimedia data locally. Thanks to the rapid
   development of cloud computing, the data owner is highly motivated to
   outsource these multimedia data, along with the computationally
   intensive processing tasks, to the cloud by leveraging its abundant
   resources for cost saving and flexibility. However, due to different
   trust domains, privacy issues also arise from the exposure to the cloud
   of private information that is intrinsically embedded in the outsourced
   data.
   As a key preprocessing step in machine learning, feature extractions
   have been extensively studied in the framework of privacy-preserving
   outsourced computation for their effectiveness in removing irrelevant
   and redundant data, and increasing learning accuracy. In this article,
   we provide a comprehensive survey of privacy-preserving outsourcing
   computation of various feature extraction algorithms in recent
   literature. Following a brief overview of each scheme, we present the
   primary technical hurdles needed to be addressed and then compare the
   state-of-the-art solutions in terms of security and effectiveness. Then
   we discuss several promising future research directions with a focus on
   privacy-preserving machine learning in the cloud.}},
DOI = {{10.1109/MNET.2017.1600240}},
ISSN = {{0890-8044}},
EISSN = {{1558-156X}},
ORCID-Numbers = {{Wang, Qian/0000-0002-8967-8525}},
Unique-ID = {{ISI:000412220600006}},
}

@article{ ISI:000403624900008,
Author = {Li, Ping and Li, Jin and Huang, Zhengan and Li, Tong and Gao, Chong-Zhi
   and Yiu, Siu-Ming and Chen, Kai},
Title = {{Multi-key privacy-preserving deep learning in cloud computing}},
Journal = {{FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE}},
Year = {{2017}},
Volume = {{74}},
Pages = {{76-85}},
Month = {{SEP}},
Abstract = {{Deep learning has attracted a lot of attention and has been applied
   successfully in many areas such as bioinformatics, imaging processing,
   game playing and computer security etc. On the other hand, deep learning
   usually requires a lot of training data which may not be provided by a
   sole owner. As the volume of data gets huge, it is common for users to
   store their data in a third-party cloud. Due to the confidentiality of
   the data, data are usually stored in encrypted form. To apply deep
   learning to these datasets owned by multiple data owners on cloud, we
   need to tackle two challenges: (i) the data are encrypted with different
   keys, all operations including intermediate results must be secure; and
   (ii) the computational cost and the communication cost of the data
   owner(s) should be kept minimal. In our work, we propose two schemes to
   solve the above problems. We first present a basic scheme based on
   multi-key fully homomorphic encryption (MK-FHE), then we propose an
   advanced scheme based on a hybrid structure by combining the double
   decryption mechanism and fully homomorphic encryption (FHE). We also
   prove that these two multi-key privacy-preserving deep learning schemes
   over encrypted data are secure. (C) 2017 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.future.2017.02.006}},
ISSN = {{0167-739X}},
EISSN = {{1872-7115}},
ResearcherID-Numbers = {{Yiu, Siu Ming/C-1843-2009}},
ORCID-Numbers = {{Yiu, Siu Ming/0000-0002-3975-8500}},
Unique-ID = {{ISI:000403624900008}},
}

@article{ ISI:000458474400004,
Author = {Deist, Timo M. and Jochems, A. and van Soest, Johan and Nalbantov,
   Georgi and Oberije, Cary and Walsh, Sean and Eble, Michael and Bulens,
   Paul and Coucke, Philippe and Dries, Wim and Dekker, Andre and Lambin,
   Philippe},
Title = {{Infrastructure and distributed learning methodology for
   privacy-preserving multi-centric rapid learning health care: euroCAT}},
Journal = {{CLINICAL AND TRANSLATIONAL RADIATION ONCOLOGY}},
Year = {{2017}},
Volume = {{4}},
Pages = {{24-31}},
Month = {{JUN}},
Abstract = {{Machine learning applications for personalized medicine are highly
   dependent on access to sufficient data. For personalized radiation
   oncology, datasets representing the variation in the entire cancer
   patient population need to be acquired and used to learn prediction
   models. Ethical and legal boundaries to ensure data privacy hamper
   collaboration between research institutes. We hypothesize that data
   sharing is possible without identifiable patient data leaving the
   radiation clinics and that building machine learning applications on
   distributed datasets is feasible.
   We developed and implemented an IT infrastructure in five radiation
   clinics across three countries (Belgium, Germany, and The Netherlands).
   We present here a proof-of-principle for future `big data'
   infrastructures and distributed learning studies. Lung cancer patient
   data was collected in all five locations and stored in local databases.
   Exemplary support vector machine (SVM) models were learned using the
   Alternating Direction Method of Multipliers (ADMM) from the distributed
   databases to predict post-radiotherapy dyspnea grade >= 2. The
   discriminative performance was assessed by the area under the curve
   (AUC) in a five-fold cross-validation (learning on four sites and
   validating on the fifth). The performance of the distributed learning
   algorithm was compared to centralized learning where datasets of all
   institutes are jointly analyzed.
   The euroCAT infrastructure has been successfully implemented in five
   radiation clinics across three countries. SVM models can be learned on
   data distributed over all five clinics. Furthermore, the infrastructure
   provides a general framework to execute learning algorithms on
   distributed data. The ongoing expansion of the euroCAT network will
   facilitate machine learning in radiation oncology. The resulting access
   to larger datasets with sufficient variation will pave the way for
   generalizable prediction models and personalized medicine. (C) 2016 The
   Authors. Published by Elsevier Ireland Ltd on behalf of European Society
   for Radiotherapy and Oncology.}},
DOI = {{10.1016/j.ctro.2016.12.004}},
EISSN = {{2405-6308}},
ORCID-Numbers = {{van Soest, Johan/0000-0003-2548-0330}},
Unique-ID = {{ISI:000458474400004}},
}

@article{ ISI:000401096700027,
Author = {Zhu, Hui and Liu, Xiaoxia and Lu, Rongxing and Li, Hui},
Title = {{Efficient and Privacy-Preserving Online Medical Prediagnosis Framework
   Using Nonlinear SVM}},
Journal = {{IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS}},
Year = {{2017}},
Volume = {{21}},
Number = {{3}},
Pages = {{838-850}},
Month = {{MAY}},
Abstract = {{With the advances of machine learning algorithms and the pervasiveness
   of network terminals, the online medical prediagnosis system, which can
   provide the diagnosis of healthcare provider anywhere anytime, has
   attracted considerable interest recently. However, the flourish of
   online medical prediagnosis system still faces many challenges including
   information security and privacy preservation. In this paper, we propose
   an efficient and privacy-preserving online medical prediagnosis
   framework, called eDiag, by using nonlinear kernel support vector
   machine (SVM). With eDiag, the sensitive personal health information can
   be processed without privacy disclosure during online prediagnosis
   service. Specifically, based on an improved expression for the nonlinear
   SVM, an efficient and privacy-preserving classification scheme is
   introduced with lightweight multiparty random masking and polynomial
   aggregation techniques. The encrypted user query is directly operated at
   the service provider without decryption, and the diagnosis result can
   only be decrypted by user. Through extensive analysis, we show that
   eDiag can ensure that users' health information and healthcare
   provider's prediction model are kept confidential, and has significantly
   less computation and communication overhead than existing schemes. In
   addition, performance evaluations via implementing eDiag on smartphone
   and computer demonstrate eDiag's effectiveness in term of real online
   environment.}},
DOI = {{10.1109/JBHI.2016.2548248}},
ISSN = {{2168-2194}},
ORCID-Numbers = {{Zhu, Hui/0000-0002-5853-633X}},
Unique-ID = {{ISI:000401096700027}},
}

@article{ ISI:000395563900017,
Author = {Li, Bo and Vorobeychik, Yevgeniy and Li, Muqun and Malin, Bradley},
Title = {{Scalable Iterative Classification for Sanitizing Large-Scale Datasets}},
Journal = {{IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING}},
Year = {{2017}},
Volume = {{29}},
Number = {{3}},
Pages = {{698-711}},
Month = {{MAR 1}},
Abstract = {{Cheap ubiquitous computing enables the collection of massive amounts of
   personal data in a wide variety of domains. Many organizations aim to
   share such data while obscuring features that could disclose personally
   identifiable information. Much of this data exhibits weak structure
   (e.g., text), such that machine learning approaches have been developed
   to detect and remove identifiers from it. While learning is never
   perfect, and relying on such approaches to sanitize data can leak
   sensitive information, a small risk is often acceptable. Our goal is to
   balance the value of published data and the risk of an adversary
   discovering leaked identifiers. We model data sanitization as a game
   between 1) a publisher who chooses a set of classifiers to apply to data
   and publishes only instances predicted as non-sensitive and 2) an
   attacker who combines machine learning and manual inspection to uncover
   leaked identifying information. We introduce a fast iterative greedy
   algorithm for the publisher that ensures a low utility for a
   resource-limited adversary. Moreover, using five text data sets we
   illustrate that our algorithm leaves virtually no automatically
   identifiable sensitive instances for a state-of-the-art learning
   algorithm, while sharing over 93 percent of the original data, and
   completes after at most five iterations.}},
DOI = {{10.1109/TKDE.2016.2628180}},
ISSN = {{1041-4347}},
EISSN = {{1558-2191}},
Unique-ID = {{ISI:000395563900017}},
}

@inproceedings{ ISI:000413241700036,
Author = {Pereira, Hilder V. L. and Aranha, Diego F.},
Editor = {{Mori, P and Furnell, S and Camp, O}},
Title = {{Non-interactive Privacy-preserving k-NN Classifier}},
Booktitle = {{ICISSP: PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON INFORMATION
   SYSTEMS SECURITY AND PRIVACY}},
Year = {{2017}},
Pages = {{362-371}},
Note = {{3rd International Conference on Information Systems Security and Privacy
   (ICISSP), Porto, PORTUGAL, FEB 19-21, 2017}},
Abstract = {{Machine learning tasks typically require large amounts of sensitive data
   to be shared, which is notoriously intrusive in terms of privacy.
   Outsourcing this computation to the cloud requires the server to be
   trusted, introducing a non-realistic security assumption and high risk
   of abuse or data breaches. In this paper, we propose privacy-preserving
   versions of the k-NN classifier which operate over encrypted data,
   combining order-preserving encryption and homomorphic encryption.
   According to our experiments, the privacy-preserving variant achieves
   the same accuracy as the conventional k-NN classifier, but considerably
   impacts the original performance. However, the performance penalty is
   still viable for practical use in sensitive applications when the
   additional security properties provided by the approach are considered.
   In particular, the cloud server does not need to be trusted beyond
   correct execution of the protocol and computes the algorithm over
   encrypted data and encrypted classes. As a result, the cloud server
   never learns the real dataset values, the number of classes, the query
   vectors or their classification.}},
DOI = {{10.5220/0006187703620371}},
ISBN = {{978-989-758-209-7}},
ResearcherID-Numbers = {{Aranha, Diego/J-9961-2012}},
ORCID-Numbers = {{Aranha, Diego/0000-0002-2457-0783}},
Unique-ID = {{ISI:000413241700036}},
}

@inproceedings{ ISI:000404171500092,
Author = {Gonzalez, J. Jose E. and Luo, Chen and Shrivastava, Anshumali and Palem,
   Krishna and Moon, Yongshik and Noh, Soonhyun and Park, Daedong and Hong,
   Seongsoo},
Book-Group-Author = {{IEEE}},
Title = {{Location Detection for Navigation Using IMUs with a Map Through
   Coarse-Grained Machine Learning}},
Booktitle = {{PROCEEDINGS OF THE 2017 DESIGN, AUTOMATION \& TEST IN EUROPE CONFERENCE
   \& EXHIBITION (DATE)}},
Series = {{Design Automation and Test in Europe Conference and Exhibition}},
Year = {{2017}},
Pages = {{500-505}},
Note = {{20th Conference and Exhibition on Design, Automation and Test in Europe
   (DATE), EPFL Campus, Lausanne, SWITZERLAND, MAR 27-31, 2017}},
Organization = {{European Design \& Automat Assoc; Elect Syst Design Alliance; IEEE
   Council Elect Design Automat; European Elect Chips \& Syst Design
   Initiat; ACM Special Interest Grp Design Automat; Russian Acad Sci; IEEE
   Comp Soc Test Technol Tech Council; IEEE Solid State Circuits Soc; Int
   Federat Informat Proc; CMP; SWISS; Cadence Acad Network; Ecocloud; Inst
   Elect Engn; Mentor Graph; Nano Tera Ch; Synopsys; Texas Instruments}},
Abstract = {{Location detection or localization supporting navigation has assumed
   significant importance in the recent past. In particular, techniques
   that exploit cheap inertial measurement units (IMU), the gyroscope and
   the accelerometer, have garnered attention, especially in an embedded
   computing context. However, these sensors measurements are quite
   unreliable, and it is widely believed that these sensors by themselves
   are too noisy for localization with acceptable accuracy. Consequently,
   several lines of work embody other costly alternatives to lower the
   impact of accumulated errors associated with IMU based approaches,
   invariably leading to very high energy costs resulting in lowered
   battery life. In this paper, we show that IMUs are sufficient by
   themselves if we augment them with known structural or geographical
   information about the physical area being explored by the user. By using
   the map of the region being explored and the fact that humans typically
   walk in a structured manner, our approach sidesteps the challenges
   created by noise and concomitant accumulation of error. Specifically, we
   show that a simple coarse-grained machine learning approach mitigates
   the effect of the noisy perturbations in the information from our IMUs,
   provided we have accurate maps. Throughout, we rely on the principle of
   inexactness in an overarching manner and relax the need for absolute
   accuracy in return for significant lowering of resource (energy) costs.
   Notably, our approach is completely independent of any external guidance
   from sources including GPS, Bluetooth or WiFi support, and is this
   privacy preserving. Specifically, we show through experimental results
   that by relying on gyroscope and accelerometer data alone, we can
   correctly identify the path-segment where the user is walking/running on
   a known map, as well as the position within the path with an accuracy of
   4.3 meters on the average using 0.44 Joules. This is a factor of 27X
   cheaper in energy lower than the ``gold standard{''} that one could
   consider based on GPS support which, surprisingly, has an associated
   error of 8.7 meters on the average.}},
ISSN = {{1530-1591}},
ISBN = {{978-3-9815370-9-3}},
Unique-ID = {{ISI:000404171500092}},
}

@inproceedings{ ISI:000452550000139,
Author = {Machanavajjhala, Ashwin and He, Xi and Hay, Michael},
Book-Group-Author = {{ACM SIGMOD}},
Title = {{Differential Privacy in the Wild: A Tutorial on Current Practices \&
   Open Challenges}},
Booktitle = {{SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON
   MANAGEMENT OF DATA}},
Year = {{2017}},
Pages = {{1727-1730}},
Note = {{ACM International Conference on Management of Data, Chicago, IL, MAY
   14-19, 2017}},
Organization = {{Assoc Comp Machinery; ACM SIGMOD; Oracle; Microsoft; Amazon; Facebook;
   Google; Huawei; IBM; MongoDB; Recruit; SAP; Vertica; Northwestern Univ,
   McCormick Sch Engn \& Appl Sci; Cloudera; Intel; LogicBlox; MemSQL;
   Snowflake; Workday}},
Abstract = {{Differential privacy has emerged as an important standard for privacy
   preserving computation over databases containing sensitive information
   about individuals. Research on differential privacy spanning a number of
   research areas, including theory, security, database, networks, machine
   learning, and statistics, over the last decade has resulted in a variety
   of privacy preserving algorithms for a number of analysis tasks. Despite
   maturing research efforts, the adoption of differential privacy by
   practitioners in industry, academia, or government agencies has so far
   been rare. Hence, in this tutorial, we will first describe the
   foundations of differentially private algorithm design that cover the
   state of the art in private computation on tabular data. In the second
   half of the tutorial we will highlight real world applications on
   complex data types, and identify research challenges in applying
   differential privacy to real world applications.}},
DOI = {{10.1145/3035918.3054779}},
ISBN = {{978-1-4503-4197-4}},
Unique-ID = {{ISI:000452550000139}},
}

@inproceedings{ ISI:000450296400037,
Author = {Al-Rubaie, Mohammad and Wu, Pei-yuan and Chang, J. Morris and Kung,
   Sun-Yuan},
Book-Group-Author = {{IEEE}},
Title = {{Privacy-Preserving PCA on Horizontally-Partitioned Data}},
Booktitle = {{2017 IEEE CONFERENCE ON DEPENDABLE AND SECURE COMPUTING}},
Year = {{2017}},
Pages = {{280-287}},
Note = {{IEEE Conference on Dependable and Secure Computing, Taipei, TAIWAN, AUG
   07-10, 2017}},
Organization = {{IEEE; Reliabil Soc; TWISC@NCTU; CCISA; Taiwan IC Design Soc; Hacker
   Coll; Minist Sci \& Technol, Sci Commun; Bur Foreign Trade; Ind Technol
   Res Inst; Editage; Dept Comp Sci}},
Abstract = {{Private data is used on daily basis by a variety of applications where
   machine learning algorithms predict our shopping patterns and movie
   preferences among other things. Principal component analysis (PCA) is a
   widely used method to reduce the dimensionality of data. Reducing the
   data dimension is essential for data visualization, preventing
   overfitting and resisting reconstruction attacks. In this paper, we
   propose methods that would enable the PCA computation to be performed on
   horizontally-partitioned data among multiple data owners without
   requiring them to stay online for the execution of the protocol. To
   address this problem, we propose a new protocol for computing the total
   scatter matrix using additive homomorphic encryption, and performing the
   Eigen decomposition using Garbled circuits. Our hybrid protocol does not
   reveal any of the data owner's input; thus protecting their privacy. We
   implemented our protocols using Java and Obliv-C, and conducted
   experiments using public datasets. We show that our protocols are
   efficient, and preserve the privacy while maintaining the accuracy.}},
ISBN = {{978-1-5090-5569-2}},
ResearcherID-Numbers = {{Chang, Jien/C-7672-2019}},
ORCID-Numbers = {{Chang, Jien/0000-0002-0660-7191}},
Unique-ID = {{ISI:000450296400037}},
}

@inproceedings{ ISI:000450296400060,
Author = {Zhuang, Di and Wang, Sen and Chang, J. Morris},
Book-Group-Author = {{IEEE}},
Title = {{FRiPAL: Face Recognition in Privacy Abstraction Layer}},
Booktitle = {{2017 IEEE CONFERENCE ON DEPENDABLE AND SECURE COMPUTING}},
Year = {{2017}},
Pages = {{441-448}},
Note = {{IEEE Conference on Dependable and Secure Computing, Taipei, TAIWAN, AUG
   07-10, 2017}},
Organization = {{IEEE; Reliabil Soc; TWISC@NCTU; CCISA; Taiwan IC Design Soc; Hacker
   Coll; Minist Sci \& Technol, Sci Commun; Bur Foreign Trade; Ind Technol
   Res Inst; Editage; Dept Comp Sci}},
Abstract = {{Data-driven mobile applications are becoming increasingly popular in
   civilian and law enforcement. RapidGather, for instance, is an
   smartphone application that collects data from individual, and spreads
   rapid emergency responses. Image data is widely used in such
   applications, and machine learning methods could be utilized to analyze
   the image data. However, people would hesitate to share the data without
   protecting their privacy. In this paper, we propose to utilize
   dimensionality reduction techniques for privacy-preserving machine
   learning in face recognition for the image data. To demonstrate the
   proposed approach, we implement a client server system, FRiPAL. With
   extensive experiments, we show that FRiPAL is efficient, and could
   preserve the privacy of data owners while maintaining the utility for
   data users.}},
ISBN = {{978-1-5090-5569-2}},
ResearcherID-Numbers = {{Chang, Jien/C-7672-2019}},
ORCID-Numbers = {{Chang, Jien/0000-0002-0660-7191}},
Unique-ID = {{ISI:000450296400060}},
}

@inproceedings{ ISI:000447643500022,
Author = {Attrapadung, Nuttapong and Hanaoka, Goichiro and Kiyomoto, Shinsaku and
   Mimoto, Tomoaki and Schuldt, Jacob C. N.},
Book-Group-Author = {{IEEE}},
Title = {{A Taxonomy of Secure Two-party Comparison Protocols and Efficient
   Constructions}},
Booktitle = {{2017 15TH ANNUAL CONFERENCE ON PRIVACY, SECURITY AND TRUST (PST)}},
Series = {{Annual Conference on Privacy Security and Trust-PST}},
Year = {{2017}},
Pages = {{215-224}},
Note = {{15th Annual Conference on Privacy, Security and Trust (PST), Calgary,
   CANADA, AUG 28-30, 2017}},
Organization = {{nulli; GuildOne; IEEE; ATB Listens; Secur Professionals Informat
   Exchange; Univ New Brunswick; Univ Calgary, Fac Sci, Dept Comp Sci; Univ
   Calgary, Sch Engn; Univ Calgary, Inst Secur Privacy \& Informat
   Assurance}},
Abstract = {{Secure two-party comparison plays a crucial role in many
   privacy-preserving applications, such as privacy-preserving data mining
   and machine learning. In particular, the available comparison protocols
   with the appropriate input/output configuration have a significant
   impact on the performance of these applications. In this paper, we
   firstly describe a taxonomy of secure two-party comparison protocols
   which allows us to describe the different configurations used for these
   protocols in a systematic manner. This taxonomy leads to a total of 216
   types of comparison protocols. We then describe conversions among these
   types. While these conversions are based on known techniques and have
   explicitly or implicitly been considered previously, we show that a
   combination of these conversion techniques can be used to convert a
   perhaps less-known two-party comparison protocol by Nergiz et al. (IEEE
   SocialCom 2010) into a very efficient protocol in a configuration where
   the two parties hold shares of the values being compared, and obtain a
   share of the comparison result. This setting is often used in
   multi-party computation protocols, and hence in many privacy-preserving
   applications as well. We furthermore implement the protocol and measure
   its performance. Our measurement suggests that the protocol outperforms
   the previously proposed protocols for this input/output configuration,
   when off-line pre-computation is not permitted.}},
DOI = {{10.1109/PST.2017.00033}},
ISSN = {{1712-364X}},
ISBN = {{978-1-5386-2487-6}},
Unique-ID = {{ISI:000447643500022}},
}

@inproceedings{ ISI:000444073000038,
Author = {Le Trieu Phong},
Editor = {{Yan, Z and Molva, R and Mazurczyk, W and Kantola, R}},
Title = {{Privacy-Preserving Stochastic Gradient Descent with Multiple Distributed
   Trainers}},
Booktitle = {{NETWORK AND SYSTEM SECURITY}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2017}},
Volume = {{10394}},
Pages = {{510-518}},
Note = {{11th International Conference on Network and System Security (NSS),
   Aalto Univ, Helsinki, FINLAND, AUG 21-23, 2017}},
Organization = {{Xidian Univ; Xidian Univ, State Key Lab Integrated Serv Networks;
   Federat Finnish Learned Soc; TEKES Finnish Funding Agcy Innovat; Xidian
   Univ, Natl 111 Project Mobile Internet Secur}},
Abstract = {{Assume that there are L local datasets distributed among L owners (also
   called trainers hereafter). The problem is as follows: the owners wish
   to apply a machine learning method over the combined dataset of all to
   obtain the best possible learning output; but do not want to publicly
   share the local datasets due to privacy concerns. In this paper we
   design a system solving the problem in which stochastic gradient descent
   (SGD) algorithm is used as the machine learning method, as SGD is at the
   heart of recent deep learning techniques. Our system differs from
   existing work by following features: (1) we do not share the gradients
   in SGD but share the weight parameters; and (2) we use symmetric
   encryption to protect the weight parameters against an
   honest-but-curious server used as a common place for storage. Therefore,
   we are able to avoid information leakage of local data to the server;
   and the efficiency of our system is kept reasonably compared to the
   original SGD over the combined dataset. Finally, we experiment over a
   real dataset to verify the practicality of our system.}},
DOI = {{10.1007/978-3-319-64701-2\_38}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-64701-2; 978-3-319-64700-5}},
Unique-ID = {{ISI:000444073000038}},
}

@inproceedings{ ISI:000443110500006,
Author = {Li, Yi and Duan, Yitao and Xu, Wei},
Editor = {{Ceci, M and Hollmen, J and Todorovski, L and Vens, C and Dzeroski, S}},
Title = {{PEM: A Practical Differentially Private System for Large-Scale
   Cross-Institutional Data Mining}},
Booktitle = {{MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES, ECML PKDD 2017,
   PT II}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2017}},
Volume = {{10535}},
Number = {{II}},
Pages = {{89-105}},
Note = {{European Conference on Machine Learning and Principles and Practice of
   Knowledge Discovery in Databases (ECML PKDD), Skopje, MACEDONIA, SEP
   18-22, 2017}},
Organization = {{Deutsche Post DHL Grp; Google; AGT; ASML; Deloitte; NEC Europe Ltd;
   Siemens; Cambridge Univ Press; IEEE CAA Journal Automatica Sinica;
   Springer; IBM Res; Data Min \& Knowledge Discovery, Machine Learning;
   EurAi; GrabIT}},
Abstract = {{Privacy has become a serious concern in data mining. Achieving adequate
   privacy is especially challenging when the scale of the problem is
   large. Fundamentally, designing a practical privacy-preserving data
   mining system involves tradeoffs among several factors such as the
   privacy guarantee, the accuracy or utility of the mining result, the
   computation efficiency and the generality of the approach. In this
   paper, we present PEM, a practical system that tries to strike the right
   balance among these factors. We use a combination of noise-based and
   noise-free techniques to achieve provable differential privacy at a low
   computational overhead while obtaining more accurate result than
   previous approaches. PEM provides an efficient private gradient descent
   that can be the basis for many practical data mining and machine
   learning algorithms, like logistic regression, k-means, and Apriori. We
   evaluate these algorithms on three real-world open datasets in a cloud
   computing environment. The results show that PEM achieves good accuracy,
   high scalability, low computation cost while maintaining differential
   privacy.}},
DOI = {{10.1007/978-3-319-71246-8\_6}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-71246-8; 978-3-319-71245-1}},
Unique-ID = {{ISI:000443110500006}},
}

@inproceedings{ ISI:000440307700039,
Author = {Hitaj, Briland and Ateniese, Giuseppe and Perez-Cruz, Fernando},
Book-Group-Author = {{ACM}},
Title = {{Deep Models Under the GAN: Information Leakage from Collaborative Deep
   Learning}},
Booktitle = {{CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND
   COMMUNICATIONS SECURITY}},
Year = {{2017}},
Pages = {{603-618}},
Note = {{24th ACM-SIGSAC Conference on Computer and Communications Security (ACM
   CCS), Dallas, TX, OCT 30-NOV 03, 2017}},
Organization = {{ACM SIGSAC; Assoc Comp Machinery; AT \& T Business; Baidu; NSF; CISCO;
   Internet Finance Authenticat Alliance; Samsung; Univ Texas Dallas;
   Google; IBM Res; Paloalto Networks; Visa Res; Army Res Off; Nasher
   Sculpture Ctr}},
Abstract = {{Deep Learning has recently become hugely popular in machine learning for
   its ability to solve end-to-end learning systems, in which the features
   and the classifiers are learned simultaneously, providing significant
   improvements in classification accuracy in the presence of
   highly-structured and large databases.
   Its success is due to a combination of recent algorithmic breakthroughs,
   increasingly powerful computers, and access to significant amounts of
   data.
   Researchers have also considered privacy implications of deep learning.
   Models are typically trained in a centralized manner with all the data
   being processed by the same training algorithm. If the data is a
   collection of users' private data, including habits, personal pictures,
   geographical positions, interests, and more, the centralized server will
   have access to sensitive information that could potentially be
   mishandled. To tackle this problem, collaborative deep learning models
   have recently been proposed where parties locally train their deep
   learning structures and only share a subset of the parameters in the
   attempt to keep their respective training sets private. Parameters can
   also be obfuscated via differential privacy (DP) to make information
   extraction even more challenging, as proposed by Shokri and Shmatikov at
   CCS' 15.
   Unfortunately, we show that any privacy-preserving collaborative deep
   learning is susceptible to a powerful attack that we devise in this
   paper. In particular, we show that a distributed, federated, or
   decentralized deep learning approach is fundamentally broken and does
   not protect the training sets of honest participants. The attack we
   developed exploits the real-time nature of the learning process that
   allows the adversary to train a Generative Adversarial Network (GAN)
   that generates prototypical samples of the targeted training set that
   was meant to be private (the samples generated by the GAN are intended
   to come from the same distribution as the training data). Interestingly,
   we show that record-level differential privacy applied to the shared
   parameters of the model, as suggested in previous work, is ineffective
   (i.e., record-level DP is not designed to address our attack).}},
DOI = {{10.1145/3133956.3134012}},
ISBN = {{978-1-4503-4946-8}},
Unique-ID = {{ISI:000440307700039}},
}

@inproceedings{ ISI:000440307700040,
Author = {Liu, Jian and Juuti, Mika and Lu, Yao and Asokan, N.},
Book-Group-Author = {{ACM}},
Title = {{Oblivious Neural Network Predictions via MiniONN Transformations}},
Booktitle = {{CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND
   COMMUNICATIONS SECURITY}},
Year = {{2017}},
Pages = {{619-631}},
Note = {{24th ACM-SIGSAC Conference on Computer and Communications Security (ACM
   CCS), Dallas, TX, OCT 30-NOV 03, 2017}},
Organization = {{ACM SIGSAC; Assoc Comp Machinery; AT \& T Business; Baidu; NSF; CISCO;
   Internet Finance Authenticat Alliance; Samsung; Univ Texas Dallas;
   Google; IBM Res; Paloalto Networks; Visa Res; Army Res Off; Nasher
   Sculpture Ctr}},
Abstract = {{Machine learning models hosted in a cloud service are increasingly
   popular but risk privacy: clients sending prediction requests to the
   service need to disclose potentially sensitive information. In this
   paper, we explore the problem of privacy-preserving predictions: after
   each prediction, the server learns nothing about clients' input and
   clients learn nothing about the model.
   We present MiniONN, the first approach for transforming an existing
   neural network to an oblivious neural network supporting
   privacy-preserving predictions with reasonable efficiency. Unlike prior
   work, MiniONN requires no change to how models are trained. To this end,
   we design oblivious protocols for commonly used operations in neural
   network prediction models. We show that MiniONN outperforms existing
   work in terms of response latency and message sizes. We demonstrate the
   wide applicability of MiniONN by transforming several typical neural
   network models trained from standard datasets.}},
DOI = {{10.1145/3133956.3134056}},
ISBN = {{978-1-4503-4946-8}},
ResearcherID-Numbers = {{Asokan, N./D-3182-2012}},
ORCID-Numbers = {{Asokan, N./0000-0002-5093-9871}},
Unique-ID = {{ISI:000440307700040}},
}

@inproceedings{ ISI:000440307700074,
Author = {Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone,
   Antonio and McMahan, H. Brendan and Patel, Sarvar and Ramage, Daniel and
   Segal, Aaron and Seth, Karn},
Book-Group-Author = {{ACM}},
Title = {{Practical Secure Aggregation for Privacy-Preserving Machine Learning}},
Booktitle = {{CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND
   COMMUNICATIONS SECURITY}},
Year = {{2017}},
Pages = {{1175-1191}},
Note = {{24th ACM-SIGSAC Conference on Computer and Communications Security (ACM
   CCS), Dallas, TX, OCT 30-NOV 03, 2017}},
Organization = {{ACM SIGSAC; Assoc Comp Machinery; AT \& T Business; Baidu; NSF; CISCO;
   Internet Finance Authenticat Alliance; Samsung; Univ Texas Dallas;
   Google; IBM Res; Paloalto Networks; Visa Res; Army Res Off; Nasher
   Sculpture Ctr}},
Abstract = {{We design a novel, communication-efficient, failure-robust protocol for
   secure aggregation of high-dimensional data. Our protocol allows a
   server to compute the sum of large, user-held data vectors from mobile
   devices in a secure manner (i.e. without learning each user's individual
   contribution), and can be used, for example, in a federated learning
   setting, to aggregate user-provided model updates for a deep neural
   network. We prove the security of our protocol in the honest-but-curious
   and active adversary settings, and show that security is maintained even
   if an arbitrarily chosen subset of users drop out at any time. We
   evaluate the efficiency of our protocol and show, by complexity analysis
   and a concrete implementation, that its runtime and communication
   overhead remain low even on large data sets and client pools. For 16-bit
   input values, our protocol offers 1.73x communication expansion for 210
   users and 220-dimensional vectors, and 1.98x expansion for 214 users and
   224-dimensional vectors over sending data in the clear.}},
DOI = {{10.1145/3133956.3133982}},
ISBN = {{978-1-4503-4946-8}},
Unique-ID = {{ISI:000440307700074}},
}

@inproceedings{ ISI:000428251401059,
Author = {Kuri, Shohei and Hayashi, Takuya and Omori, Toshiaki and Ozawa, Seiichi
   and Aono, Yoshinori and Le Trieu Phong and Wang, Lihua and Moriai, Shiho},
Book-Group-Author = {{IEEE}},
Title = {{Privacy Preserving Extreme Learning Machine Using Additively Homomorphic
   Encryption}},
Booktitle = {{2017 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (SSCI)}},
Year = {{2017}},
Pages = {{1350-1357}},
Note = {{IEEE Symposium Series on Computational Intelligence (IEEE SSCI),
   Honolulu, HI, NOV 27-DEC 01, 2017}},
Organization = {{IEEE; IEEE Computat Intelligence Soc}},
Abstract = {{Recently, computational outsourcing using cloud services is getting
   popular for big data analysis, and many cloud sourcing providers provide
   machine learning platforms where we can perform various prediction and
   classification tasks very easily. On the other hand, there still remains
   a big hurdle to analyze personal big data on cloud services because the
   leakage of personal information is a critical issue. As a remedy for
   this, we propose a privacy preserving machine learning algorithm for
   Extreme Learning Machine (PP-ELM), which can learn from data encrypted
   with an additively homomorphic encryption. In the proposed outsourcing
   method, we consider a three-participants model consisting of data
   contributors, outsourced server, and data analyst. A data contributor
   preprocesses and encrypts data, and an outsourced server receives
   encrypted data and calculate hidden layer outputs using additive
   operations. Then, a data analyst receives the hidden outputs of ELM from
   the outsourced server and they are used to obtain ELM connection
   weights. Since the proposed outsourcing model can learn ELM over
   encrypted data, it is expected to mitigate a hurdle to deal with
   personal data on cloud services. In addition, the proposed PP-ELM allows
   us to learn multiple sources of personal data in a secure way, and this
   might lead to a better solution for a practical problem than before.}},
ISBN = {{978-1-5386-2726-6}},
ResearcherID-Numbers = {{Omori, Toshiaki/P-4952-2016}},
Unique-ID = {{ISI:000428251401059}},
}

@inproceedings{ ISI:000428141800008,
Author = {Truex, Stacey and Liu, Ling and Gursoy, Mehmet Emre and Yu, Lei},
Editor = {{Karypis, G and Zhang, J}},
Title = {{Privacy-Preserving Inductive Learning with Decision Trees}},
Booktitle = {{2017 IEEE 6TH INTERNATIONAL CONGRESS ON BIG DATA (BIGDATA CONGRESS 2017)}},
Series = {{IEEE International Congress on Big Data}},
Year = {{2017}},
Pages = {{57-64}},
Note = {{IEEE 6th International Congress on Big Data (BigData Congress),
   Honolulu, HI, JUN 25-30, 2017}},
Organization = {{IEEE; IEEE Comp Soc; Serv Comp; Serv Soc; Big Data; Hp; IBM; SAP; IBM
   Res; Huawei; Object Management Grp; Business Proc Integrat \&
   Management; IT Profess; Int Journal Web Serv Res; Comp Now; IEEE
   Transact Serv Comp}},
Abstract = {{With the continued explosion of digitized data, data mining and data
   collection have become more prevalent. With this growth, we have also
   seen increased concern over data privacy and intellectual property.
   Within this environment, an important question has emerged: Can machine
   learning and data mining techniques be leveraged without compromising
   privacy? This paper revisits the concepts and techniques of
   privacy-preserving decision tree learning, a fundamental model of
   inductive learning. We first examine different privacy risks during
   decision tree based inductive learning processes, including the
   sensitivity of private input data and potential privacy risks induced by
   inference over both the learning output and the intermediate results of
   inductive learning iterations. We then review and compare the privacy
   notions and properties of three orthogonal and yet complimentary
   technical frameworks: randomization-based data obfuscation, differential
   privacy, and secure multiparty computation. We analyze their
   effectiveness and review representative approaches in each of these
   three frameworks. Finally, we highlight some of the open challenges to
   privacy-preserving solutions for decision tree learning.}},
DOI = {{10.1109/BigDataCongress.2017.17}},
ISSN = {{2379-7703}},
ISBN = {{978-1-5386-1996-4}},
Unique-ID = {{ISI:000428141800008}},
}

@inproceedings{ ISI:000428199300002,
Author = {Luo, Yuchuan and Fu, Shaojing and Wang, Dongsheng and Xu, Ming and Jia,
   Xiaohua},
Book-Group-Author = {{IEEE}},
Title = {{Efficient and Generalized Geometric Range Search on Encrypted Spatial
   Data in the Cloud}},
Booktitle = {{2017 IEEE/ACM 25TH INTERNATIONAL SYMPOSIUM ON QUALITY OF SERVICE (IWQOS)}},
Year = {{2017}},
Note = {{25th IEEE/ACM International Symposium on Quality of Service (IWQoS),
   Vilanova i la Geltru, SPAIN, JUN 14-16, 2017}},
Organization = {{IEEE; Assoc Comp Machinery; Univ Politecnica Catalunya Barcelonatech,
   CRAAX Lab; IEEE Commun Soc; Univ Politecnica Catalunya, Escola
   Politecnica Super Engn Vilanova Geltru; Ajuntament Vilanova i la Geltru}},
Abstract = {{With cloud services, users can easily host their data in the cloud and
   retrieve the part needed by search. Searchable encryption is proposed to
   conduct such process in a privacy-preserving way, which allows a cloud
   server to perform search over the encrypted data in the cloud according
   to the search token submitted by the user. However, existing works
   mainly focus on textual data and merely take numerical spatial data into
   account. Especially, geometric range search is an important queries on
   spatial data and has wide applications in machine learning,
   location-based services(LBS), computer-aided design(CAD), and
   computational geometry.
   In this paper, we proposed an efficient and generalized symmetric-key
   geometric range search scheme on encrypted spatial data in the cloud,
   which supports queries with different range shapes and dimensions. To
   provide secure and efficient search, we extend the secure kNN
   computation with dynamic geometric transformation, which dynamically
   transforms the points in the dataset and the queried geometric range
   simultaneously. Besides, we further extend the proposed scheme to
   support sub-linear search efficiency through novel usage of tree
   structures. We also present extensive experiments to evaluate the
   proposed schemes on a real-world dataset. The results show that the
   proposed schemes are efficient over encrypted datasets and secure
   against the curious cloud servers.}},
ISBN = {{978-1-5386-2704-4}},
Unique-ID = {{ISI:000428199300002}},
}

@inproceedings{ ISI:000428577000021,
Author = {Zheng, Tianhang and Sun, Zhi and Ren, Kui},
Book-Group-Author = {{IEEE}},
Title = {{Data Independent Identification for Privacy Preservation}},
Booktitle = {{2017 1ST IEEE SYMPOSIUM ON PRIVACY-AWARE COMPUTING (PAC)}},
Year = {{2017}},
Pages = {{186-187}},
Note = {{1st IEEE Symposium on Privacy-Aware Computing (PAC), George Washington
   Univ, Washington, DC, AUG 01-04, 2017}},
Organization = {{IEEE; IEEE Comp Soc, Tech Comm Secur \& Privacy; IEEE Comp Soc, Tech
   Comm Internet; IEEE Comp Soc}},
Abstract = {{A potential data independent physical layer identification mechanism is
   proposed to defend against impersonation attacks and protect the privacy
   of the users with authenticated devices. A nonlinear function is derived
   from the impairments of a unique device to be used as the fingerprint in
   this mechanism and two methods are presented to learn this function. One
   is Kernel Regression which is commonly used for function modeling in
   Machine Learning and the other one is Neural Network which can
   theoretically approximate any continuous function arbitrarily well. In
   this paper, the implementation and experiment results of Kernel
   Regression are discussed in detail due to its higher efficiency. The
   experiment results show that the training and testing accuracy of
   nonlinear function modeling using kernel regression can reach nearly
   90\%, indicating that it is a potential scheme for data independent
   identification.}},
DOI = {{10.1109/PAC.2017.20}},
ISBN = {{978-1-5386-1027-5}},
Unique-ID = {{ISI:000428577000021}},
}

@inproceedings{ ISI:000427068800043,
Author = {Mehnaz, Shagufta and Bertino, Elisa},
Editor = {{Fox, GC}},
Title = {{Privacy-preserving Multi-party Analytics over Arbitrarily Partitioned
   Data}},
Booktitle = {{2017 IEEE 10TH INTERNATIONAL CONFERENCE ON CLOUD COMPUTING (CLOUD)}},
Series = {{IEEE International Conference on Cloud Computing}},
Year = {{2017}},
Pages = {{342-349}},
Note = {{10th IEEE International Conference on Cloud Computing (CLOUD), Honolulu,
   HI, JUN 25-30, 2017}},
Organization = {{IEEE; IEEE Comp Soc; IBM Res; Huawei; Serv Comp; Serv Comp; Serv Soc;
   Object Management Grp; Int Journal Business Proc Integrat \& Management;
   ITProfessional; Int Journal Web Serv Res; Big Data; HP; IBM; Comp Now;
   SAP; IEEE Transact Serv Comp}},
Abstract = {{Data-driven business processes are gaining popularity among enterprises
   now-a-days. In many situations, multiple parties would share data
   towards a common goal if it were possible to simultaneously protect the
   privacy of the individuals and organizations described in the data.
   Existing solutions for multi-party analytics require parties to transfer
   their raw data to a trusted mediator, who then performs the desired
   analysis on the global data, and shares the results with the parties.
   Unfortunately, such a solution does not fit many applications where
   privacy is a strong concern such as healthcare, finance and the
   internet-of-things. Motivated by the increasing demands for data
   privacy, in this paper, we study the problem of privacy-preserving
   multi-party analytics, where the goal is to enable analytics on
   multi-party data without compromising the data privacy of each
   individual party. We propose a secure gradient descent algorithm that
   enables analytics on data that is arbitrarily partitioned among multiple
   parties. The proposed algorithm is generic and applies to a wide class
   of machine learning problems. We demonstrate our solution for a popular
   use-case (i.e., regression), and evaluate the performance of the
   proposed secure solution in terms of accuracy, latency and communication
   cost. We also perform a scalability analysis to evaluate the performance
   of the proposed solution as the data size and the number of parties
   increase.}},
DOI = {{10.1109/CLOUD.2017.51}},
ISSN = {{2159-6182}},
ISBN = {{978-1-5386-1993-3}},
Unique-ID = {{ISI:000427068800043}},
}

@inproceedings{ ISI:000427068800103,
Author = {Budhraja, Karan K. and Malvankar, Abhishek and Bahrami, Mehdi and Kundu,
   Chinmay and Kundu, Ashish and Singhal, Mukesh},
Editor = {{Fox, GC}},
Title = {{Risk-Based Packet Routing for Privacy and Compliance-Preserving SDN}},
Booktitle = {{2017 IEEE 10TH INTERNATIONAL CONFERENCE ON CLOUD COMPUTING (CLOUD)}},
Series = {{IEEE International Conference on Cloud Computing}},
Year = {{2017}},
Pages = {{761-765}},
Note = {{10th IEEE International Conference on Cloud Computing (CLOUD), Honolulu,
   HI, JUN 25-30, 2017}},
Organization = {{IEEE; IEEE Comp Soc; IBM Res; Huawei; Serv Comp; Serv Comp; Serv Soc;
   Object Management Grp; Int Journal Business Proc Integrat \& Management;
   ITProfessional; Int Journal Web Serv Res; Big Data; HP; IBM; Comp Now;
   SAP; IEEE Transact Serv Comp}},
Abstract = {{Software Defined Networking (SDN) is increasingly being used in data
   centers as well as enterprise networks. In an environment that has
   strict compliance requirements, such as HIPAA compliance, a critical
   role for an SDN controller is to route all data packets while
   considering data privacy-preservation and compliance-preservation. In
   this paper, we address this problem by proposing a routing protocol for
   SDN which is an efficient risk-based swarm routing protocol. The
   programmable capability of controllers is exploited in order to minimize
   privacy and compliance risks in data transmission. The proposed routing
   protocol is based on the Ant Colony Optimization technique and machine
   learning, while the data for learning is obtained from OVSDB and the
   OpenvSwitch Database management protocol. We collect a history of packet
   transfers for training purposes and learn from the training data to
   efficiently and intelligently route sensitive data packets while it
   preserves the target compliance. This routing is obtained by intelligent
   eviction of rules that are downloaded to the switches. We have
   implemented the proposed schemes based on an RYU controller.}},
DOI = {{10.1109/CLOUD.2017.109}},
ISSN = {{2159-6182}},
ISBN = {{978-1-5386-1993-3}},
Unique-ID = {{ISI:000427068800103}},
}

@inproceedings{ ISI:000428083300020,
Author = {Hossain, Shaikh Farhad and Islam, Md. Zahurul and Ali, Md. Liakot},
Book-Group-Author = {{IEEE}},
Title = {{Real Time Direction-Sensitive Fall Detection System Using Accelerometer
   and Learning Classifier}},
Booktitle = {{2017 4TH INTERNATIONAL CONFERENCE ON ADVANCES IN ELECTRICAL ENGINEERING
   (ICAEE)}},
Series = {{International Conference on Advances in Electrical Engineering}},
Year = {{2017}},
Pages = {{99-104}},
Note = {{4th International Conference on Advances in Electrical Engineering
   (ICAEE), Independent Univ, Dhaka, BANGLADESH, SEP 28-30, 2017}},
Organization = {{IEEE Bangladesh Sect EMB Soc; Inst Elect \& Elect Engineers}},
Abstract = {{Continuous fall recognition framework screens the day by day action of
   particularly elderly individuals to enroll somebody's assistance as
   quick as conceivable if there should be an occurrence of crisis. This
   paper presents a real-time fall detection using a single 3D commercial
   accelerometer (3DCA) and support vector machine learning algorithm
   (SVMLA). In past, two machine learning (ML) based calculations SVMLA and
   k-Nearest Neighbors (K-NN) were executed for mandate fall discovery in
   reproduction. Among the two strategies, SVMLA give better exhibitions
   which prompts 96.45\% of exactness utilizing PCA mean and standard
   deviation highlights, surpassing the exhibitions detailed in the
   writing. The performances of the developed system in real time are also
   evaluated and they are found same accuracy, precision and recall. When
   applied to experimental data from 13 male subjects, the real time system
   discriminates between falls and activities of daily living (ADL) with
   same level like simulation. The system utilizes privacy preserving
   sensor. The system is reliable, user friendly and cost effective with
   less technical error rate and high classification accuracy.}},
ISSN = {{2378-2668}},
ISBN = {{978-1-5386-0869-2}},
Unique-ID = {{ISI:000428083300020}},
}

@inproceedings{ ISI:000427487200062,
Author = {Ma, Keng-Teck and Xu, Qianli and Lim, Rosary and Li, Liyuan and Sim,
   Terence and Kankanhalli, Mohan},
Book-Group-Author = {{IEEE}},
Title = {{Eye-2-I: Eye-Tracking for Just-in-Time Implicit User Profiling}},
Booktitle = {{2017 IEEE 2ND INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING
   (ICSIP)}},
Year = {{2017}},
Pages = {{311-315}},
Note = {{2nd IEEE International Conference on Signal and Image Processing
   (ICSIP), Singapore, SINGAPORE, AUG 04-06, 2017}},
Organization = {{IEEE}},
Abstract = {{For many applications, such as targeted advertising and content
   recommendation, knowing users' traits and interests is a prerequisite.
   User profiling is a helpful approach for this purpose. However, current
   methods, i.e. self-reporting, web-activity monitoring and social media
   mining are either intrusive or require data over long periods of time.
   Recently, there is growing evidence in cognitive science that a variety
   of users' profile is significantly correlated with eye-tracking data. A
   novel just-in-time implicit profiling method, Eye-2-I, which learns the
   user's demographic and personality traits from the eye-tracking data
   while the user is watching videos is proposed. Although seemingly
   conspicuous by closely monitoring the user's eye behaviors, the proposed
   method is unobtrusive and privacy-preserving owing to its unique
   combination of speed and implicitness. As a proof-of-concept, the
   proposed method is evaluated in a user study with 51 subjects.}},
ISBN = {{978-1-5386-0969-9}},
ResearcherID-Numbers = {{Kankanhalli, Mohan/Q-9284-2019}},
ORCID-Numbers = {{Kankanhalli, Mohan/0000-0002-4846-2015}},
Unique-ID = {{ISI:000427487200062}},
}

@inproceedings{ ISI:000426968704044,
Author = {Chanyaswad, Thee and Chang, J. Morris and Kung, S. Y.},
Book-Group-Author = {{IEEE}},
Title = {{A Compressive Multi-Kernel Method for Privacy-Preserving Machine
   Learning}},
Booktitle = {{2017 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)}},
Series = {{IEEE International Joint Conference on Neural Networks (IJCNN)}},
Year = {{2017}},
Pages = {{4079-4086}},
Note = {{International Joint Conference on Neural Networks (IJCNN), Anchorage,
   AK, MAY 14-19, 2017}},
Organization = {{Int Neurol Network Soc; IEEE Computat Intelligence Soc; Intel; BMI;
   Budapest Semester Cognit Sci}},
Abstract = {{As the analytic tools become more powerful, and more data are generated
   on a daily basis, the issue of data privacy arises. This leads to the
   study of the design of privacy-preserving machine learning algorithms.
   Given two objectives, namely, utility maximization and privacy-loss
   minimization, this work is based on two previously non-intersecting
   regimes - Compressive Privacy and multi-kernel method. Compressive
   Privacy is a privacy framework that employs utility-preserving
   lossy-encoding scheme to protect the privacy of the data, while
   multi-kernel method is a kernel-based machine learning regime that
   explores the idea of using multiple kernels for building better
   predictors. In relation to the neural-network architecture, multi-kernel
   method can be described as a two-hidden-layered network with its width
   proportional to the number of kernels.
   The compressive multi-kernel method proposed consists of two stages -
   the compression stage and the multi-kernel stage. The compression stage
   follows the Compressive Privacy paradigm to provide the desired privacy
   protection. Each kernel matrix is compressed with a lossy projection
   matrix derived from the Discriminant Component Analysis (DCA). The
   multikernel stage uses the signal-to-noise ratio (SNR) score of each
   kernel to non-uniformly combine multiple compressive kernels.
   The proposed method is evaluated on two mobile-sensing datasets -
   MHEALTH and HAR - where activity recognition is defined as utility and
   person identification is defined as privacy. The results show that the
   compression regime is successful in privacy preservation as the privacy
   classification accuracies are almost at the random-guess level in all
   experiments. On the other hand, the novel SNR-based multi-kernel shows
   utility classification accuracy improvement upon the state-of-the-art in
   both datasets. These results indicate a promising direction for research
   in privacy-preserving machine learning.}},
ISSN = {{2161-4393}},
ISBN = {{978-1-5090-6182-2}},
ResearcherID-Numbers = {{Chang, Jien/C-7672-2019}},
ORCID-Numbers = {{Chang, Jien/0000-0002-0660-7191}},
Unique-ID = {{ISI:000426968704044}},
}

@inproceedings{ ISI:000426981000001,
Author = {Xiang, Liyao and Li, Baochun and Li, Bo},
Book-Group-Author = {{IEEE}},
Title = {{Privacy-Preserving Inference in Crowdsourcing Systems}},
Booktitle = {{2017 IEEE CONFERENCE ON COMMUNICATIONS AND NETWORK SECURITY (CNS)}},
Series = {{IEEE Conference on Communications and Network Security}},
Year = {{2017}},
Pages = {{1-9}},
Note = {{2017 IEEE Conference on Communications and Network Security (CNS), Las
   Vegas, NV, OCT 09-11, 2017}},
Organization = {{IEEE}},
Abstract = {{Machine learning has widely been used in crowd-sourcing systems to
   analyze the behavior of their mobile users. However, it naturally raises
   privacy concerns, as personal data needs to be collected and analyzed in
   the cloud, and results need to be sent back to the users to improve
   their local estimates. In this paper, we focus on the use of a specific
   type of learning algorithms, called maximum a posteriori ( MAP)
   inference, in crowdsourcing systems, and use a crowdsourced localization
   system as an example. With MAP inference, the accuracy of each estimate
   of the user state may be improved by analyzing other users' estimates.
   Naturally, the privacy of the user state needs to be protected. Within
   the general framework of differential privacy, we show how private user
   states can be perturbed while preserving statistically accurate results.
   For the crowdsourcing system, we design a non-interactive mechanism for
   a group of users to perform inference without revealing their true
   states to any other party. The mechanism is implemented and verified in
   an indoor localization system. By comparing with the state-of-the-art,
   we have shown that our proposed privacy-preserving mechanism produces
   highly accurate results efficiently.}},
ISSN = {{2474-025X}},
ISBN = {{978-1-5386-0683-4}},
Unique-ID = {{ISI:000426981000001}},
}

@inproceedings{ ISI:000427097000016,
Author = {Fakhr, Mohamed Waleed},
Book-Group-Author = {{IEEE}},
Title = {{A Multi-Key Compressed Sensing and Machine Learning Privacy Preserving
   Computing Scheme}},
Booktitle = {{2017 5TH INTERNATIONAL SYMPOSIUM ON COMPUTATIONAL AND BUSINESS
   INTELLIGENCE (ISCBI)}},
Year = {{2017}},
Pages = {{75-80}},
Note = {{5th International Symposium on Computational and Business Intelligence
   (ISCBI), Dubai, U ARAB EMIRATES, AUG 11-14, 2017}},
Organization = {{IEEE; IEEE Comp Intelligence Soc}},
Abstract = {{Recently there has been a huge interest in secure and private cloud
   computing. In particular; to perform signal processing and machine
   learning tasks in the encrypted domain. Homomorphic encryption offers
   provably secure, asymmetric encryption solution to this problem,
   however, it comes with a high storage and computation cost. Compressed
   sensing (CS) and random projection (RP) approaches are much lighter;
   however, they lack privacy since the encryption uses a symmetric key
   which is the random projection matrix. A multi-key, compressed sensing
   encryption approach is proposed in this paper for performing basic
   generic computations. The computing architecture consists of a User, a
   Cloud (which stores encrypted data from an Owner), and a Trusted Third
   Party (TTP) which is responsible for distributing the random CS keys.
   The TTP also trains two machine learning modules; ML1 and ML2. ML1, used
   at the cloud, takes as input the multi-key encrypted data and produces
   an intermediate encrypted result. ML2, available at the user side,
   decrypts the results. This novel approach is much cheaper than
   homomorphic encryption in terms of data expansion, storage as well as
   encryption time. Also, it offers the privacy of the multi-keys. The
   proposed approach is applied on 2 generic computing tasks; namely,
   squared Euclidean distance and dot product. The developed approach is
   tested on the COREL image classification task using the squared
   Euclidean distance and on an autoregressive (AR) stock prediction task
   using the dot product.}},
ISBN = {{978-1-5386-1772-4}},
Unique-ID = {{ISI:000427097000016}},
}

@inproceedings{ ISI:000426068500018,
Author = {Zhou, Li and Ying, Mingsheng},
Book-Group-Author = {{IEEE}},
Title = {{Differential Privacy in Quantum Computation}},
Booktitle = {{2017 IEEE 30TH COMPUTER SECURITY FOUNDATIONS SYMPOSIUM (CSF)}},
Series = {{Proceedings-Computer Security Foundations Workshop}},
Year = {{2017}},
Pages = {{249-262}},
Note = {{30th IEEE Computer Security Foundations Symposium (CSF), Santa Barbara,
   CA, AUG 21-25, 2017}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Comp Soc Tech Comm Secur \& Privacy}},
Abstract = {{More and more quantum algorithms have been designed for solving problems
   in machine learning, database search and data analytics. An important
   problem then arises: how privacy can be protected when these algorithms
   are used on private data? For classical computing, the notion of
   differential privacy provides a very useful conceptual framework in
   which a great number of mechanisms that protect privacy by introducing
   certain noises into algorithms have been successfully developed. This
   paper defines a notion of differential privacy for quantum information
   processing. We carefully examine how the mechanisms using three
   important types of quantum noise, the amplitude/phase damping and
   depolarizing, can protect differential privacy. A composition theorem is
   proved that enables us to combine multiple privacy-preserving operations
   in quantum information processing.}},
DOI = {{10.1109/CSF.2017.23}},
ISSN = {{1063-6900}},
ISBN = {{978-1-5386-3217-8}},
ResearcherID-Numbers = {{Ying, Mingsheng/A-8379-2008
   Huang, Meijun/G-4406-2012}},
ORCID-Numbers = {{Ying, Mingsheng/0000-0003-4847-702X
   }},
Unique-ID = {{ISI:000426068500018}},
}

@inproceedings{ ISI:000426121000022,
Author = {Helsloot, Leon J. and Tillem, Gamze and Erkin, Zekeriya},
Book-Group-Author = {{IEEE}},
Title = {{AHEad: Privacy-preserving Online Behavioural Advertising using
   Homomorphic Encryption}},
Booktitle = {{2017 IEEE WORKSHOP ON INFORMATION FORENSICS AND SECURITY (WIFS)}},
Series = {{IEEE International Workshop on Information Forensics and Security}},
Year = {{2017}},
Note = {{9th IEEE Workshop on Information Forensics and Security (WIFS), Rennes,
   FR POLYNESIA, DEC 04-07, 2017}},
Organization = {{IEEE}},
Abstract = {{Online advertising is a rapidly growing industry, forming the primary
   source of income for many publishers that offer free web content. The
   practice of serving advertisements based on individuals' interests
   greatly improves the expected effectiveness of advertisements, and is
   believed to be beneficial to publishers and users alike. However, the
   widespread data collection required for such behavioural advertising
   sparks concerns over user privacy. In this paper, we present AHEad, a
   privacypreserving protocol for Online Behavioural Advertising that
   ensures user privacy by processing data in encrypted form. AHEad
   combines homomorphic encryption with a machine learning method commonly
   encountered in existing advertising systems. Advertisements are served
   based on detailed user profiles, while achieving performance linear in
   the size of user profiles. To the best of our knowledge, AHEad is the
   first protocol that preserves user privacy in behavioural advertising
   while allowing the use of detailed user profiles and machine learning
   methods.}},
ISSN = {{2157-4766}},
ISBN = {{978-1-5090-6769-5}},
Unique-ID = {{ISI:000426121000022}},
}

@inproceedings{ ISI:000425232200012,
Author = {Wang, Qian and Hu, Shengshan and Du, Minxin and Wang, Jingjun and Ren,
   Kui},
Book-Group-Author = {{IEEE}},
Title = {{Learning Privately: Privacy-Preserving Canonical Correlation Analysis
   for Cross-Media Retrieval}},
Booktitle = {{IEEE INFOCOM 2017 - IEEE CONFERENCE ON COMPUTER COMMUNICATIONS}},
Series = {{IEEE INFOCOM}},
Year = {{2017}},
Note = {{IEEE Conference on Computer Communications (INFOCOM), Atlanta, GA, MAY
   01-04, 2017}},
Organization = {{IEEE}},
Abstract = {{A massive explosion of various types of data has been triggered in the
   ``Big Data{''} era. In big data systems, machine learning plays an
   important role due to its effectiveness in discovering hidden
   information and valuable knowledge. Data privacy, however, becomes an
   unavoidable concern since big data usually involve multiple
   organizations, e.g., different healthcare systems and hospitals, who are
   not in the same trust domain and may be reluctant to share their data
   publicly. Applying traditional cryptographic tools is a straightforward
   approach to protect sensitive information, but it often renders learning
   algorithms useless inevitably. In this work, we, for the first time,
   propose a novel privacy-preserving scheme for canonical correlation
   analysis (CCA), which is a well-known learning technique and has been
   widely used in cross-media retrieval system. We first develop a library
   of building blocks to support various arithmetics over encrypted real
   numbers by leveraging additively homomorphic encryption and garbled
   circuits. Then we encrypt private data by randomly splitting the
   numerical data, formalize CCA problem and reduce it to a symmetric
   eigenvalue problem by designing new protocols for privacy-preserving QR
   decomposition. Finally, we solve all the eigenvalues and the
   corresponding eigenvectors by running Newton-Raphson method and inverse
   power method over the ciphertext domain. We carefully analyze the
   security and extensively evaluate the effectiveness of our design. The
   results show that our scheme is practically secure, incurs negligible
   errors compared with performing CCA in the clear and performs comparably
   in cross-media retrieval systems.}},
ISSN = {{0743-166X}},
ISBN = {{978-1-5090-5336-0}},
Unique-ID = {{ISI:000425232200012}},
}

@inproceedings{ ISI:000425458700072,
Author = {Chanyaswad, Thee and Al, Mert and Chang, J. Morris and Kung, S. Y.},
Editor = {{Ueda, N and Watanabe, S and Matsui, T and Chien, JT and Larsen, J}},
Title = {{DIFFERENTIAL MUTUAL INFORMATION FORWARD SEARCH FOR MULTI-KERNEL
   DISCRIMINANT-COMPONENT SELECTION WITH AN APPLICATION TO
   PRIVACY-PRESERVING CLASSIFICATION}},
Booktitle = {{2017 IEEE 27TH INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL
   PROCESSING}},
Series = {{IEEE International Workshop on Machine Learning for Signal Processing}},
Year = {{2017}},
Note = {{27th IEEE International Workshop on Machine Learning for Signal
   Processing (MLSP), Int House Japan, Tokyo, JAPAN, SEP 25-28, 2017}},
Organization = {{IEEE; IEEE Signal Proc Soc, Machine Learning Signal Proc Tech Comm}},
Abstract = {{In machine learning, feature engineering has been a pivotal stage in
   building a high-quality predictor. Particularly, this work explores the
   multiple Kernel Discriminant Component Analysis (mKDCA) feature-map and
   its variants. However, seeking the right subset of kernels for mKDCA
   feature-map can be challenging. Therefore, we consider the problem of
   kernel selection, and propose an algorithm based on Differential Mutual
   Information (DMI) and incremental forward search. DMI serves as an
   effective metric for selecting kernels, as is theoretically supported by
   mutual information and Fisher's discriminant analysis. On the other
   hand, incremental forward search plays a role in removing redundancy
   among kernels. Finally, we illustrate the potential of the method via an
   application in privacy-aware classification, and show on three
   mobile-sensing datasets that selecting an effective set of kernels for
   mKDCA feature-maps can enhance the utility classification performance,
   while successfully preserve the data privacy. Specifically, the results
   show that the proposed DMI forward search method can perform better than
   the state-of-the-art, and, with much smaller computational cost, can
   perform as well as the optimal, yet computationally expensive,
   exhaustive search.}},
ISSN = {{2161-0363}},
ISBN = {{978-1-5090-6341-3}},
Unique-ID = {{ISI:000425458700072}},
}

@inproceedings{ ISI:000424741600244,
Author = {Senekane, Makhamisa and Mafu, Mhlambululi and Taele, Benedict Molibeli},
Editor = {{Cornish, DR}},
Title = {{Privacy-preserving quantum machine learning using differential privacy}},
Booktitle = {{2017 IEEE AFRICON}},
Series = {{Africon}},
Year = {{2017}},
Pages = {{1432-1435}},
Note = {{IEEE AFRICON Conference - Science, Technology and Innovation for Africa,
   Cape Town, SOUTH AFRICA, SEP 18-20, 2017}},
Organization = {{IEEE; mlab; IEEE Reg 8; IEEE S Africa Sect; IES; Univ Pretoria; SAiEE;
   IBM; Altair; CST}},
Abstract = {{The advance of artificial intelligence in general and machine learning
   in particular has resulted in the need to pay more attention to the
   provision of privacy to the data being anlyzed. An example of sensitive
   data analysis might be in the analysis of individuals' medical records.
   In such a case, there might be a need to draw insights from data while
   at the same time maintaining privacy of the participants. Such cases
   have given birth to privacy-preserving data analyitics. Privacy is
   typically guaranteed by a differentially private mechanism. In this
   paper, we present a novel mechanism for privacy-preserving quantum
   machine learning. The mechanism is tested on the sensitive dataset that
   contains features and target labels for breast cancer prediction. The
   results obtained underline the utility of this mechanism.}},
ISSN = {{2153-0025}},
ISBN = {{978-1-5386-2775-4}},
Unique-ID = {{ISI:000424741600244}},
}

@inproceedings{ ISI:000414286206112,
Author = {Hamm, Jihun and Luken, Jackson and Xie, Yani},
Book-Group-Author = {{IEEE}},
Title = {{CROWD-ML: A LIBRARY FOR PRIVACY-PRESERVING MACHINE LEARNING ON SMART
   DEVICES}},
Booktitle = {{2017 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)}},
Series = {{International Conference on Acoustics Speech and Signal Processing
   ICASSP}},
Year = {{2017}},
Pages = {{6394-6398}},
Note = {{IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP), New Orleans, LA, MAR 05-09, 2017}},
Organization = {{IEEE; Inst Elect \& Elect Engineers, Signal Proc Soc}},
Abstract = {{When user-generated data such as audio and video signals are used to
   train machine learning algorithms, users' privacy must be considered
   before the learned model is released. In this work, we present an
   open-source library for privacy-preserving machine learning framework on
   smart devices. The library allows Android and iOS devices to
   collectively learn a common classifier/regression model from distributed
   data with differential privacy, using a variant of minibatch stochastic
   gradient descent method. The library allows researchers and developers
   to easily implement and deploy customized tasks that use on-device
   sensors to collect sensitive data for machine learning.}},
ISSN = {{1520-6149}},
ISBN = {{978-1-5090-4117-6}},
Unique-ID = {{ISI:000414286206112}},
}

@inproceedings{ ISI:000413081300002,
Author = {Mohassel, Payman and Zhang, Yupeng},
Book-Group-Author = {{IEEE}},
Title = {{SecureML: A System for Scalable Privacy-Preserving Machine Learning}},
Booktitle = {{2017 IEEE SYMPOSIUM ON SECURITY AND PRIVACY (SP)}},
Series = {{IEEE Symposium on Security and Privacy}},
Year = {{2017}},
Pages = {{19-38}},
Note = {{38th IEEE Symposium on Security and Privacy (SP), San Jose, CA, MAY
   22-26, 2017}},
Organization = {{IEEE; IEEE Comp Soc Tech Comm Secur \& Privacy; IEEE Comp Soc; Natl Sci
   Fdn}},
Abstract = {{Machine learning is widely used in practice to produce predictive models
   for applications such as image processing, speech and text recognition.
   These models are more accurate when trained on large amount of data
   collected from different sources. However, the massive data collection
   raises privacy concerns.
   In this paper, we present new and efficient protocols for privacy
   preserving machine learning for linear regression, logistic regression
   and neural network training using the stochastic gradient descent
   method. Our protocols fall in the two-server model where data owners
   distribute their private data among two non-colluding servers who train
   various models on the joint data using secure two-party computation
   (2PC). We develop new techniques to support secure arithmetic operations
   on shared decimal numbers, and propose MPC-friendly alternatives to
   non-linear functions such as sigmoid and softmax that are superior to
   prior work.
   We implement our system in C++. Our experiments validate that our
   protocols are several orders of magnitude faster than the state of the
   art implementations for privacy preserving linear and logistic
   regressions, and scale to millions of data samples with thousands of
   features. We also implement the first privacy preserving system for
   training neural networks.}},
DOI = {{10.1109/SP.2017.12}},
ISSN = {{1081-6011}},
ISBN = {{978-1-5090-5533-3}},
Unique-ID = {{ISI:000413081300002}},
}

@inproceedings{ ISI:000413081300058,
Author = {Olejnik, Katarzyna and Dacosta, Italo and Machado, Joana Soares and
   Huguenin, Kevin and Khan, Mohammad Emtiyaz and Hubaux, Jean-Pierre},
Book-Group-Author = {{IEEE}},
Title = {{SmarPer: Context-Aware and Automatic Runtime-Permissions for Mobile
   Devices}},
Booktitle = {{2017 IEEE SYMPOSIUM ON SECURITY AND PRIVACY (SP)}},
Series = {{IEEE Symposium on Security and Privacy}},
Year = {{2017}},
Pages = {{1058-1076}},
Note = {{38th IEEE Symposium on Security and Privacy (SP), San Jose, CA, MAY
   22-26, 2017}},
Organization = {{IEEE; IEEE Comp Soc Tech Comm Secur \& Privacy; IEEE Comp Soc; Natl Sci
   Fdn}},
Abstract = {{Permission systems are the main defense that mobile platforms, such as
   Android and iOS, offer to users to protect their private data from
   prying apps. However, due to the tension between usability and control,
   such systems have several limitations that often force users to
   overshare sensitive data. We address some of these limitations with
   SmarPer, an advanced permission mechanism for Android. To address the
   rigidity of current permission systems and their poor matching of users'
   privacy preferences, SmarPer relies on contextual information and
   machine learning methods to predict permission decisions at runtime.
   Note that the goal of SmarPer is to mimic the users' decisions, not to
   make privacy-preserving decisions per se. Using our SmarPer
   implementation, we collected 8,521 runtime permission decisions from 41
   participants in real conditions. With this unique data set, we show that
   using an efficient Bayesian linear regression model results in a mean
   correct classification rate of 80\% (+/- 3\%). This represents a mean
   relative reduction of approximately 50\% in the number of incorrect
   decisions when compared with a user-defined static permission policy,
   i.e., the model used in current permission systems. SmarPer also focuses
   on the suboptimal trade-off between privacy and utility; instead of only
   ``allow{''} or ``deny{''} type of decisions, SmarPer also offers an
   ``obfuscate{''} option where users can still obtain utility by revealing
   partial information to apps. We implemented obfuscation techniques in
   SmarPer for different data types and evaluated them during our data
   collection campaign. Our results show that 73\% of the participants
   found obfuscation useful and it accounted for almost a third of the
   total number of decisions. In short, we are the first to show, using a
   large dataset of real in situ permission decisions, that it is possible
   to learn users' unique decision patterns at runtime using contextual
   information while supporting data obfuscation; this is an important step
   towards automating the management of permissions in smartphones.}},
DOI = {{10.1109/SP.2017.25}},
ISSN = {{1081-6011}},
ISBN = {{978-1-5090-5533-3}},
ORCID-Numbers = {{Huguenin, Kevin/0000-0001-7147-1828}},
Unique-ID = {{ISI:000413081300058}},
}

@inproceedings{ ISI:000413241700042,
Author = {Ligier, Damien and Carpov, Sergiu and Fontaine, Caroline and Sirdey,
   Renaud},
Editor = {{Mori, P and Furnell, S and Camp, O}},
Title = {{Privacy Preserving Data Classification using Inner-product Functional
   Encryption}},
Booktitle = {{ICISSP: PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON INFORMATION
   SYSTEMS SECURITY AND PRIVACY}},
Year = {{2017}},
Pages = {{423-430}},
Note = {{3rd International Conference on Information Systems Security and Privacy
   (ICISSP), Porto, PORTUGAL, FEB 19-21, 2017}},
Abstract = {{In the context of data outsourcing more and more concerns raise about
   the privacy of user's data. Simultaneously, cryptographers are designing
   schemes enabling computation on ciphertexts (homomorphic encryption,
   functional encryption, etc.). Their use in real world applications is
   difficult. In this work we focus on functional encryption schemes
   enabling computation of inner-product on encrypted vectors and their use
   in real world scenarios. We propose a protocol combining such type of
   functional encryption schemes with machine learning algorithms. Indeed,
   we think that being able to perform classification over encrypted data
   is useful in many scenarios, in particular when the owners of the data
   are not ready to share it. After explaining our protocol, we detail the
   implemented handwritten digit recognition use case, and then, we study
   its security.}},
DOI = {{10.5220/0006206704230430}},
ISBN = {{978-989-758-209-7}},
Unique-ID = {{ISI:000413241700042}},
}

@inproceedings{ ISI:000411208400046,
Author = {Murmuria, Rahul and Stavrou, Angelos and Barbara, Daniel and Sritapan,
   Vincent},
Book-Group-Author = {{IEEE}},
Title = {{Your Data in Your Hands: Privacy-preserving User Behavior Models for
   Context Computation}},
Booktitle = {{2017 IEEE INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING AND
   COMMUNICATIONS WORKSHOPS (PERCOM WORKSHOPS)}},
Series = {{International Conference on Pervasive Computing and Communications}},
Year = {{2017}},
Note = {{IEEE International Conference on Pervasive Computing and Communications
   (PerCom), HI, MAR 13-17, 2017}},
Organization = {{IEEE}},
Abstract = {{Modern smartphone applications rely on contextual information while
   providing the users with relevant and timely content and services. One
   way of generating such contextual information is by employing learning
   systems to model user behavior. Motion-based sensors, such as the
   accelerometer or gyroscope, have been previously employed for
   recognizing pre-defined high-level physical activities such as climbing
   stairs, jogging, or driving. In practice, human activities are highly
   diverse and unsupervised methods must be used to expose complex
   behavioral characteristics that are user-centric. This paper proposes a
   novel machine learning model for user authentication and trust that is
   continuously assessing the user activities in an effort to expose
   deviations from known training data. The goal is to export this trust
   score as a contextual input to mobile apps for detection of unauthorized
   access, fraudulent transactions, the progress of a disease, or other
   behavioral changes such as stage fright, intoxicated behavior, or mood
   changes. All collected data and generated models of the user remains on
   the smartphone, and only the score needs to be revealed to the apps. As
   a result, the user controls the data without the need to share with any
   remote entity. The paper presents preliminary performance results of
   this technique.}},
ISSN = {{2474-2503}},
ISBN = {{978-1-5090-4338-5}},
ResearcherID-Numbers = {{Murmuria, Rahul/I-2313-2019}},
ORCID-Numbers = {{Murmuria, Rahul/0000-0001-6327-9609}},
Unique-ID = {{ISI:000411208400046}},
}

@article{ ISI:000408452200001,
Author = {Leppakoski, Helena and Rivero-Rodriguez, Alejandro and Rautalin, Sakari
   and Martinez, David Munoz and Kappi, Jani and Ali-Loytty, Simo and
   Piche, Robert},
Title = {{Semantic Labeling of User Location Context Based on Phone Usage Features}},
Journal = {{MOBILE INFORMATION SYSTEMS}},
Year = {{2017}},
Abstract = {{In mobile phones, the awareness of the user's context allows services
   better tailored to the user's needs. We propose a machine learning based
   method for semantic labeling that utilizes phone usage features to
   detect the user's home, work, and other visited places. For place
   detection, we compare seven different classification methods. We
   organize the phone usage data based on periods of uninterrupted time
   that the user has been in a certain place. We consider three approaches
   to represent this data: visits, places, and cumulative samples. Our main
   contribution is semantic place labeling using a small set of
   privacy-preserving features and novel data representations suitable for
   resource constrained mobile devices. The contributions include (1)
   introduction of novel data representations including accumulation and
   averaging of the usage, (2) analysis of the effect of the data
   accumulation time on the accuracy of the place classification, (3)
   analysis of the confidence on the classification outcome, and (4)
   identification of the most relevant features obtained through feature
   selection methods. With a small set of privacy-preserving features and
   our data representations, we detect the user's home and work with
   probability of 90\% or better, and in 3-class problem the overall
   classification accuracy was 89\% or better.}},
DOI = {{10.1155/2017/3876906}},
Article-Number = {{3876906}},
ISSN = {{1574-017X}},
EISSN = {{1875-905X}},
ResearcherID-Numbers = {{Piche, Robert/H-4859-2012
   Leppakoski, Helena/A-5295-2016
   Ali-Loytty, Simo/B-9925-2015}},
ORCID-Numbers = {{Piche, Robert/0000-0003-1158-6951
   Leppakoski, Helena/0000-0002-2333-3065
   Ali-Loytty, Simo/0000-0002-6720-7722}},
Unique-ID = {{ISI:000408452200001}},
}

@inproceedings{ ISI:000403607900030,
Author = {Lee, Hosub and Kobsa, Alfred},
Book-Group-Author = {{IEEE}},
Title = {{Privacy Preference Modeling and Prediction in a Simulated Campuswide IoT
   Environment}},
Booktitle = {{2017 IEEE INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING AND
   COMMUNICATIONS (PERCOM)}},
Series = {{International Conference on Pervasive Computing and Communications}},
Year = {{2017}},
Note = {{IEEE International Conference on Pervasive Computing and Communications
   (PerCom), HI, MAR 13-17, 2017}},
Organization = {{IEEE}},
Abstract = {{With the advent of the Internet of Things (IoT), users are more likely
   to have privacy concerns since their personal information could be
   collected, analyzed, and utilized without notice by the networked IoT
   devices and services. Users may want to control all such activities by
   explicitly expressing their privacy preferences. However, it is becoming
   increasingly difficult for users to do so, not only because of the
   cognitive burden of continuously making privacy decisions for IoT
   services, but also because IoT devices have no, or only very restricted,
   user interfaces. Intelligent software helping users make better privacy
   decisions will be an important component of privacy preserving IoT
   environments. In order to construct such a component, we aim to verify
   whether it will be possible to computationally model and predict users'
   privacy preferences in IoT. To that end, we survey 172 participants in a
   simulated campuswide IoT environment about their privacy preferences
   regarding hypothetical personal information tracking scenarios. Then, we
   cluster the scenarios based on the survey responses, arriving at four
   clusters with distinct associated privacy preferences. Based on the
   clustering results, we uncover contextual factors that induce privacy
   violations in IoT. Finally, we build machine learning models to predict
   users' privacy decisions, using both contextual information and the
   corresponding cluster membership as training data. The final trained
   model shows 77\% accuracy in predicting users' decisions whether or not
   to allow the respective IoT scenario.}},
ISSN = {{2474-2503}},
ISBN = {{978-1-5090-4327-9}},
Unique-ID = {{ISI:000403607900030}},
}

@article{ ISI:000397388800002,
Author = {Bindschaedler, Vincent and Shokri, Reza and Gunter, Carl A.},
Title = {{Plausible Deniability for Privacy-Preserving Data Synthesis}},
Journal = {{PROCEEDINGS OF THE VLDB ENDOWMENT}},
Year = {{2017}},
Volume = {{10}},
Number = {{5}},
Pages = {{481-492}},
Month = {{JAN}},
Abstract = {{Releasing full data records is one of the most challenging problems in
   data privacy. On the one hand, many of the popular techniques such as
   data de-identification are problematic because of their dependence on
   the background knowledge of adversaries. On the other hand, rigorous
   methods such as the exponential mechanism for differential privacy are
   often computationally impractical to use for releasing high dimensional
   data or cannot preserve high utility of original data due to their
   extensive data perturbation.
   This paper presents a criterion called plausible deniability that
   provides a formal privacy guarantee, notably for releasing sensitive
   datasets: an output record can be released only if a certain amount of
   input records are indistinguishable, up to a privacy parameter. This
   notion does not depend on the background knowledge of an adversary.
   Also, it can efficiently be checked by privacy tests. We present
   mechanisms to generate synt he tic data sets with similar statistical
   properties to the input data and the same format. We study this
   technique both theoretically and experimentally. A key theoretical
   result shows that, with proper randomization, the plausible deniability
   mechanism generates differentially private synthetic data. We
   demonstrate the efficiency of this generative technique on a large
   dataset; it is shown to preserve the utility of original data with
   respect to various statistical analysis and machine learning measures.}},
DOI = {{10.14778/3055540.3055542}},
ISSN = {{2150-8097}},
Unique-ID = {{ISI:000397388800002}},
}

@inproceedings{ ISI:000454622300060,
Author = {Cyphers, Bennett and Veeramachaneni, Kalyan},
Book-Group-Author = {{IEEE}},
Title = {{AnonML: Locally private machine learning over a network of peers}},
Booktitle = {{2017 IEEE INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED
   ANALYTICS (DSAA)}},
Series = {{Proceedings of the International Conference on Data Science and Advanced
   Analytics}},
Year = {{2017}},
Pages = {{549-560}},
Note = {{4th IEEE / ACM / ASA International Conference on Data Science and
   Advanced Analytics (DSAA), Tokyo, JAPAN, OCT 19-21, 2017}},
Organization = {{IEEE; IEEE Computat Intelligence Soc; Kozo Keikaku Engn Inc; NEC Corp;
   Air Force Off Sci Res, Asian Off Aerosp Res \& Dev; U S Army RDECOM;
   FEG, NS Solut Grp; KDDI Res; ACM; Amer Stat Assoc; Off Naval Res Global;
   Int Technol Ctr Pacific; Financial Engn Grp; Gunosy; FRONTEO; Automagi;
   Sansan; XCompass; Google; RECRUIT Holfdings; LIFULL; FINATEXT; Yahoo
   Japan Corp; Panasonic; Honda Res Inst Japan}},
Abstract = {{We present AnonML, a system for privacy preserving model generation over
   a network of peers. Our goal is to allow a group of users to combine
   enough data to generate useful machine learning models without revealing
   private information. In our setting, each peer has a single row of
   featurized data according to a shared schema, and an aggregator would
   like to train a binary classification model on the union of all peers'
   data. Our system horizontally and vertically partitions the set of all
   peers' data and assembles a differentially-private histogram for each
   partition. An ensemble classifier can then be trained on the set of
   noisy partitions. AnonML can be used with or without differentially
   private data perturbation. Without perturbation, the resulting
   classifiers achieve performance competitive with centrally-generated
   models. With local differential privacy, a strong theoretical guarantee,
   AnonML is capable of producing useful models for practical prediction
   problems.}},
DOI = {{10.1109/DSAA.2017.80}},
ISSN = {{2472-1573}},
ISBN = {{978-1-5090-5004-8}},
Unique-ID = {{ISI:000454622300060}},
}

@inproceedings{ ISI:000426121000024,
Author = {Razeghi, Behrooz and Voloshynovskiy, Slava and Kostadinov, Dimche and
   Taran, Olga},
Book-Group-Author = {{IEEE}},
Title = {{Privacy Preserving Identification Using Sparse Approximation with
   Ambiguization}},
Booktitle = {{2017 IEEE WORKSHOP ON INFORMATION FORENSICS AND SECURITY (WIFS)}},
Series = {{IEEE International Workshop on Information Forensics and Security}},
Year = {{2017}},
Note = {{9th IEEE Workshop on Information Forensics and Security (WIFS), Rennes,
   FR POLYNESIA, DEC 04-07, 2017}},
Organization = {{IEEE}},
Abstract = {{In this paper, we consider a privacy preserving encoding framework for
   identification applications covering biometrics, physical object
   security and the Internet of Things (IoT). The proposed framework is
   based on a sparsifying transform, which consists of a trained linear
   map, an element-wise nonlinearity, and privacy amplification. The
   sparsifying transform and privacy amplification are not symmetric for
   the data owner and data user. We demonstrate that the proposed approach
   is closely related to sparse ternary codes (STC), a recent
   information-theoretic concept proposed for fast approximate nearest
   neighbor (ANN) search in high dimensional feature spaces that being
   machine learning in nature also offers significant benefits in
   comparison to sparse approximation and binary embedding approaches. We
   demonstrate that the privacy of the database outsourced to a server as
   well as the privacy of the data user are preserved at a low
   computational cost, storage and communication burdens.}},
ISSN = {{2157-4766}},
ISBN = {{978-1-5090-6769-5}},
Unique-ID = {{ISI:000426121000024}},
}

@inproceedings{ ISI:000455029500203,
Author = {Yang, Mary Qu and Yu, Shucheng and Yang, William and Milanova,
   Mariofanna and Zhao, Wenbing and Yang, Jack Y. and Arabnia, Hamid R.},
Editor = {{Arabnia, HR and Deligiannidis, L and Tinetti, FG and Tran, QN and Yang, MQ}},
Title = {{Developing Secure Privacy Preserving and Causal Genetic Alteration
   Research in Building an Innovative Systematic Pedagogy for Integrated
   Research and Education - The INSPIRE Model}},
Booktitle = {{PROCEEDINGS 2017 INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE AND
   COMPUTATIONAL INTELLIGENCE (CSCI)}},
Year = {{2017}},
Pages = {{1149-1154}},
Note = {{International Conference on Computational Science and Computational
   Intelligence (CSCI), Las Vegas, NV, DEC 14-16, 2017}},
Organization = {{American Council Sci \& Educ}},
Abstract = {{To meet current keen demand for producing next-generation workforce
   equipped with skills and expertise in big-data analytics, we developed
   an innovative systematic pedagogy for integrated research-education
   (INSPIRE model) that is centered around two great challenges: (1)
   Transforming multidisciplinary STEM training so that it enhances
   emerging problem-solving capacity and (2) Training STEM students how to
   have a bigger hand in performing large-scale scientific work. To help
   strengthen the problem-solving skills and leadership abilities of STEM
   graduates, we reform the current STEM research training in
   Bioinformatics and CIS (Computer and Information Sciences) so that it
   helps us reach the goal of catalyzing science and research training. Our
   main research hypothesis is that critical improvement in the way
   big-data scientists are trained comes not solely from large-scale data
   mining but, in addition, comes from developing useful machine learning
   and artificial intelligence techniques that automate intelligent
   learning derived from big-data. The INSPIRE model was built by enablers
   in the scientific community, and indeed, by the community at large, to
   help resolve the scarcity of those Professionally Skilled / Trained in
   Big Data analytics (PSTBD) issue by equipping students with a versatile
   cross-disciplinary skill set. There is a dire need for those of us in
   the scientific and academic community to be able to transfer our own
   successes into perfecting the feedback-based machine learning -
   cognitive science INSPIRE model, one that places a heavy emphasis on
   providing individualized training to individuals from all walks of life,
   including large populations of minorities and women, so that all efforts
   are made as collaboratively as possible, and the benefits of the sewn
   seeds may be reaped by everyone. We integrate our secure privacy
   preserving and causal genetic alteration research at single-cell
   resolution to demonstrate the model. On an even grander scale, we
   enhance the PSTBD research by developing the INSPIRE model so that
   broader social impacts can be made by such newly created fields as
   Systems Genomics at single-cell level and fields fostered by creative
   cross-disciplinary genomic big-data analytics
   (http://americancse.org/events/csce2017/keynotes\_lectures/yang\_talk)
   with catalyzed learning-research synergies.}},
DOI = {{10.1109/CSCI.2017.200}},
ISBN = {{978-1-5386-2652-8}},
Unique-ID = {{ISI:000455029500203}},
}

@inproceedings{ ISI:000455029500221,
Author = {Yang, Mary Qu and Yu, Shucheng and Cruz-Neira, Carolina and Yang,
   William and Tudoreanu, M. Eduard and Li, Dan and Zhang, Yifan and He,
   Qingfang and Guan, Renchu and Wang, Richard Y. and Zhao, Wenbing},
Editor = {{Arabnia, HR and Deligiannidis, L and Tinetti, FG and Tran, QN and Yang, MQ}},
Title = {{Secure privacy preserving across personal health data and single cell
   genomics research INSPIRE academic pedagogy -merging big data
   multiplatform with deep learning}},
Booktitle = {{PROCEEDINGS 2017 INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE AND
   COMPUTATIONAL INTELLIGENCE (CSCI)}},
Year = {{2017}},
Pages = {{1244-1251}},
Note = {{International Conference on Computational Science and Computational
   Intelligence (CSCI), Las Vegas, NV, DEC 14-16, 2017}},
Organization = {{American Council Sci \& Educ}},
Abstract = {{Enhancing student academic performance and transdisciplinary ability is
   challenging, but the time and effort put into accomplishing this
   ambitious feat is priceless. We develop secure privacy preserving across
   Personal Health Data (PHD) repository and single-cell genomics research
   for building an Innovative Systematic Pedagogy for Integrated Research -
   Education (INSPIRE)
   (http://americancse.org/events/csce2017/csce17\_awards). In this paper
   we further build a novel, eclectic, and insightful framework based on
   classical and popular machine learning approaches to help us meet the
   educational challenge. Our framework focuses on using integrative
   research technologies to help solve ``Education's Performance Prediction
   Data Mining Crisis{''} (EPPDMC), by putting to rest issues associated
   with mining and making best use of big data for educational enhancement,
   such as multi-source education acquisition, data fusion, and
   unstructured data analysis. We exploit the uses of deep learning, text
   classification, and semi-supervised learning approaches to solve
   challenging problems that educators face when analyzing multiplatform
   big data involved in education, research and training students. Based on
   new machine learning approached we developed for genomic big-data
   research and in combination with machine learning methods
   (http://americancse.org/events/csce2017/keynotes\_lectures/yang\_talk)
   and the vast availability of education data available to us, not only
   can we utilize structured, unstructured, and even multi-media data, but
   while engaging in leaning intelligent thinking along the way, we can
   also maximize the utilization of big data by studying the motion and
   performance of these data. Hence we build the INSPIRE model that can
   further incorporate Student Face Expression in Class (SFEiC) to help
   educators and managers make further improvements as they become involved
   in the teaching-learning process. This research further facilitates the
   effectiveness of the INSPIRE model.}},
DOI = {{10.1109/CSCI.2017.219}},
ISBN = {{978-1-5386-2652-8}},
ResearcherID-Numbers = {{Guan, Renchu/S-5503-2019}},
ORCID-Numbers = {{Guan, Renchu/0000-0002-7162-7826}},
Unique-ID = {{ISI:000455029500221}},
}

@article{ ISI:000388122000013,
Author = {Zhang, Tao and Zhu, Quanyan},
Title = {{Dynamic Differential Privacy for ADMM-Based Distributed Classification
   Learning}},
Journal = {{IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY}},
Year = {{2017}},
Volume = {{12}},
Number = {{1}},
Pages = {{172-187}},
Month = {{JAN}},
Abstract = {{Privacy-preserving distributed machine learning becomes increasingly
   important due to the recent rapid growth of data. This paper focuses on
   a class of regularized empirical risk minimization machine learning
   problems, and develops two methods to provide differential privacy to
   distributed learning algorithms over a network. We first decentralize
   the learning algorithm using the alternating direction method of
   multipliers, and propose the methods of dual variable perturbation and
   primal variable perturbation to provide dynamic differential privacy.
   The two mechanisms lead to algorithms that can provide privacy
   guarantees under mild conditions of the convexity and differentiability
   of the loss function and the regularizer. We study the performance of
   the algorithms, and show that the dual variable perturbation outperforms
   its primal counterpart. To design an optimal privacy mechanism, we
   analyze the fundamental tradeoff between privacy and accuracy, and
   provide guidelines to choose privacy parameters. Numerical experiments
   using customer information database are performed to corroborate the
   results on privacy and utility tradeoffs and design.}},
DOI = {{10.1109/TIFS.2016.2607691}},
ISSN = {{1556-6013}},
EISSN = {{1556-6021}},
Unique-ID = {{ISI:000388122000013}},
}

@article{ ISI:000386225000002,
Author = {Al-Rubaie, Mohammad and Chang, J. Morris},
Title = {{Reconstruction Attacks Against Mobile-Based Continuous Authentication
   Systems in the Cloud}},
Journal = {{IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY}},
Year = {{2016}},
Volume = {{11}},
Number = {{12}},
Pages = {{2648-2663}},
Month = {{DEC}},
Abstract = {{Continuous authentication for mobile devices using behavioral biometrics
   is being suggested to complement initial authentication for securing
   mobile devices, and the cloud services accessed through them. This area
   has been studied over the past few years, and low error rates were
   achieved; however, it was based on training and testing using support
   vector machine (SVM) and other non-privacy-preserving machine learning
   algorithms. To stress the importance of carefully designed
   privacy-preserving systems, we investigate the possibility of
   reconstructing gestures raw data from users' authentication profiles or
   synthesized samples' testing results. We propose two types of
   reconstruction attacks based on whether actual user samples are
   available to the adversary (as in SVM profiles) or not. We also propose
   two algorithms to reconstruct raw data: a numerical-based algorithm that
   is specific to one compromised system, and a randomization-based
   algorithm that can work against almost any compromised system. For our
   experiments, we selected one compromised and four attacked gesture-based
   continuous authentication systems from the recent literature. The
   experiments, performed using a public data set, showed that the attacks
   were feasible, with a median ranging from 80\% to 100\% against one
   attacked system using all types of attacks and algorithms, and a median
   ranging from 73\% to 100\% against all attacked systems using the
   randomization-based algorithm and the negative support vector attack.
   Finally, we analyze the results, and provide recommendations for
   building active authentication systems that could resist reconstruction
   attacks.}},
DOI = {{10.1109/TIFS.2016.2594132}},
ISSN = {{1556-6013}},
EISSN = {{1556-6021}},
ResearcherID-Numbers = {{Chang, Jien/C-7672-2019}},
ORCID-Numbers = {{Chang, Jien/0000-0002-0660-7191}},
Unique-ID = {{ISI:000386225000002}},
}

@article{ ISI:000391905200018,
Author = {Jochems, Arthur and Deist, Timo M. and Van Soest, Johan and Eble,
   Michael and Bulens, Paul and Coucke, Philippe and Dries, Wim and Lambin,
   Philippe and Dekker, Andre},
Title = {{Distributed learning: Developing a predictive model based on data from
   multiple hospitals without data leaving the hospital - A real life proof
   of concept}},
Journal = {{RADIOTHERAPY AND ONCOLOGY}},
Year = {{2016}},
Volume = {{121}},
Number = {{3}},
Pages = {{459-467}},
Month = {{DEC}},
Abstract = {{Purpose: One of the major hurdles in enabling personalized medicine is
   obtaining sufficient patient data to feed into predictive models.
   Combining data originating from multiple hospitals is difficult because
   of ethical, legal, political, and administrative barriers associated
   with data sharing. In order to avoid these issues, a distributed
   learning approach can be used. Distributed learning is defined as
   learning from data without the data leaving the hospital.
   Patients and methods: Clinical data from 287 lung cancer patients,
   treated with curative intent with chemoradiation (CRT) or radiotherapy
   (RT) alone were collected from and stored in 5 different medical
   institutes (123 patients at MAASTRO (Netherlands, Dutch), 24 at Jessa
   (Belgium, Dutch), 34 at Liege (Belgium, Dutch and French), 48 at Aachen
   (Germany, German) and 58 at Eindhoven (Netherlands, Dutch)).
   A Bayesian network model is adapted for distributed learning (watch the
   animation: http://youtu.bei nQpqMIuHyOk). The model predicts dyspnea,
   which is a common side effect after radiotherapy treatment of lung
   cancer.
   Results: We show that it is possible to use the distributed learning
   approach to train a Bayesian network model on patient data originating
   from multiple hospitals without these data leaving the individual
   hospital. The AUC of the model is 0.61 (95\%Cl, 0.51-0.70) on a 5-fold
   cross-validation and ranges from 0.59 to 0.71 on external validation
   sets.
   Conclusion: Distributed learning can allow the learning of predictive
   models on data originating from multiple hospitals while avoiding many
   of the data sharing barriers. Furthermore, the distributed learning
   approach can be used to extract and employ knowledge from routine
   patient data from multiple hospitals while being compliant to the
   various national and European privacy laws. (C) 2016 The Author(s).
   Published by Elsevier Ireland Ltd.}},
DOI = {{10.1016/j.radonc.2016.10.002}},
ISSN = {{0167-8140}},
EISSN = {{1879-0887}},
ORCID-Numbers = {{Dekker, Andre/0000-0002-0422-7996
   van Soest, Johan/0000-0003-2548-0330}},
Unique-ID = {{ISI:000391905200018}},
}

@article{ ISI:000383756700002,
Author = {Sodiya, Adesina S. and Adegbuyi, B.},
Title = {{A Framework for Protecting Users' Privacy in Cloud}},
Journal = {{INTERNATIONAL JOURNAL OF INFORMATION SECURITY AND PRIVACY}},
Year = {{2016}},
Volume = {{10}},
Number = {{4}},
Pages = {{33-43}},
Month = {{OCT-DEC}},
Abstract = {{Data and document privacy concerns are increasingly important in the
   online world. In Cloud Computing, the story is the same, as the secure
   processing of personal data represents a huge challenge The main focus
   is to to preserve and protect personally identifiable information (PII)
   of individuals, customers, businesses, governments and organisations.
   The current use of anonymization techniques is not quite efficient
   because of its failure to use the structure of the datasets under
   consideration and inability to use a metric that balances the usefulness
   of information with privacy preservation. In this work, an adaptive
   lossy decomposition algorithm was developed for preserving privacy in
   cloud computing. The algorithm uses the foreign key associations to
   determine the generalizations possible for any attribute in the
   database. It generates penalties for each obscured attribute when
   sharing and proposes an optimal decomposition of the relation.
   Postgraduate database of Federal University of Agriculture, Abeokuta,
   Nigeria and Adult database provided at the UCIrvine Machine Learning
   Repository were used for the evaluation. The result shows a system that
   could be used to improve privacy in cloud computing.}},
DOI = {{10.4018/IJISP.2016100102}},
ISSN = {{1930-1650}},
EISSN = {{1930-1669}},
Unique-ID = {{ISI:000383756700002}},
}

@article{ ISI:000386431200059,
Author = {Machanavajjhala, Ashwin and He, Xi and Hay, Michael},
Title = {{Differential Privacy in the Wild: A tutorial on current practices \&
   open challenges}},
Journal = {{PROCEEDINGS OF THE VLDB ENDOWMENT}},
Year = {{2016}},
Volume = {{9}},
Number = {{13}},
Pages = {{1611-1614}},
Month = {{SEP}},
Abstract = {{Differential privacy has emerged as an important standard for privacy
   preserving computation over databases containing sensitive information
   about individuals. Research on differential privacy spanning a number of
   research areas, including theory, security, database, networks, machine
   learning, and statistics, over the last decade has resulted in a variety
   of privacy preserving algorithms for a number of analysis tasks. Despite
   maturing research efforts, the adoption of differential privacy by
   practitioners in industry, academia, or government agencies has so far
   been rare. Hence, in this tutorial, we will first describe the
   foundations of differentially private algorithm design that cover the
   state of the art in private computation on tabular data. In the second
   half of the tutorial we will highlight real world applications on
   complex data types, and identify research challenges in applying
   differential privacy to real world applications.}},
DOI = {{10.14778/3007263.3007322}},
ISSN = {{2150-8097}},
ORCID-Numbers = {{Hay, Michael/0000-0001-9085-893X}},
Unique-ID = {{ISI:000386431200059}},
}

@article{ ISI:000386365800010,
Author = {Thanabal, M. S. and Ganesan, L.},
Title = {{A Contrary Recurrent Item Set Hierarchy in PPDDM}},
Journal = {{NATIONAL ACADEMY SCIENCE LETTERS-INDIA}},
Year = {{2016}},
Volume = {{39}},
Number = {{4}},
Pages = {{283-286}},
Month = {{AUG}},
Abstract = {{Distributed data mining technology has become an eloquent tool for
   recognizing patterns and designating ideas from large pools of data
   procured from different sections (multiple parties). The primary focus
   of this research is to formulate association rules that are accepted
   universally, restraining the information shared about each party. A
   privacy-preserving algorithm is projected to mine association rules from
   horizontally partitioned data through a new Inverse Frequent Item set
   Tree. This tree was formulated using inverse frequent items and
   propagated only with the collaboration parties from whom the data was to
   be merged. A central third party was utilized in order to mine the
   association rules with infrequent item sets. The proposed approach
   constitutes a guided assistance in preserving corporate privacy,
   furthered by experimenting (analyzing) with the large data sets from the
   UCI machine learning repository. The resultant output from the approach
   showed positivity of preserving data with a high degree of security in
   multiparty computation.}},
DOI = {{10.1007/s40009-016-0460-2}},
ISSN = {{0250-541X}},
Unique-ID = {{ISI:000386365800010}},
}

@article{ ISI:000381563200015,
Author = {Aono, Yoshinori and Hayashi, Takuya and Phong, Le Trieu and Wang, Lihua},
Title = {{Privacy-Preserving Logistic Regression with Distributed Data Sources via
   Homomorphic Encryption}},
Journal = {{IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS}},
Year = {{2016}},
Volume = {{E99D}},
Number = {{8}},
Pages = {{2079-2089}},
Month = {{AUG}},
Abstract = {{Logistic regression is a powerful machine learning tool to classify
   data. When dealing with sensitive or private data, cares are necessary.
   In this paper, we propose a secure system for privacy-protecting both
   the training and predicting data in logistic regression via homomorphic
   encryption. Perhaps surprisingly, despite the non-polynomial tasks of
   training and predicting in logistic regression, we show that only
   additively homomorphic encryption is needed to build our system. Indeed,
   we instantiate our system with Paillier, LWE-based, and ring-LWE-based
   encryption schemes, highlighting the merits and demerits of each
   instantiation. Besides examining the costs of computation and
   communication, we carefully test our system over real datasets to
   demonstrate its utility.}},
DOI = {{10.1587/transinf.2015INP0020}},
ISSN = {{1745-1361}},
ORCID-Numbers = {{Hayashi, Takuya/0000-0001-5869-2319}},
Unique-ID = {{ISI:000381563200015}},
}

@article{ ISI:000367485300015,
Author = {Li, Yan and Bai, Changxin and Reddy, Chandan K.},
Title = {{A distributed ensemble approach for mining healthcare data under privacy
   constraints}},
Journal = {{INFORMATION SCIENCES}},
Year = {{2016}},
Volume = {{330}},
Number = {{SI}},
Pages = {{245-259}},
Month = {{FEB 10}},
Abstract = {{In recent years, electronic health records (EHRs) have been widely
   adapted at many healthcare facilities in an attempt to improve the
   quality of patient care and increase the productivity and efficiency of
   healthcare delivery. These EHRs can accurately diagnose diseases if
   utilized appropriately. While the EHRs can potentially resolve many of
   the existing problems associated with disease diagnosis, one of the main
   obstacles in effectively using them is the patient privacy and
   sensitivity of the medical information available in the EHR. Due to
   these concerns, even if the EHRs are available for storage and retrieval
   purposes, sharing of the patient records between different healthcare
   facilities has become a major concern and has hampered some of the
   effective advantages of using EHRs. Due to this lack of data sharing,
   most of the facilities aim at building clinical decision support systems
   using limited amount of patient data from their own EHR systems to
   provide important diagnosis related decisions. It becomes quite
   infeasible for a newly established healthcare facility to build a robust
   decision making system due to the lack of sufficient patient records.
   However, to make effective decisions from clinical data, it is
   indispensable to have large amounts of data to train the decision
   models. In this regard, there are conflicting objectives of preserving
   patient privacy and having sufficient data for modeling and decision
   making. To handle such disparate goals, we develop two adaptive
   distributed privacy-preserving algorithms based on a distributed
   ensemble strategy. The basic idea of our approach is to build an elegant
   model for each participating facility to accurately learn the data
   distribution, and then transfer the useful healthcare knowledge acquired
   on their data from these participators in the form of their own decision
   models without revealing and sharing the patient-level sensitive data,
   thus protecting patient privacy. We demonstrate that our approach can
   successfully build accurate and robust prediction models, under privacy
   constraints, using the healthcare data collected from different
   geographical locations. We demonstrate the performance of our method
   using the type-2 diabetes EHRs accumulated from multiple sources from
   all fifty states in the U.S. Our method was evaluated on diagnosing
   diabetes in the presence of insufficient number of patient records from
   certain regions without revealing the actual patient data from other
   regions. Using the proposed approach, we also discovered the important
   biomarkers, both universal and region-specific, and validated the
   selected biomarkers using the biomedical literature. (C) 2015 Elsevier
   Inc. All rights reserved.}},
DOI = {{10.1016/j.ins.2015.10.011}},
ISSN = {{0020-0255}},
EISSN = {{1872-6291}},
ORCID-Numbers = {{Reddy, Chandan/0000-0003-2839-3662}},
Unique-ID = {{ISI:000367485300015}},
}

@article{ ISI:000367276900055,
Author = {Li, Yun and Yang, Jun and Ji, Wei},
Title = {{Local learning-based feature weighting with privacy preservation}},
Journal = {{NEUROCOMPUTING}},
Year = {{2016}},
Volume = {{174}},
Number = {{B}},
Pages = {{1107-1115}},
Month = {{JAN 22}},
Abstract = {{The privacy-preserving data analysis has been gained significant
   interest across several research communities. The current researches
   mainly focus on privacy-preserving classification and regression. On the
   other hand, feature selection is also one of the key problems in data
   mining and machine learning. However, for privacy-preserving feature
   selection, the relevant papers are few. In this paper, a local
   learning-based feature weighting framework is introduced. Moreover, in
   order to preserve the data privacy during local learning-based feature
   selection, the objective perturbation and output perturbation strategies
   are used to produce local learning-based feature selection algorithms
   with privacy preservation. Meanwhile, we give deep analysis about their
   privacy preserving property based on the differential privacy model.
   Some experiments are conducted on benchmark data sets. The experimental
   results show that our algorithms can preserve the data privacy to some
   extent and the objective perturbation always obtains higher
   classification performance than output perturbation when the privacy
   preserving degree is constant (C) 2015 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.neucom.2015.10.038}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
Unique-ID = {{ISI:000367276900055}},
}

@inproceedings{ ISI:000409394400185,
Author = {Bassiou, Nikoletta and Tsiartas, Andreas and Smith, Jennifer and Bratt,
   Harry and Richey, Colleen and Shriberg, Elizabeth and D'Angelo, Cynthia
   and Alozie, Nonye},
Book-Group-Author = {{Int Speech Commun Assoc}},
Title = {{Privacy-Preserving Speech Analytics for Automatic Assessment of Student
   Collaboration}},
Booktitle = {{17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES}},
Series = {{Interspeech}},
Year = {{2016}},
Pages = {{888-892}},
Note = {{17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016), San
   Francisco, CA, SEP 08-12, 2016}},
Organization = {{apple; amazon alexa; Google; Microsoft; ebay; facebook; YAHOO JAPAN;
   Baidu Res; IBM Res; CIRRUS LOGIC; DATATANG; NUANCE; Speechocean Ltd;
   Yandex; Raytheon Technol}},
Abstract = {{This work investigates whether nonlexical information from speech can
   automatically predict the quality of small group collaborations. Audio
   was collected from students as they collaborated in groups of three to
   solve math problems. Experts in education annotated 30-second time
   windows by hand for collaboration quality. Speech activity features
   (computed at the group level) and spectral, temporal and prosodic
   features (extracted at the speaker level) were explored. After the
   latter were transformed from the speaker level to the group level,
   features were fused. Results using support vector machines and random
   forests show that feature fusion yields best classification performance.
   The corresponding unweighted average F1 measure on a 4-class prediction
   task ranges between 40\% and 50\%, significantly higher than chance
   (12\%). Speech activity features alone are strong predictors of
   collaboration quality, achieving an F1 measure between 35\% and 43\%.
   Speaker-based acoustic features alone achieve lower classification
   performance, but offer value in fusion. These findings illustrate that
   the approach under study offers promise for future monitoring of group
   dynamics, and should be attractive for many collaboration activity
   settings in which privacy is desired.}},
DOI = {{10.21437/Interspeech.2016-1569}},
ISSN = {{2308-457X}},
ISBN = {{978-1-5108-3313-5}},
Unique-ID = {{ISI:000409394400185}},
}

@inproceedings{ ISI:000406669300054,
Author = {Baryalai, Mehmood and Jang-Jaccard, Julian and Liu, Dongxi},
Book-Group-Author = {{IEEE}},
Title = {{Towards Privacy-Preserving Classification in Neural Networks}},
Booktitle = {{2016 14TH ANNUAL CONFERENCE ON PRIVACY, SECURITY AND TRUST (PST)}},
Series = {{Annual Conference on Privacy Security and Trust-PST}},
Year = {{2016}},
Note = {{14th Annual International Conference on Privacy, Security and Trust
   (PST), Auckland, NEW ZEALAND, DEC 12-14, 2016}},
Organization = {{Endace; Vodafone; ATEED; RealMe; Insomnia; InternetNZ; Aura Informat
   Secur; ASB; Check Point; Delta Insurance; Vigilance; Dimension Data;
   NICT Japan}},
Abstract = {{The requirement for data privacy is limiting to exploit the full
   potential of what modern data analytic capability could offer. To
   address such privacy concern, a number of techniques based on
   homomorphic encryption (HE) have been proposed to allow analytic
   computation, such as classification based on machine learning
   techniques, to run on encrypted data. However, these HE-based techniques
   suffer from a heavy computation overhead due to cryptographic
   computations having to be done on the encrypted data. We propose a
   non-colluding dual cloud system that utilizes Paillier cryptosystem. We
   illustrate how our proposal could reduce inherent computation overhead
   many similar techniques suffer. Such reduction could make our proposed
   system to be an ideal solution to use in the real world application.}},
ISSN = {{1712-364X}},
ResearcherID-Numbers = {{Liu, Dongxi/B-7488-2011}},
Unique-ID = {{ISI:000406669300054}},
}

@inproceedings{ ISI:000405708600033,
Author = {Hu, Donghui and Chen, Fan and Wu, Xintao and Zhao, Zhongqiu},
Book-Group-Author = {{IEEE}},
Title = {{A Framework of Privacy Decision Recommendation for Image Sharing in
   Online Social Networks}},
Booktitle = {{2016 IEEE FIRST INTERNATIONAL CONFERENCE ON DATA SCIENCE IN CYBERSPACE
   (DSC 2016)}},
Year = {{2016}},
Pages = {{243-251}},
Note = {{1st IEEE International Conference on Data Science in Cyberspace (DSC),
   Changsha, PEOPLES R CHINA, JUN 13-16, 2016}},
Organization = {{IEEE; IEEE Comp Soc; IEEE TCSC; Chinese Acad Engn; Natl Univ Defense
   Technol; Beijing Univ Posts \& Telecommunicat; Huawei; Peking Univ}},
Abstract = {{Image sharing in Online Social Networks (OSNs) faces potential risks of
   exposing users' private or sensitive information to others. In this
   paper, we develop a framework of calculating the privacy level of a
   digital image based on perceptual hashing and semantic privacy rules. In
   particular, we design two privacy preserving perception hashing methods:
   the first one is based on the SIFT features which focus on the
   description of sensitive objects in the image and the second one is
   based on the LBP features which focus on the description of faces in the
   image. Both methods use the secret key to protect hashes from
   adversaries or untrusted servers. The recommendation of a privacy level
   for a given image is based on two types of privacy rules, user-defined
   specific rules based on the logic combination of the semantic tags of
   image objects retrieved from the perceptual hash database, and general
   rules based on machine learning. The privacy level, as the measurement
   of the degree of sensitivity of a digital image, can be directly fed to
   the user before he shares the image to OSNs. It can also be fed to the
   user's access control component. Experimental results show the
   effectiveness and usability of the proposed framework.}},
DOI = {{10.1109/DSC.2016.100}},
ISBN = {{978-1-5090-1192-6}},
Unique-ID = {{ISI:000405708600033}},
}

@inproceedings{ ISI:000401929800004,
Author = {Hu, Shengshan and Wang, Qian and Wang, Jingjun and Chow, Sherman S. M.
   and Zou, Qin},
Book-Group-Author = {{IEEE}},
Title = {{Securing Fast Learning! Ridge Regression over Encrypted Big Data}},
Booktitle = {{2016 IEEE TRUSTCOM/BIGDATASE/ISPA}},
Series = {{IEEE Trustcom BigDataSE ISPA}},
Year = {{2016}},
Pages = {{19-26}},
Note = {{15th IEEE Int Conf on Trust, Security and Privacy in Comp and Commun /
   10th IEEE Int Conf on Big Data Science and Engineering / 14th IEEE Int
   Symposium on Parallel and Distributed Proc with Applicat (IEEE
   Trustcom/BigDataSE/ISPA), Tianjin, PEOPLES R CHINA, AUG 23-26, 2016}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Tech Comm Scalable Comp}},
Abstract = {{Ridge regression is an important algorithm in machine learning and has
   been widely used in real- world applications like recommendation
   systems. Trained with a large training dataset, it outputs a curve that
   models the relationship between a scalar dependent variable and one or
   more explanatory variables. In general, the more training dataset it is
   fed, the more accurate the resulting model will be. In this era of ``Big
   Data{''}, however, data is usually split among different users. Exposing
   data to the other parties arouse privacy violation. It is difficult to
   apply conventional algorithms when the contributing users care about
   their privacy.
   In this work, we propose a new efficient privacy-preserving ridge
   regression scheme. We first design a packed secure multiplication
   protocol by utilizing Paillier encryption that is applicable to real
   numbers. Then we transform the ridge regression problem into the problem
   of solving linear equations, so we can employ Gaussian elimination and
   Jacobi iterative method to efficiently derive the learned model.
   Finally, extensive experiments on real-world datasets are conducted to
   show that our scheme outperforms the state-of-the-art solutions in terms
   of both computation and communication efficiencies, and incurs
   negligible errors compared with performing ridge regression in the
   clear.}},
DOI = {{10.1109/TrustCom.2016.40}},
ISSN = {{2324-9013}},
ISBN = {{978-1-5090-3205-1}},
ORCID-Numbers = {{Chow, Sherman S. M./0000-0001-7306-453X}},
Unique-ID = {{ISI:000401929800004}},
}

@inproceedings{ ISI:000401929800115,
Author = {Guo, Longkun and Shen, Hong},
Book-Group-Author = {{IEEE}},
Title = {{Privacy-Preserving Internet Traffic Publication}},
Booktitle = {{2016 IEEE TRUSTCOM/BIGDATASE/ISPA}},
Series = {{IEEE Trustcom BigDataSE ISPA}},
Year = {{2016}},
Pages = {{884-891}},
Note = {{15th IEEE Int Conf on Trust, Security and Privacy in Comp and Commun /
   10th IEEE Int Conf on Big Data Science and Engineering / 14th IEEE Int
   Symposium on Parallel and Distributed Proc with Applicat (IEEE
   Trustcom/BigDataSE/ISPA), Tianjin, PEOPLES R CHINA, AUG 23-26, 2016}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Tech Comm Scalable Comp}},
Abstract = {{As machine learning (ML)-based traffic classification develops, Internet
   traffic data is published in public to serve as test data. Although the
   IP addresses therein are anonymized, it is given explicitly which data
   belongs to an identical user. Then using the information, an adversary
   can identify a user from the anonymized users. The paper first gives a
   k-anonymity method to reduce the probability of information leak to P/k,
   where P is the probability of information leak without k-anonymity.
   Assume the number of the flows belonging to an IP address follows Normal
   distribution, the information loss is shown mu(2) + sigma(2) / k mu(2) +
   sigma(2), where mu and sigma are respectively the mean and the variance
   of the Normal distribution. Later, random noise is added to further
   reduce the probability of information leak to P/k(2), with an expected
   distortion rate of approximately 2(d+log k-log vertical bar X vertical
   bar), where d is the number of dimensions and vertical bar X vertical
   bar is the number of the vectors. At last, real-world Internet traffic
   data is used to evaluate the utility of the anonymized traffic data.
   According to the experimental results, the k-anonymized noised data can
   be clustered with an overall accuracy rate close to the state-of-the-art
   results for non-anonymized traffic data.}},
DOI = {{10.1109/TrustCom.2016.151}},
ISSN = {{2324-9013}},
ISBN = {{978-1-5090-3205-1}},
ResearcherID-Numbers = {{Guo, Longkun/R-8634-2019}},
Unique-ID = {{ISI:000401929800115}},
}

@inproceedings{ ISI:000402318100043,
Author = {Syed, Toqeer Ali and Jan, Salman and Musa, Shahrulniza and Ali, Jawad},
Book-Group-Author = {{IEEE}},
Title = {{Providing Efficient, Scalable and Privacy Preserved Verification
   Mechanism in Remote Attestation}},
Booktitle = {{2016 PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INFORMATION AND
   COMMUNICATION TECHNOLOGY (ICICTM)}},
Year = {{2016}},
Pages = {{236-245}},
Note = {{1st International Conference on Information and Communication Technology
   (ICICTM), Kuala Lumpur, MALAYSIA, MAY 16-17, 2016}},
Organization = {{IEEE; CompTIA; Univ Kuala Lumpur; Univ Pertahanan Nas Malaysia}},
Abstract = {{Numerous applications are running in a distributed environment in
   today's large networked world. Corporations really need a mechanism to
   monitor their own application(s) running on remote devices. One such
   mechanism by Trusted Computing Group (TCG) called remote attestation
   that can monitor and verify trustworthiness of remote applications. In
   this regard, many solutions have been provided on how to monitor remote
   applications. However, It becomes quite challenging task, when
   applications are running on millions of devices and it becomes necessary
   for the corporates to verify all of the applications. In this paper we
   have provided an efficient, scalable and privacy preserved mechanism to
   tackle the scalability of all these kinds of verifications. Machine
   learning algorithms are incorporated as Hadoop/MapReduce functions on
   the public cloud. The rest of low CPU intensive and privacy preserved
   verifications are performed on the private cloud.}},
ISBN = {{978-1-5090-0412-6}},
Unique-ID = {{ISI:000402318100043}},
}

@inproceedings{ ISI:000401700900191,
Author = {Sei, Yuichi and Okumura, Hiroshi and Ohsuga, Akihiko},
Editor = {{Chen, J and Yang, LT}},
Title = {{Privacy-Preserving Publication of Deep Neural Networks}},
Booktitle = {{PROCEEDINGS OF 2016 IEEE 18TH INTERNATIONAL CONFERENCE ON HIGH
   PERFORMANCE COMPUTING AND COMMUNICATIONS; IEEE 14TH INTERNATIONAL
   CONFERENCE ON SMART CITY; IEEE 2ND INTERNATIONAL CONFERENCE ON DATA
   SCIENCE AND SYSTEMS (HPCC/SMARTCITY/DSS)}},
Year = {{2016}},
Pages = {{1418-1425}},
Note = {{18th IEEE International Conference on High Performance Computing and
   Communications (HPCC) / 14th IEEE International Conference on Smart City
   (Smart City) / 2nd IEEE International Conference on Data Science and
   Systems (DSS), Sydney, AUSTRALIA, DEC 12-14, 2016}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Tech Comm Scalable Comp; Univ Technol Sydney}},
Abstract = {{An organization that has a lot of personal data can create a deep neural
   network (DNN), which predicts sensitive attribute values such as the
   salary and diseases of people based on other attribute values such as
   age and hobbies. Moreover, by putting this data on the Cloud and
   providing the functionality of the DNN to other organizations, they can
   obtain new knowledge and can subsequently create new services. However,
   because such DNNs are generated from sensitive attribute values, we
   cannot share them freely without the explicit consent of the persons
   whose data are used for the DNNs. On the other hand, in recent years,
   epsilon-differential privacy has emerged as the de facto privacy metric.
   Many researchers use epsilon-differential privacy for privacy-preserving
   data mining such as correlation analysis and association rule analysis.
   In this paper, we modify epsilon-differential privacy for machine
   learning, and we propose three approaches for creating privacy-preserved
   DNNs based on the modified epsilon-differential privacy. Our proposed
   approaches are experimentally evaluated using a real data set, and we
   show that our approaches can protect personal attribute values while
   maintaining the accuracy of the DNNs.}},
DOI = {{10.1109/HPCC-SmartCity-DSS.2016.162}},
ISBN = {{978-1-5090-4297-5}},
Unique-ID = {{ISI:000401700900191}},
}

@article{ ISI:000397926700002,
Author = {Kaur, Kulwinder and Zandu, Vikas},
Title = {{A Secure Data Classification Model in Cloud Computing Using Machine
   Learning Approach}},
Journal = {{INTERNATIONAL JOURNAL OF GRID AND DISTRIBUTED COMPUTING}},
Year = {{2016}},
Volume = {{9}},
Number = {{8}},
Pages = {{13-21}},
Abstract = {{Cloud computing offers numerous benefits including scalability,
   availability and many services. But with its wide acceptance all over
   the globe, new risks and vulnerabilities have appeared too. Cloud
   computing provides facility of storing and accessing information and
   programs over the web without bothering the storage space on system.
   Storing the data on cloud eliminates ones worries about space
   considerations, buying new storage equipment or managing their data,
   rather they are able to access their data any time from any place
   provided they have internet access. But the rising security problems
   have resisted the organizations from connecting with cloud computing
   completely. Hence security risks have appeared as the main disadvantage
   of cloud computing. This paper involves the efforts to analyze the
   security issues and then proposes a framework to address these security
   issues at the authentication and storage level in cloud computing. While
   addressing the security issues the first and the foremost thing is to
   classify what data needs security and what data needn't bother with
   security and hence data gets classified into two classes sensitive and
   non-sensitive. To achieve data classification, a data classification
   approach based on the confidentiality of data is proposed in this paper.
   Following that an efficient security mechanism has to be deployed by
   means of encryption, authentication, and authorization or by some other
   method to ensure the privacy of consumers data on cloud storage.}},
DOI = {{10.14257/ijgdc.2016.9.8.02}},
ISSN = {{2005-4262}},
EISSN = {{2207-6379}},
Unique-ID = {{ISI:000397926700002}},
}

@inproceedings{ ISI:000392177200063,
Author = {Chanyaswad, Thee and Chang, J. Morris and Mittal, Prateek and Kung, S.
   Y.},
Book-Group-Author = {{IEEE}},
Title = {{DISCRIMINANT-COMPONENT EIGENFACES FOR PRIVACY-PRESERVING FACE
   RECOGNITION}},
Booktitle = {{2016 IEEE 26TH INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL
   PROCESSING (MLSP)}},
Series = {{IEEE International Workshop on Machine Learning for Signal Processing}},
Year = {{2016}},
Note = {{26th IEEE International Workshop on Machine Learning for Signal
   Processing (MLSP), Salerno, ITALY, SEP 13-16, 2016}},
Organization = {{IEEE; IEEE Signal Proc Soc, Machine Learning Signal Proc Tech Comm}},
Abstract = {{Over the past decades, face recognition has been a problem of critical
   interest in the machine learning and signal processing communities.
   However, conventional approaches such as eigenfaces do not protect the
   privacy of user data, which is emerging as an important design
   consideration in today's society. In this work, we leverage a
   supervised-learning subspace projection method called Discriminant
   Component Analysis (DCA) for privacy-preserving face recognition. By
   projecting the data onto the lower-dimensional signal subspace
   prescribed by DCA, high performance of face recognition is achievable
   without compromising privacy of the data owners. We evaluate our
   approach on three image datasets: Yale, Olivetti and Glasses datasets -
   the last is derived from the former two. Our approach can serve as a key
   enabler for real-world deployment of privacy-preserving face recognition
   applications, and provides a promising direction to researchers and
   private sectors.}},
ISSN = {{2161-0363}},
ISBN = {{978-1-5090-0746-2}},
Unique-ID = {{ISI:000392177200063}},
}

@inproceedings{ ISI:000391250900056,
Author = {Catak, Ferhat Ozgur and Mustacoglu, Ahmet Fatih and Topcu, Ahmet Ercan},
Book-Group-Author = {{IEEE}},
Title = {{Privacy Preserving Extreme Learning Machine Classification Model for
   Distributed Systems}},
Booktitle = {{2016 24TH SIGNAL PROCESSING AND COMMUNICATION APPLICATION CONFERENCE
   (SIU)}},
Year = {{2016}},
Pages = {{313-316}},
Note = {{24th Signal Processing and Communication Application Conference (SIU),
   Zonguldak, TURKEY, MAY 16-19, 2016}},
Organization = {{IEEE; Bulent Ecevit Univ, Dept Elect \& Elect Engn; Bulent Ecevit Univ,
   Dept Biomed Engn; Bulent Ecevit Univ, Dept Comp Engn}},
Abstract = {{Machine learning based classification methods are widely used to analyze
   large scale datasets in this age of big data. Extreme learning machine
   (ELM) classification algorithm is a relatively new method based on
   generalized single-layer feed-forward network structure. Traditional ELM
   learning algorithm implicitly assumes complete access to whole data set.
   This is a major privacy concern in most of cases. Sharing of private
   data (i.e. medical records) is prevented because of security concerns.
   In this research, we proposed an efficient and secure privacy-preserving
   learning algorithm for ELM classification over data that is vertically
   partitioned among several parties. The new learning method preserves the
   privacy on numerical attributes, builds a classification model without
   sharing private data without disclosing the data of each party to
   others.}},
ISBN = {{978-1-5090-1679-2}},
ResearcherID-Numbers = {{Catak, Ferhat Ozgur/C-1810-2016}},
ORCID-Numbers = {{Catak, Ferhat Ozgur/0000-0002-2434-9966}},
Unique-ID = {{ISI:000391250900056}},
}

@inproceedings{ ISI:000391925900008,
Author = {Nguyen Dong Phuong and Vo Thi Ngoc Chau},
Editor = {{Cao, T and Ho, YS}},
Title = {{Automatic De-Identification of Medical Records with a Multilevel Hybrid
   Semi-Supervised Learning Approach}},
Booktitle = {{2016 IEEE RIVF INTERNATIONAL CONFERENCE ON COMPUTING \& COMMUNICATION
   TECHNOLOGIES, RESEARCH, INNOVATION, AND VISION FOR THE FUTURE (RIVF)}},
Year = {{2016}},
Pages = {{43-48}},
Note = {{IEEE RIVF International Conference on Computing \& Communication
   Technologies, Research, Innovation, and Vision for the Future (RIVF),
   Thuyloi Univ, Hanoi, VIETNAM, NOV 07-09, 2016}},
Organization = {{IEEE; IEEE Vietnam Sect; IEEE Commun Soc; IEEE Computat Intelligence Soc}},
Abstract = {{In recent years, sharing electronic medical records (EMRs) for more
   researchers outside the associated institutions is significant. For
   privacy preservation of the corresponding patients and the associated
   institutions, a de-identification task on the EMRs to be shared is a
   must. Although the de-identification task has been considered with
   positive research outcomes worldwide, especially those from the i2b2
   (Informatics for Integrating Biology and the Bedside) shared tasks in
   2006 and 2014, the task has not yet been a solved problem and still
   needs more investigation realistically. In this paper, we propose an
   automatic de-identification solution in a multilevel hybrid
   semi-supervised learning paradigm with a key focus on correctly
   identifying protected health information (PHI) in the EMRs. Similar to
   the existing works, our work defines a hybrid approach by combining a
   machine learning-based method with a conditional random fields model and
   a rule-based method in a post-processing phase to handle the PHI types
   with disambiguity. Nevertheless, our work is more general and practical.
   First, it considers the structure complexity of each EMR so that each
   section can be treated properly for more correct PHI identification up
   to its structure complexity: structured, semi-structured, or
   un-structured. Second, each EMR is then examined in our approach at
   three different levels of granularity such as a token level in the
   supervised learning phase, an entity level in the rule-based
   post-processing phase, and a section level along with the structure
   complexity in the semi-supervised learning phase. Many various detail
   levels will give our approach a deeper look at each EMR for more
   effectiveness. Third, our solution is conducted in a self-training
   manner so that it can get started with a small annotated data set in
   practice and get more effective with new EMRs over time. Evaluated with
   the i2b2 data set in comparison with the related works, our solution is
   effective with better F-measure values for the AGE, LOCATION, and PHONE
   PHI types and comparable for the other PHI types.}},
ISBN = {{978-1-5090-4134-3}},
Unique-ID = {{ISI:000391925900008}},
}

@article{ ISI:000391824600001,
Author = {Wang, Yu-Xiang and Lei, Jing and Fienberg, Stephen E.},
Title = {{Learning with Differential Privacy: Stability, Learnability and the
   Sufficiency and Necessity of ERM Principle}},
Journal = {{JOURNAL OF MACHINE LEARNING RESEARCH}},
Year = {{2016}},
Volume = {{17}},
Abstract = {{While machine learning has proven to be a powerful data-driven solution
   to many real life problems, its use in sensitive domains has been
   limited due to privacy concerns. A popular approach known as
   differential privacy offers provable privacy guarantees, but it is often
   observed in practice that it could substantially hamper learning
   accuracy. In this paper we study the learnability (whether a problem can
   be learned by any algorithm) under Vapnik's general learning setting
   with differential privacy constraint, and reveal some intricate
   relationships between privacy, stability and learnability. In
   particular, we show that a problem is privately learnable if an only if
   there is a private algorithm that asymptotically minimizes the empirical
   risk (AERM). In contrast, for non-private learning AERM alone is not
   sufficient for learnability. This result suggests that when searching
   for private learning algorithms, we can restrict the search to
   algorithms that are AERM. In light of this, we propose a conceptual
   procedure that always finds a universally consistent algorithm whenever
   the problem is learnable under privacy constraint. We also propose a
   generic and practical algorithm and show that under very general
   conditions it privately learns a wide class of learning problems.
   Lastly, we extend some of the results to the more practical
   (epsilon,delta)-differential privacy and establish the existence of a
   phase-transition on the class of problems that are approximately
   privately learnable with respect to how small delta needs to be.}},
Article-Number = {{183}},
ISSN = {{1532-4435}},
Unique-ID = {{ISI:000391824600001}},
}

@article{ ISI:000391472600001,
Author = {Kairouz, Peter and Oh, Sewoong and Viswanath, Pramod},
Title = {{Extremal Mechanisms for Local Differential Privacy}},
Journal = {{JOURNAL OF MACHINE LEARNING RESEARCH}},
Year = {{2016}},
Volume = {{17}},
Abstract = {{Local differential privacy has recently surfaced as a strong measure of
   privacy in contexts where personal information remains private even from
   data analysts. Working in a setting where both the data providers and
   data analysts want to maximize the utility of statistical analyses
   performed on the released data, we study the fundamental trade-off
   between local differential privacy and utility. This trade-off is
   formulated as a constrained optimization problem: maximize utility
   subject to local differential privacy constraints. We introduce a
   combinatorial family of extremal privatization mechanisms, which we call
   staircase mechanisms, and show that it contains the optimal
   privatization mechanisms for a broad class of information theoretic
   utilities such as mutual information and f-divergences. We further prove
   that for any utility function and any privacy level, solving the
   privacy-utility maximization problem is equivalent to solving a
   finite-dimensional linear program, the outcome of which is the optimal
   staircase mechanism. However, solving this linear program can be
   computationally expensive since it has a number of variables that is
   exponential in the size of the alphabet the data lives in. To account
   for this, we show that two simple privatization mechanisms, the binary
   and randomized response mechanisms, are universally optimal in the low
   and high privacy regimes, and well approximate the intermediate regime.}},
Article-Number = {{17}},
ISSN = {{1532-4435}},
Unique-ID = {{ISI:000391472600001}},
}

@inproceedings{ ISI:000391051600012,
Author = {Zhang, Tao and Zhu, Quanyan},
Book-Group-Author = {{ACM}},
Title = {{A Dual Perturbation Approach for Differential Private ADMM-Based
   Distributed Empirical Risk Minimization}},
Booktitle = {{AISEC'16: PROCEEDINGS OF THE 2016 ACM WORKSHOP ON ARTIFICIAL
   INTELLIGENCE AND SECURITY}},
Year = {{2016}},
Pages = {{129-137}},
Note = {{9th ACM Workshop on Artificial Intelligence and Security (AISec),
   Vienna, AUSTRIA, OCT 28, 2016}},
Organization = {{Assoc Comp Machinery; ACM SIGSAC}},
Abstract = {{The rapid growth of data has raised the importance of privacy-preserving
   techniques in distributed machine learning. In this paper, we develop a
   privacy-preserving method to a class of regularized empirical risk
   minimization (ERM) machine learning problems. We first decentralize the
   learning algorithm using the alternating direction method of multipliers
   (ADMM), and propose the method of dual variable perturbation to provide
   dynamic differential privacy. The mechanism leads to a
   privacy-preserving algorithm under mild conditions of the convexity and
   differentiability of the loss function and the regularizer. We study the
   performance of the algorithm measured by the number of data points
   required to achieve a bounded error. To design an optimal privacy
   mechanism, we analyze the fundamental tradeoff between privacy and
   accuracy, and provide guidelines to choose privacy parameters. Numerical
   experiments using the real world database are performed to corroborate
   the results on the privacy and utility tradeoffs and design.}},
DOI = {{10.1145/2996758.2996762}},
ISBN = {{978-1-4503-4573-6}},
Unique-ID = {{ISI:000391051600012}},
}

@inproceedings{ ISI:000390760100233,
Author = {Wang, Sisi and Poon, Wing-Sea and Farnadi, Golnoosh and Horst, Caleb and
   Thompson, Kebra and Nickels, Michael and Nascimento, Anderson and De
   Cock, Martine},
Editor = {{Kumar, R and Caverlee, J and Tong, H}},
Title = {{VirtualIdentity: Privacy Preserving User Profiling}},
Booktitle = {{PROCEEDINGS OF THE 2016 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN
   SOCIAL NETWORKS ANALYSIS AND MINING ASONAM 2016}},
Year = {{2016}},
Pages = {{1434-1437}},
Note = {{8th IEEE/ACM International Conference on Advances in Social Networks
   Analysis and Mining (ASONAM), San Francisco, CA, AUG 18-21, 2016}},
Organization = {{IEEE; Assoc Comp Machinery; ACM SIGMOD; IEEE Comp Soc; IEEE TCDE;
   Springer; VEEPIO}},
Abstract = {{User profiling from user generated content (UGC) is a common practice
   that supports the business models of many social media companies.
   Existing systems require that the UGC is fully exposed to the module
   that constructs the user profiles. In this paper we show that it is
   possible to build user profiles without ever accessing the user's
   original data, and without exposing the trained machine learning models
   for user profiling - which are the intellectual property of the company
   - to the users of the social media site. We present VirtualIdentity, an
   application that uses secure multi-party cryptographic protocols to
   detect the age, gender and personality traits of users by classifying
   their user-generated text and personal pictures with trained support
   vector machine models in a privacy preserving manner.}},
ISBN = {{978-1-5090-2846-7}},
ResearcherID-Numbers = {{Nascimento, Anderson/A-2107-2011}},
Unique-ID = {{ISI:000390760100233}},
}

@inproceedings{ ISI:000387820900022,
Author = {Liu, Yushan and Ji, Shouling and Mittal, Prateek},
Book-Group-Author = {{ACM}},
Title = {{SmartWalk: Enhancing Social Network Security via Adaptive Random Walks}},
Booktitle = {{CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND
   COMMUNICATIONS SECURITY}},
Year = {{2016}},
Pages = {{492-503}},
Note = {{23rd ACM Conference on Computer and Communications Security (CCS),
   Vienna, AUSTRIA, OCT 24-28, 2016}},
Organization = {{Assoc Comp Machinery; ACM Special Interest Grp Secur Audit \& Control}},
Abstract = {{Random walks form a critical foundation in many social network based
   security systems and applications. Currently, the design of such social
   security mechanisms is limited to the classical paradigm of using
   fixed-length random walks for all nodes on a social graph. However, the
   fixed-length walk paradigm induces a poor trade-off between security and
   other desirable properties.
   In this paper, we propose SmartWalk, a security enhancing system which
   incorporates adaptive random walks in social network security
   applications. We utilize a set of supervised machine learning techniques
   to predict the necessary random walk length based on the structural
   characteristics of a social graph. Using experiments on multiple real
   world topologies, we show that the desired walk length starting from a
   specific node can be well predicted given the local features of the
   node, and limited knowledge for a small set of training nodes. We
   describe node-adaptive and pathadaptive random walk usage models, where
   the walk length adaptively changes based on the starting node and the
   intermediate nodes on the path, respectively. We experimentally
   demonstrate the applicability of adaptive random walks on a number of
   social network based security and privacy systems, including Sybil
   defenses, anonymous communication and link privacy preserving systems,
   and show up to two orders of magnitude improvement in performance.}},
DOI = {{10.1145/2976749.2978319}},
ISBN = {{978-1-4503-4139-4}},
Unique-ID = {{ISI:000387820900022}},
}

@inproceedings{ ISI:000383224500070,
Author = {Jia, Qi and Guo, Linke and Jin, Zhanpeng and Fang, Yuguang},
Book-Group-Author = {{IEEE}},
Title = {{Privacy-preserving Data Classification and Similarity Evaluation for
   Distributed Systems}},
Booktitle = {{PROCEEDINGS 2016 IEEE 36TH INTERNATIONAL CONFERENCE ON DISTRIBUTED
   COMPUTING SYSTEMS ICDCS 2016}},
Series = {{IEEE International Conference on Distributed Computing Systems}},
Year = {{2016}},
Pages = {{690-699}},
Note = {{36th IEEE International Conference on Distributed Computing Systems
   (ICDCS), Nara, JAPAN, JUN 27-30, 2016}},
Organization = {{IEEE; IEEE Comp Soc; Natl Inst informat \& Commun Technol; IEICE Commun
   Soc; IPS}},
Abstract = {{Data classification is a widely used data mining technique for big data
   analysis. By training massive data collected from the real world, data
   classification helps learners discover hidden data patterns. In addition
   to data training, given a trained model from collected data, a user can
   classify whether a new incoming data belongs to an existing class; or,
   multiple distributed entities may collaborate to test the similarity of
   their trained results. However, due to data locality and privacy
   concerns, it is infeasible for large-scale distributed systems to share
   each individual's datasets with each other for data similarity check. On
   the one hand, the trained model is an entity's private asset and may
   leak private information, which should be well protected from all other
   non-collaborative entities. On the other hand, the new incoming data may
   contain sensitive information which cannot be disclosed directly for
   classification. To address the above privacy issues, we propose a
   privacy-preserving data classification and similarity evaluation scheme
   for distributed systems. With our scheme, neither new arriving data nor
   trained models are directly revealed during the classification and
   similarity evaluation procedures. The proposed scheme can be applied to
   many fields using data classification and evaluation. Based on extensive
   real-world experiments, we have also evaluated the privacy preservation,
   feasibility, and efficiency of the proposed scheme.}},
DOI = {{10.1109/ICDCS.2016.94}},
ISSN = {{1063-6927}},
ISBN = {{978-1-5090-1482-8}},
ResearcherID-Numbers = {{Fang, Yuguang/A-7484-2009}},
ORCID-Numbers = {{Fang, Yuguang/0000-0002-1079-3871}},
Unique-ID = {{ISI:000383224500070}},
}

@article{ ISI:000376639500010,
Author = {Zaghian, Ali and Bagheri, Ayoub},
Title = {{A Combined Model of Clustering and Classification Methods for Preserving
   Privacy in Social Networks against Inference and Neighborhood Attacks}},
Journal = {{INTERNATIONAL JOURNAL OF SECURITY AND ITS APPLICATIONS}},
Year = {{2016}},
Volume = {{10}},
Number = {{1}},
Pages = {{95-102}},
Month = {{JAN}},
Abstract = {{In the last decade online social networks has gained remarkable
   attention. Facebook or Google+, are example social network services
   which allow people to create online profiles and share personal
   information with their friends. These networks publish details about
   users while some of the information revealed inside is private. In order
   to address privacy concerns, many social networks allow users to hide
   their private or sensitive information in their profiles from the
   public. In this paper, we focus on the problem of information revelation
   in online social networks by preserving the privacy of sensitive
   information in their data using machine learning and data mining
   algorithms. We show how an adversary can launch an inference or
   neighborhood attack to exploit an online social network using released
   data and structure of the network to predict the private information and
   attributes of users. For this purpose, we propose a new data mining
   based model that uses neighborhood information and attributes details of
   a user to infer private attributes of user profiles. The proposed model
   consists of two main parts: a clustering approach to ensure the
   k-anonymity and a classification algorithm to preserve the privacy
   against inference attacks. Finally we explore the effectiveness of some
   sanitization techniques that can be used to combat such inference
   attacks, and we show experimentally the success of different
   neighborhood re-identification strategies. Our experimental results
   reveal that using combination of data mining algorithm can notably help
   to preserve private and sensitive information in social network data.}},
DOI = {{10.14257/ijsia.2016.10.1.10}},
ISSN = {{1738-9976}},
EISSN = {{2207-9629}},
ResearcherID-Numbers = {{Bagheri, Ayoub/H-2079-2018}},
ORCID-Numbers = {{Bagheri, Ayoub/0000-0001-6366-2173}},
Unique-ID = {{ISI:000376639500010}},
}

@inproceedings{ ISI:000428746400042,
Author = {Omer, Mohammed Z. and Gao, Hui and Mustafa, Nadir},
Editor = {{Li, JP and Bloshanskii, I and Ahmad, I and Yang, SX}},
Title = {{PRIVACY PRESERVING IN DISTRIBUTED SVM DATA MINING OVER HORIZONTALLY
   PARTITIONED DATA}},
Booktitle = {{2016 13TH INTERNATIONAL COMPUTER CONFERENCE ON WAVELET ACTIVE MEDIA
   TECHNOLOGY AND INFORMATION PROCESSING (ICCWAMTIP)}},
Series = {{International Computer Conference on Wavelet Active Media Technology and
   Information Processing}},
Year = {{2016}},
Pages = {{189-194}},
Note = {{13th IEEE International Computer Conference on Wavelet Active Media
   Technology and Information Processing (ICCWAMTIP), Univ Elect Sci \&
   Technol China, Chengdu, PEOPLES R CHINA, DEC 16-18, 2016}},
Organization = {{Natl Nat Sci Fdn China; Natl High Technol Res \& Dev Program China;
   China Int Talent Exchange Fdn; IEEE; IEEE Chengdu Sect}},
Abstract = {{Data Mining algorithms can tackle the data either centrally or
   distributed. Outsourcing data can solve the issues of processing,
   storing, and analyzing a massive data. A proportion of existing data in
   various places and to improve the classification results, we propose the
   following solution for data mining with preserving the privacy. However,
   a critical problem that precludes free sharing of information is
   confidentiality and security issues. One of the significant tasks of
   data mining and machine learning is classification new instances, the
   SVM algorithms a widely used in classification, which applicable in many
   various areas. We propose a privacy-preserving solution for SVM
   classification over horizontally partitioned data. Our solution
   constructs a global SVM classification model from global Gram Matrix,
   without revealing sensitive data. Our experimental results demonstrate
   the accuracy of distributed SVM using Gram matrix up to 97\% with
   preserving the privacy.}},
ISSN = {{2576-8972}},
EISSN = {{2576-8964}},
ISBN = {{978-1-5090-6126-6}},
Unique-ID = {{ISI:000428746400042}},
}

@inproceedings{ ISI:000381741700066,
Author = {Yang, Lin and Wang, Wei and Zhang, Qian},
Book-Group-Author = {{IEEE}},
Title = {{VibID: User Identification through Bio-Vibrometry}},
Booktitle = {{2016 15TH ACM/IEEE INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN
   SENSOR NETWORKS (IPSN)}},
Year = {{2016}},
Note = {{15th ACM/IEEE International Conference on Information Processing in
   Sensor Networks (IPSN), Vienna, AUSTRIA, APR 11-14, 2016}},
Organization = {{ACM; IEEE}},
Abstract = {{User identification is an essential problem for security protection and
   data privacy preservation of wearable devices. With proper user
   identification, wearable devices can adopt personalized settings for
   different users, automatically label the corresponding data to protect
   user privacy, and help prevent illegal user spoofing attacks. Current
   user identification solutions proposed for wearable devices either rely
   on dedicated devices with high cost or require user intervention which
   is not convenient. In this work, we leverage the bio-vibrometry to
   enable a novel user identification solution for wearable devices in
   small-scale scenarios, e.g., household scenario. Unlike existing user
   identification solutions, our system only uses the low-cost sensors that
   are already available for most wearable devices. The key idea is that,
   when human body is exposed to a vibration excitation, the vibration
   response reflects the physical characteristics of user, i.e., the mass,
   stiffness and damping. Meanwhile, due to users' biological diversity,
   such physical characteristics of different users are quite distinctive.
   Therefore, we can leverage the discrepancy in users' vibration responses
   as an identifier. Based on this idea, we propose VibID, which only uses
   a low-cost vibration motor and accelerometer to generate an unobtrusive
   vibration to users' arms and capture the corresponding responses. By
   examining the vibration patterns at different frequencies, VibID builds
   an ensemble machine learning model to recognize who is using the device.
   Extensive experiments are conducted on human subjects to demonstrate
   that our system is reliable in small-scale scenarios and robust to
   various confounding factors, e.g., arm position, muscle state, user
   mobility and wearing location. We also show that, in an uncontrolled
   scenario of 8 users, our system can still ensure a identification
   accuracy above 91\%.}},
ISBN = {{978-1-5090-0802-5}},
ResearcherID-Numbers = {{Zhang, Qian/B-9058-2009}},
ORCID-Numbers = {{Zhang, Qian/0000-0001-9205-1881}},
Unique-ID = {{ISI:000381741700066}},
}

@inproceedings{ ISI:000432161300010,
Author = {Pervushin, Alexey and Ermachkova, Viktoria and Spivak, Anton},
Book-Group-Author = {{IEEE}},
Title = {{Determination of Loss of Information During Data Anonymization Procedure}},
Booktitle = {{2016 IEEE 10TH INTERNATIONAL CONFERENCE ON APPLICATION OF INFORMATION
   AND COMMUNICATION TECHNOLOGIES (AICT)}},
Series = {{International Conference on Application of Information and Communication
   Technologies}},
Year = {{2016}},
Pages = {{42-46}},
Note = {{10th IEEE International Conference on Application of Information and
   Communication Technologies (AICT), Baku, AZERBAIJAN, OCT 12-14, 2016}},
Organization = {{Nar; Dell EMC; IEEE; IEEE Reg 8; Minist Commun \& High Technologies;
   Minist Educ Azerbaijan; Qafqaz Univ; Baku State Univ; Baku Higher Oil
   Sch; Lomonosov Moscow State Univ, Baku branch; Azerbaijan Tech Univ;
   Univ Malaysia Sabah; ANAS, Inst Informat Technol; ANAS, Inst Control
   Syst; UNESCO Inst Informat Technol Educ}},
Abstract = {{One of the effective approaches to the protection of personal data is
   their depersonalization, as it reduces the requirements for the level of
   data protection. Therefore anonymization procedure is widely used in
   practice. As a result of the application of such methods discards some
   of the information, which entails a loss of information content of the
   personal data. The problem of determining the loss of information during
   data anonymization procedure. It is often easier to allocate a group of
   similar objects, to analyze the loss for each group, than analyze all
   the data at once. A large amount of data and the lack of pre-known
   cluster, prompted the decision of one of the tasks of Data Mining - Data
   Clustering. Realize the data by identifying the cluster structure allows
   us to understand how the data looked at the beginning, and how have they
   changed after anonymization procedure. Analyzing the structure of the
   cluster can be seen on the data identity.}},
ISSN = {{2378-8232}},
EISSN = {{2472-8586}},
ISBN = {{978-1-5090-1841-3}},
Unique-ID = {{ISI:000432161300010}},
}

@inproceedings{ ISI:000385263000037,
Author = {Ohrimenko, Olga and Schuster, Felix and Fournet, Cedric and Mehta,
   Aastha and Nowozin, Sebastian and Vaswani, Kapil and Costa, Manuel},
Book-Group-Author = {{USENIX Assoc}},
Title = {{Oblivious Multi-Party Machine Learning on Trusted Processors}},
Booktitle = {{PROCEEDINGS OF THE 25TH USENIX SECURITY SYMPOSIUM}},
Year = {{2016}},
Pages = {{619-636}},
Note = {{25th USENIX Security Symposium, Austin, TX, AUG 10-12, 2016}},
Organization = {{USENIX; Facebook; NSF; Cisco; Google; Microsoft; Neustar; IBM Res;
   Symantec; ACM Queue; ADMIN; CRC Press; Linux Pro Magazine; NetApp;
   VMWare; LXer; UserFriendly Org; OReilly Media; No Starch Press; Virus
   Bulletin}},
Abstract = {{Privacy-preserving multi-party machine learning allows multiple
   organizations to perform collaborative data analytics while guaranteeing
   the privacy of their individual datasets. Using trusted SGX-processors
   for this task yields high performance, but requires a careful selection,
   adaptation, and implementation of machine-learning algorithms to
   provably prevent the exploitation of any side channels induced by
   data-dependent access patterns.
   We propose data-oblivious machine learning algorithms for support vector
   machines, matrix factorization, neural networks, decision trees, and
   k-means clustering. We show that our efficient implementation based on
   Intel Skylake processors scales up to large, realistic datasets, with
   overheads several orders of magnitude lower than with previous
   approaches based on advanced cryptographic multi-party computation
   schemes.}},
ISBN = {{978-1-931971-32-4}},
Unique-ID = {{ISI:000385263000037}},
}

@article{ ISI:000437872500001,
Author = {Cotha, Naveen Kumar Parachur and Sokolova, Marina},
Title = {{Multi-label learning in classification of patients' quasi-identifiers}},
Journal = {{PROGRESS IN ARTIFICIAL INTELLIGENCE}},
Year = {{2015}},
Volume = {{4}},
Number = {{3-4}},
Pages = {{37-48}},
Month = {{DEC}},
Abstract = {{Our study considers identification of demographic attributes of patients
   as a multi-label learning problem. This is a novel approach to predict
   accuracy of classification of patients' quasi-identifiers (race and
   gender attributes). To classify the sets of attributes, we applied
   ensembles of several multi-label learning algorithms. The
   best-performing multi-label ensembles include decision tree algorithms.
   In the empirical part of this study, we used on the UCI diabetics
   dataset of over 100,000 records, collected from 130 US hospitals. The
   dataset consisted of attributes that included patient demographics
   (race, gender, age), diagnoses code, lab results, etc. Experiments
   conducted on datasets of 1000, 10,000, 20,000 examples show that the
   best classifier achieves a high overall accuracy of 0.533 (1000
   examples), 0.702 (10,000 examples), 0.569 (20,000 examples), improving
   over the baseline majority class classification which achieved accuracy
   of 0.526, 0.586, 0.562, respectively. Our approach can be further
   integrated into privacy-preserving data mining, where it can be used to
   assess risk of identification of different groups of individuals within
   a large data set.}},
DOI = {{10.1007/s13748-015-0064-y}},
ISSN = {{2192-6352}},
EISSN = {{2192-6360}},
Unique-ID = {{ISI:000437872500001}},
}

@article{ ISI:000364414700013,
Author = {Kikuchi, Hiroaki and Itoh, Kouichi and Ushida, Mebae and Tsuda, Hiroshi
   and Yamaoka, Yuji},
Title = {{Privacy-Preserving Decision Tree Learning with Boolean Target Class}},
Journal = {{IEICE TRANSACTIONS ON FUNDAMENTALS OF ELECTRONICS COMMUNICATIONS AND
   COMPUTER SCIENCES}},
Year = {{2015}},
Volume = {{E98A}},
Number = {{11}},
Pages = {{2291-2300}},
Month = {{NOV}},
Note = {{International Workshop on Smart Info-Media Systems in Asia (SISA), Ho
   Chi Minh, VIETNAM, OCT 08-10, 2014}},
Abstract = {{This paper studies a privacy-preserving decision tree learning protocol
   (PPDT) for vertically partitioned datasets. In vertically partitioned
   datasets, a single class (target) attribute is shared by both parities
   or carefully treated by either party in existing studies. The proposed
   scheme allows both parties to have independent class attributes in a
   secure way and to combine multiple class attributes in arbitrary boolean
   function, which gives parties some flexibility in data-mining. Our
   proposed PPDT protocol reduces the CPU-intensive computation of
   logarithms by approximating with a piecewise linear function defined by
   light-weight fundamental operations of addition and constant
   multiplication so that information gain for attributes can be evaluated
   in a secure function evaluation scheme. Using the UCI Machine Learning
   dataset and a synthesized dataset, the proposed protocol is evaluated in
   terms of its accuracy and the sizes of trees{*}.}},
DOI = {{10.1587/transfun.E98.A.2291}},
ISSN = {{1745-1337}},
Unique-ID = {{ISI:000364414700013}},
}

@article{ ISI:000437426000001,
Author = {Meeds, Edward and Hendriks, Remco and Al Faraby, Said and Bruntink,
   Magiel and Welling, Max},
Title = {{MLitB: machine learning in the browser}},
Journal = {{PEERJ COMPUTER SCIENCE}},
Year = {{2015}},
Month = {{JUL}},
Abstract = {{With few exceptions, the field of Machine Learning (ML) research has
   largely ignored the browser as a computational engine. Beyond an
   educational resource for ML, the browser has vast potential to not only
   improve the state-of-the-art in ML research, but also, inexpensively and
   on a massive scale, to bring sophisticated ML learning and prediction to
   the public at large. This paper introduces MLitB, a prototype ML
   framework written entirely in Javascript, capable of performing
   large-scale distributed computing with heterogeneous classes of devices.
   The development of MLitB has been driven by several underlying
   objectives whose aim is to make ML learning and usage ubiquitous (by
   using ubiquitous compute devices), cheap and effortlessly distributed,
   and collaborative. This is achieved by allowing every internet capable
   device to run training algorithms and predictive models with no software
   installation and by saving models in universally readable formats. Our
   prototype library is capable of training deep neural networks with
   synchronized, distributed stochastic gradient descent. MLitB offers
   several important opportunities for novel ML research, including:
   development of distributed learning algorithms, advancement of web GPU
   algorithms, novel field and mobile applications, privacy preserving
   computing, and green grid-computing. MLitB is available as open source
   software.}},
DOI = {{10.7717/peerj-cs.11}},
Article-Number = {{e11}},
ISSN = {{2376-5992}},
Unique-ID = {{ISI:000437426000001}},
}

@article{ ISI:000354826200007,
Author = {Murukannaiah, Pradeep K. and Singh, Munindar P.},
Title = {{Platys: An Active Learning Framework for Place-Aware Application
   Development and Its Evaluation}},
Journal = {{ACM TRANSACTIONS ON SOFTWARE ENGINEERING AND METHODOLOGY}},
Year = {{2015}},
Volume = {{24}},
Number = {{3}},
Month = {{MAY}},
Abstract = {{We introduce a high-level abstraction of location called place. A place
   derives its meaning from a user's physical space, activities, or social
   context. In this manner, place can facilitate improved user experience
   compared to the traditional representation of location, which is spatial
   coordinates. We propose the Platys framework as a way to address the
   special challenges of place-aware application development. The core of
   Platys is a middleware that (1) learns a model of places specific to
   each user via active learning, a machine learning paradigm that seeks to
   reduce the user-effort required for training the middleware, and (2)
   exposes the learned user-specific model of places to applications at run
   time, insulating application developers from dealing with both low-level
   sensors and user idiosyncrasies in perceiving places.
   We evaluated Platys via two studies. First, we collected place labels
   and Android phone sensor readings from 10 users. We applied Platys'
   active learning approach to learn each user's places and found that
   Platys (1) requires fewer place labels to learn a user's places with a
   desired accuracy than do two traditional supervised approaches, and (2)
   learns places with higher accuracy than two unsupervised approaches.
   Second, we conducted a developer study to evaluate Platys' efficiency in
   assisting developers and its effectiveness in enabling usable
   applications. In this study, 46 developers employed either Platys or the
   Android location API to develop a place-aware application. Our results
   indicate that application developers employing Platys, when compared to
   those employing the Android API, (1) develop a place-aware application
   faster and perceive reduced difficulty and (2) produce applications that
   are easier to understand (for developers) and potentially more usable
   and privacy preserving (for application users).}},
DOI = {{10.1145/2729976}},
Article-Number = {{19}},
ISSN = {{1049-331X}},
EISSN = {{1557-7392}},
ResearcherID-Numbers = {{Singh, Munindar/U-3881-2019}},
ORCID-Numbers = {{Singh, Munindar/0000-0003-3599-3893}},
Unique-ID = {{ISI:000354826200007}},
}

@inproceedings{ ISI:000380583900084,
Author = {Honda, Katsuhiro and Omori, Masahiro and Ubukata, Seiki and Notsu, Akira},
Book-Group-Author = {{IEEE}},
Title = {{A Privacy-preserving Crowd Movement Analysis by k-member Clustering of
   Face Images}},
Booktitle = {{2015 4TH INTERNATIONAL CONFERENCE ON INFORMATICS, ELECTRONICS \& VISION
   ICIEV 15}},
Year = {{2015}},
Note = {{4th International Conference on Informatics, Electronics \& Vision
   (lCIEV), Fukuoka, JAPAN, JUN 15-18, 2015}},
Organization = {{IEEE Comp Soc; Soc Instrument \& Control Engineers; IEEE Comp Soc e Govt
   STC, IEEE Tech Comm Multimedia Comp; IEEE Kansai Sect; Inst Elect Engn
   Japan; Japan Soc Fuzzy Theory \& Intelligent Informat; Human Interface
   Soc; IEEE CIS Task Force Fuzzy Logic Med Sci; Himeji Initiative
   Computational Med \& Hlth Technology; Inst Mobile Japan; Japan Ergonom
   Soc; JES TC Maritime Ergonom; Japan Inst Nav; W Japan Ind \& Trade
   Convention Assoc; Ctr Nat Sci \& Engn Res}},
Abstract = {{Crowd movement analysis is an important issue in social design. This
   paper studies an machine learning approach to crowd movement estimation
   through face image recognition. Although high performance face
   recognition is a powerful tool in individual authentication with
   surveillance camera images in public spaces, utilization of personal
   information is often hesitated under fear of privacy violation. In this
   paper, a privacy preserving framework for crowd movement analysis is
   proposed considering k-anonymization of face image features. k-anonymity
   is a quantitative measure of secureness in data mining and is expected
   to enhance the utility of personal information. An experimental result
   demonstrates the applicability of the secure framework in capturing
   crowd movement characteristics even if individual features are
   k-aonymized so that each individual is not distinguishable from others k
   - 1 ones.}},
ISBN = {{978-1-4673-6902-2}},
Unique-ID = {{ISI:000380583900084}},
}

@inproceedings{ ISI:000380516200002,
Author = {Hamm, Jihun and Champion, Adam C. and Chen, Guoxing and Belkin, Mikhail
   and Xuan, Dong},
Book-Group-Author = {{IEEE}},
Title = {{Crowd-ML: A Privacy-Preserving Learning Framework for a Crowd of Smart
   Devices}},
Booktitle = {{2015 IEEE 35TH INTERNATIONAL CONFERENCE ON DISTRIBUTED COMPUTING SYSTEMS}},
Series = {{IEEE International Conference on Distributed Computing Systems}},
Year = {{2015}},
Pages = {{11-20}},
Note = {{2015 IEEE 35th International Conference on Distributed Computing
   Systems, Ohio State Univ, Columbus, OH, JUN 29-JUL 02, 2015}},
Abstract = {{Smart devices with built-in sensors, computational capabilities, and
   network connectivity have become increasingly pervasive. Crowds of smart
   devices offer opportunities to collectively sense and perform computing
   tasks at an unprecedented scale. This paper presents Crowd-ML, a
   privacy-preserving machine learning framework for a crowd of smart
   devices, which can solve a wide range of learning problems for
   crowdsensing data with differential privacy guarantees. Crowd-ML endows
   a crowdsensing system with the ability to learn classifiers or
   predictors online from crowdsensing data privately with minimal
   computational overhead on devices and servers, suitable for practical
   large-scale use of the framework. We analyze the performance and
   scalability of Crowd-ML and implement the system with off-the-shelf
   smartphones as a proof of concept. We demonstrate the advantages of
   Crowd-ML with real and simulated experiments under various conditions.}},
DOI = {{10.1109/ICDCS.2015.10}},
ISSN = {{1063-6927}},
ISBN = {{978-1-4673-7214-5}},
Unique-ID = {{ISI:000380516200002}},
}

@inproceedings{ ISI:000380516200032,
Author = {Xu, Kaihe and Yue, Hao and Guo, Linke and Guo, Yuanxiong and Fang,
   Yuguang},
Book-Group-Author = {{IEEE}},
Title = {{Privacy-preserving Machine Learning Algorithms for Big Data Systems}},
Booktitle = {{2015 IEEE 35TH INTERNATIONAL CONFERENCE ON DISTRIBUTED COMPUTING SYSTEMS}},
Series = {{IEEE International Conference on Distributed Computing Systems}},
Year = {{2015}},
Pages = {{318-327}},
Note = {{2015 IEEE 35th International Conference on Distributed Computing
   Systems, Ohio State Univ, Columbus, OH, JUN 29-JUL 02, 2015}},
Abstract = {{Machine learning has played an increasing important role in big data
   systems due to its capability of efficiently discovering valuable
   knowledge and hidden information. Often times big data such as
   healthcare systems or financial systems may involve with multiple
   organizations who may have different privacy policy, and may not
   explicitly share their data publicly while joint data processing may be
   a must. Thus, how to share big data among distributed data processing
   entities while mitigating privacy concerns becomes a challenging
   problem. Traditional methods rely on cryptographic tools and/ or
   randomization to preserve privacy. Unfortunately, this alone may be
   inadequate for the emerging big data systems because they are mainly
   designed for traditional small-scale data sets. In this paper, we
   propose a novel framework to achieve privacy-preserving machine learning
   where the training data are distributed and each shared data portion is
   of large volume. Specifically, we utilize the data locality property of
   Apache Hadoop architecture and only a limited number of cryptographic
   operations at the Reduce() procedures to achieve privacy-preservation.
   We show that the proposed scheme is secure in the semi-honest model and
   use extensive simulations to demonstrate its scalability and
   correctness.}},
DOI = {{10.1109/ICDCS.2015.40}},
ISSN = {{1063-6927}},
ISBN = {{978-1-4673-7214-5}},
ResearcherID-Numbers = {{Fang, Yuguang/A-7484-2009}},
ORCID-Numbers = {{Fang, Yuguang/0000-0002-1079-3871}},
Unique-ID = {{ISI:000380516200032}},
}

@inproceedings{ ISI:000380518300008,
Author = {Parada, Raul and Melia-Segui, Joan and Carreras, Anna and Morenza-Cinos,
   Marc and Pous, Rafael},
Book-Group-Author = {{IEEE}},
Title = {{Measuring User-Object Interactions in IoT Spaces}},
Booktitle = {{2015 IEEE INTERNATIONAL CONFERENCE ON RFID TECHNOLOGY AND APPLICATIONS
   (RFID-TA)}},
Year = {{2015}},
Pages = {{52-58}},
Note = {{IEEE International Conference on RFID Technology and Applications
   (RFID-TA), Tokyo Bigsight, JAPAN, SEP 16-18, 2015}},
Abstract = {{Online commerce currently provides better customer activity information,
   compared with traditional retail stores. For instance, measuring
   customers' interest on products in shelves is a complex task in physical
   environments. However, these scenarios may benefit from the Internet of
   Things (IoT) technologies to obtain context-aware information hard to
   obtain otherwise. For instance, in a real store, users may show their
   interest in a given product depending on the time interacted with it. We
   present a system designed to reliably detect user-object interactions in
   an RFID-enabled context-aware shelf scenario, with the goal to measure
   user activity based on the weighted Information Gain classifier (wiG),
   an empirical machine learning technique. The system is configured by
   means of thresholds determining the classification accuracy, and it is
   automatically adapted to different scenarios by means of an automated
   calibration method. Our proposed user-object interaction measurement
   method achieves performance above 80\% in a real environment evaluation,
   indicating a high reliability. Our proposal could be used to feed
   user-centric privacy-preserving recommender systems in brick-and-mortar
   stores, or as aiding tool for visually impaired users.}},
ISBN = {{978-1-4799-7990-5}},
ResearcherID-Numbers = {{Pous, Rafael/G-3193-2010}},
ORCID-Numbers = {{Pous, Rafael/0000-0002-1122-5219}},
Unique-ID = {{ISI:000380518300008}},
}

@inproceedings{ ISI:000375153000020,
Author = {David, Bernardo and Dowsley, Rafael and Katti, Raj and Nascimento,
   Anderson C. A.},
Editor = {{Au, MH and Miyaji, A}},
Title = {{Efficient Unconditionally Secure Comparison and Privacy Preserving
   Machine Learning Classification Protocols}},
Booktitle = {{PROVABLE SECURITY, PROVSEC 2015}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2015}},
Volume = {{9451}},
Pages = {{354-367}},
Note = {{9th International Conference on Provable Security (ProvSec), Kanazawa,
   JAPAN, NOV 24-26, 2015}},
Organization = {{Informat Technol Promot Agcy; Japan Adv Inst Sci \& Technol; Mitsubishi
   Elect; Natl Inst Informat \& Commun Technol; Support Ctr Adv
   Telecommunicat Technol Res; Nippon Telegraph \& Telephone Corp}},
Abstract = {{We propose an efficient unconditionally secure protocol for privacy
   preserving comparison of l-bit integers when both integers are shared
   between two semi-honest parties. Using our comparison protocol as a
   building block, we construct two-party generic private machine learning
   classifiers. In this scenario, one party holds an input while the other
   holds a model and they wish to classify the input according to the model
   without revealing their private information to each other. Our
   constructions are based on the setup assumption that there exists
   pre-distributed correlated randomness available to the computing
   parties, the so-called commodity-based model. The protocols are storage
   and computationally efficient, consisting only of additions and
   multiplications of integers.}},
DOI = {{10.1007/978-3-319-26059-4\_20}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-26059-4; 978-3-319-26058-7}},
ResearcherID-Numbers = {{Nascimento, Anderson/A-2107-2011}},
Unique-ID = {{ISI:000375153000020}},
}

@inproceedings{ ISI:000370288300300,
Author = {Ishibuchi, Hisao and Nojima, Yusuke},
Editor = {{Yazici, A and Pal, NR and Kaymak, U and Martin, T and Ishibuchi, H and Lin, CT and Sousa, JMC and Tutmez, B}},
Title = {{Handling a Training Dataset as a Black-Box Model for Privacy Preserving
   in Fuzzy GBML Algorithms}},
Booktitle = {{2015 IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS (FUZZ-IEEE 2015)}},
Series = {{IEEE International Fuzzy Systems Conference Proceedings}},
Year = {{2015}},
Note = {{IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), Istanbul,
   TURKEY, AUG 02-05, 2015}},
Organization = {{IEEE; IEEE Computat Intelligence Soc; Middle E Tech Univ; Kadir Has
   Univ; Eindhoven Univ Technol}},
Abstract = {{In this paper, we assume that we have two types of datasets for
   classifier design. One is an in-house dataset which is fully available
   for classifier design as training data. The other is an external dataset
   which is kept under a very severe privacy preserving policy. We assume
   that the available information on the external dataset is only the error
   rate of a presented classifier. No other information is available such
   as the number of patterns, attribute values of each pattern, and its
   class label. Thus, the external dataset can be viewed as a black-box
   model where the error rate is calculated as an output for an input
   classifier. In this paper, we discuss how such a black-box type dataset
   can be utilized in fuzzy genetics-based machine leaning (GBML). We use a
   hybrid fuzzy GBML algorithm where its Michigan-style part is applied to
   each individual of a Pittsburgh-style part. Since a fuzzy rule-based
   classifier is an individual in the Pittsburgh-style part, a black-box
   type dataset can be utilized for fitness evaluation. Through
   computational experiments, we examine the effect of using a black-box
   type dataset in comparison with fuzzy rule-based classifiers design only
   from a fully available dataset.}},
ISSN = {{1544-5615}},
ISBN = {{978-1-4673-7428-6}},
Unique-ID = {{ISI:000370288300300}},
}

@inproceedings{ ISI:000380429300033,
Author = {Gokulnath, C. and Priyan, M. K. and Balan, E. Vishnu and Prabha, K. P.
   Rama and Jeyanthi, R.},
Book-Group-Author = {{IEEE}},
Title = {{Preservation of Privacy in Data Mining by using PCA Based Perturbation
   Technique}},
Booktitle = {{2015 INTERNATIONAL CONFERENCE ON SMART TECHNOLOGIES AND MANAGEMENT FOR
   COMPUTING, COMMUNICATION, CONTROLS, ENERGY AND MATERIALS (ICSTM)}},
Year = {{2015}},
Pages = {{202-206}},
Note = {{International Conference on Smart Technologies and Management for
   Computing, Communication, Controls, Energy and Materials (ICSTM),
   Chennai, INDIA, MAY 06-08, 2015}},
Organization = {{VEL TECH; OKLAHOMA STATE Univ; IEEE; IEEE Madras Section; Defence R\&D
   Org; IEEE Computer Soc; IEEE Computational Intelligence Soc; Computer
   Soc India; ISTE; IEEE PES; Indian Journal Sci and Technology; IEEE
   Technology Management; ICTACT; IEEE Youngprofessionals}},
Abstract = {{Due to raising concerns about the privacy preserving of personal
   information, organizations which are using the customers' records in
   data mining activities are rammed to take actions for protecting the
   individual's privacy. Preserving of sensitive and personal information
   is vital for the success of data mining techniques. Privacy Preserving
   Data Mining (PPDM) handles such consequences by reconciliation of both
   preserving privacy and data utilization. Conventionally, Geometrical
   Data Transformation Methods (GDTMs) have been extensively used for
   privacy conserving cluster. The major drawback in these GDTMs are
   geometric conversion function are not reversible, that results in a low
   level assurance of security. In this paper, the technique that preserves
   the privacy of delicate information in a multiparty cluster situation
   called the guideline segment investigation based technique is proposed.
   The function of this proficiency is assessed advance by employing a
   classic K-means cluster algorithms and machine learning-based cluster
   methodology on artificial and realistic world information sets. The
   effectiveness of grouping is computed prior and then afterward the
   change of security preserving. Our suggested transformation established
   on PCA when competed to the traditional GDTMs resulted in superior
   protection of privacy and improve performance.}},
ISBN = {{978-1-4799-9855-5}},
ResearcherID-Numbers = {{M K, Priyan/U-3908-2018}},
ORCID-Numbers = {{M K, Priyan/0000-0001-6149-2705}},
Unique-ID = {{ISI:000380429300033}},
}

@article{ ISI:000362966800002,
Author = {Stecking, Ralf and Schebesch, Klaus B.},
Title = {{Classification of credit scoring data with privacy constraints}},
Journal = {{INTELLIGENT DATA ANALYSIS}},
Year = {{2015}},
Volume = {{19}},
Number = {{1}},
Pages = {{S3-S18}},
Abstract = {{Modern data collections create vast opportunities for detecting useful
   hidden relationships. Also, increasingly, they fuel data privacy
   concerns. A trade-off between privacy protection and data usefulness is
   by now widely acknowledged. Real world data classification tasks, as for
   example credit scoring applications have to deal with such data security
   limitations by finding a way to effectively incorporate privacy
   preserving procedures. To this end we propose as a first stage to use a
   microaggregation procedure in order to anonymize data over personal
   credit client feature information. In a second stage we examine the
   performance of support vector machines (SVM) on such anonymized data.
   SVM are powerful and robust machine learning methods, having superior
   credit scoring classification performance when applied to original,
   non-anonymized data. We first partition the original credit scoring data
   set and construct anonymized data representatives, which are then used
   for credit client behavior forecasting models constructed by SVM and
   other comparable learning methods. The validation procedure for such
   models is adapted to the two-stage modeling approach. In order to assess
   the loss owing to data anonymization, the different classification
   models are evaluated against models that are trained on the original
   data.}},
DOI = {{10.3233/IDA-150767}},
ISSN = {{1088-467X}},
EISSN = {{1571-4128}},
Unique-ID = {{ISI:000362966800002}},
}

@inproceedings{ ISI:000455735400027,
Author = {Ullman, Jonathan},
Book-Group-Author = {{ACM/SIGMOD}},
Title = {{Private Multiplicative Weights Beyond Linear Queries}},
Booktitle = {{PODS'15: PROCEEDINGS OF THE 33RD ACM SYMPOSIUM ON PRINCIPLES OF DATABASE
   SYSTEMS}},
Year = {{2015}},
Pages = {{303-312}},
Note = {{33rd ACM Symposium on Principles of Database Systems (PODS), Melbourne,
   AUSTRALIA, MAY 31-JUN 04, 2015}},
Organization = {{Assoc Comp Machinery; Assoc Comp Machinery Special Interest Grps
   Management Data Algorithms \& Computat Theory \& Artificial Intelligence}},
Abstract = {{A wide variety of fundamental data analyses in machine learning, such as
   linear and logistic regression, require minimizing a convex function
   defined by the data. Since the data may contain sensitive information
   about individuals, and these analyses can leak that sensitive
   information, it is important to be able to solve convex minimization in
   a privacy-preserving way.
   A series of recent results show how to accurately solve a single convex
   minimization problem in a differentially private manner. However, the
   same data is often analyzed repeatedly, and little is known about
   solving multiple convex minimization problems with differential privacy.
   For simpler data analyses, such as linear queries, there are remarkable
   differentially private algorithms such as the private multiplicative
   weights mechanism (Hardt and Rothblum, FOCS 2010) that accurately answer
   exponentially many distinct queries. In this work, we extend these
   results to the case of convex minimization and show how to give accurate
   and differentially private solutions to exponentially many convex
   minimization problems on a sensitive dataset.}},
DOI = {{10.1145/2745754.2745755}},
ISBN = {{978-1-4503-2757-2}},
Unique-ID = {{ISI:000455735400027}},
}

@inproceedings{ ISI:000468650800002,
Author = {Malekzadeh, Mohammad and Clegg, Richard G. and Cavallaro, Andrea and
   Haddadi, Hamed},
Editor = {{Maia, F and Mercier, H and Brito, A}},
Title = {{Protecting Sensory Data against Sensitive Inferences}},
Booktitle = {{PROCEEDINGS OF THE WORKSHOP ON PRIVACY BY DESIGN IN DISTRIBUTED SYSTEMS
   (P2DS'18)}},
Year = {{2015}},
Note = {{Workshop on Privacy by Design in Distributed Systems (P2DS), Porto,
   PORTUGAL, APR 23, 2018}},
Abstract = {{There is growing concern about how personal data are used when users
   grant applications direct access to the sensors of their mobile devices.
   In fact, high resolution temporal data generated by motion sensors
   reflect directly the activities of a user and indirectly physical and
   demographic attributes. In this paper, we propose a feature learning
   architecture for mobile devices that provides flexible and negotiable
   privacy-preserving sensor data transmission by appropriately
   transforming raw sensor data. The objective is to move from the current
   binary setting of granting or not permission to an application, toward a
   model that allows users to grant each application permission over a
   limited range of inferences according to the provided services. The
   internal structure of each component of the proposed architecture can be
   flexibly changed and the trade-off between privacy and utility can be
   negotiated between the constraints of the user and the underlying
   application. We validated the proposed architecture in an activity
   recognition application using two real-world datasets, with the
   objective of recognizing an activity without disclosing gender as an
   example of private information. Results show that the proposed framework
   maintains the usefulness of the transformed data for activity
   recognition, with an average loss of only around three percentage
   points, while reducing the possibility of gender classification to
   around 50\%, the target random guess, from more than 90\% when using raw
   sensor data. We also present and distribute MotionSense, a new dataset
   for activity and attribute recognition collected from motion sensors.}},
DOI = {{10.1145/3195258.3195260}},
ISBN = {{978-1-4503-5654-1}},
Unique-ID = {{ISI:000468650800002}},
}

@inproceedings{ ISI:000362241500069,
Author = {Manohar, Gautam and Tucker, Conrad S.},
Book-Group-Author = {{ASME}},
Title = {{A Privacy Preserving Data Mining Methodology for Dynamically Predicting
   Emerging Human Threats}},
Booktitle = {{PROCEEDINGS OF THE ASME INTERNATIONAL DESIGN ENGINEERING TECHNICAL
   CONFERENCES AND COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE,
   2013, VOL 2A}},
Year = {{2014}},
Note = {{ASME International Design Engineering Technical Conferences / Computers
   and Information in Engineering Conference (IDETC/CIE), Portland, OR, AUG
   04-07, 2013}},
Organization = {{ASME, Design Engn Div; ASME, Comp \& Informat Engn Div}},
Abstract = {{This paper proposes a privacy preserving data mining driven methodology
   for predicting emerging human threats in a public space by capturing
   large scale, real time body movement data (spatial data represented in
   X, Y, Z coordinate space) using Red-Green-Blue (RGB) image, infrared
   depth and skeletal image sensing technology. Unlike traditional passive
   surveillance systems (e.g., CCTV video surveillance systems), multimodal
   surveillance technologies have the ability to capture multiple data
   streams in a real time dynamic manner. However, mathematical models
   based on machine learning principles are needed to convert the
   large-scale data into knowledge to serve as a decision support system
   for autonomously predicting emerging threats, rather than just recording
   and observing them as they occur.
   To this end, the authors of this work present a privacy preserving data
   mining driven methodology that captures emergent behavior of individuals
   in a public space and classifies them as a threat or not a threat, based
   on the underlying body movements through space and time. An audience in
   a public environment is presented as the case study for this paper with
   the aim of classifying individuals in the audience as threats (or not),
   based on their temporal body behavior profiles.}},
Article-Number = {{V02AT02A069}},
ISBN = {{978-0-7918-5585-0}},
Unique-ID = {{ISI:000362241500069}},
}

@article{ ISI:000332477600010,
Author = {Iqbal, Khalid and Yin, Xu-Cheng and Hao, Hong-Wei and Ilyas, Qazi
   Mudassar and Yin, Xuwang},
Title = {{A central tendency-based privacy preserving model for sensitive XML
   association rules using Bayesian networks}},
Journal = {{INTELLIGENT DATA ANALYSIS}},
Year = {{2014}},
Volume = {{18}},
Number = {{2}},
Pages = {{281-303}},
Abstract = {{The rationale of XML design is to transfer and store data at different
   levels. A key feature of these levels in an XML document is to identify
   its components for additional processing. XML components can expose
   sensitive information after application of data mining techniques over a
   shared database. Therefore, privacy preservation of sensitive
   information must be ensured prior to signify the outcome especially in
   sensitive XML Association Rules. Privacy issues in XML domain are not
   exceptionally addressed to determine a solution by the academia in a
   reliable and precise manner. In this paper, we have proposed a model for
   identifying sensitive items (nodes) to declare sensitive XML association
   rules and then to hide them. Bayesian networks-based central tendency
   measures are applied in declaration of sensitive XML association rules.
   K2 algorithm is used to generate Bayesian networks to ensure reliability
   and accuracy in preserving privacy of XML Association Rules. The
   proposed model is tested and compared using several case studies and
   large UCI machine learning datasets. The experimental results show
   improved accuracy and reliability of proposed model without any side
   effects such as new rules and lost rules. The proposed model uses the
   same minimum support threshold to find XML Association Rules from the
   original and transformed data sources. The significance of the proposed
   model is to minimize an incredible disclosure risk involved in XML
   association rule mining from external parties in a competitive business
   environment.}},
DOI = {{10.3233/IDA-140641}},
ISSN = {{1088-467X}},
EISSN = {{1571-4128}},
ORCID-Numbers = {{Yin, Xucheng/0000-0003-0023-0220}},
Unique-ID = {{ISI:000332477600010}},
}

@inproceedings{ ISI:000380559500032,
Author = {Saravanan, M. and Thoufeeq, A. M. and Akshaya, S. and Manchari, V. L.
   Jayasre},
Book-Group-Author = {{IEEE}},
Title = {{Exploring New Privacy Approaches in a Scalable Classification Framework}},
Booktitle = {{2014 INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED ANALYTICS
   (DSAA)}},
Year = {{2014}},
Pages = {{209-215}},
Note = {{IEEE International Conference on Data Science \& Advanced Analytics,
   Dept Comp Sci \& Technol Shanghai Jiaotong Univ, Shanghai, PEOPLES R
   CHINA, OCT 30-NOV 01, 2014}},
Organization = {{KDD; ACM Chapter; IEEE; IEEE Computat Intelligence Soc; KDnuggets;
   Springer; CRC; Huawei; Ctrip; Univ Joseph Fourier; Univ Technol Sydney;
   UTS:AAI; LN Tech Univ; Stanford Ctr Sustainable Dev \& Global
   Competitiveness; IEEE Task Force Behav Economic \& Socio-Cultural Comp;
   IEEE Task Force Data Sci \& Adv Analytics; Natl Univ Def Technol}},
Abstract = {{Recent advancements in Information and Communication Technologies (ICT)
   enable many organizations to collect, store and control massive amount
   of various types of details of individuals from their regular
   transactions (credit card, mobile phone, smart meter etc.). While using
   these wealth of information for Personalized Recommendations provides
   enormous opportunities for applying data mining (or machine learning)
   tasks, there is a need to address the challenge of preserving
   individuals privacy during the time of running predictive analytics on
   Big Data. Privacy Preserving Data Mining (PPDM) on these applications is
   particularly challenging, because it involves and process large volume
   of complex, heterogeneous, and dynamic details of individuals. Ensuring
   that privacy-protected data remains useful in intended applications,
   such as building accurate data mining models or enabling complex
   analytic tasks, is essential. Differential Privacy has been tried with
   few of the PPDM methods and is immune to attacks with auxiliary
   information. In this paper, we propose a distributed implementation
   based on Map Reduce computing model for C4.5 Decision Tree algorithm and
   run extensive experiments on three different datasets using Hadoop
   Cluster. The novelty of this work is to experiment two different privacy
   methods: First method is to use perturbed data on decision tree
   algorithm for prediction in privacy-preserving data sharing and the
   second method is based on applying raw data to the privacy-preserving
   decision tree algorithm for private data analysis. In addition to this,
   we propose the combination of the methods as hybrid technique to
   maintain accuracy (Utility) and privacy in an acceptable level. The
   proposed privacy approaches has two potential benefits in the context of
   data mining tasks: it allows the service providers to outsource data
   mining tasks without exposing the raw data, and it allows data providers
   to share data access to third parties while limiting privacy risks.}},
ISBN = {{978-1-4799-6991-3}},
Unique-ID = {{ISI:000380559500032}},
}

@inproceedings{ ISI:000452647101062,
Author = {Chaudhuri, Kamalika and Hsu, Daniel and Song, Shuang},
Editor = {{Ghahramani, Z and Welling, M and Cortes, C and Lawrence, ND and Weinberger, KQ}},
Title = {{The Large Margin Mechanism for Differentially Private Maximization}},
Booktitle = {{ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)}},
Series = {{Advances in Neural Information Processing Systems}},
Year = {{2014}},
Volume = {{27}},
Note = {{28th Conference on Neural Information Processing Systems (NIPS),
   Montreal, CANADA, DEC 08-13, 2014}},
Abstract = {{A basic problem in the design of privacy-preserving algorithms is the
   private maximization problem: the goal is to pick an item from a
   universe that (approximately) maximizes a data-dependent function, all
   under the constraint of differential privacy. This problem has been used
   as a sub-routine in many privacy-preserving algorithms for statistics
   and machine learning.
   Previous algorithms for this problem are either range-dependent-i.e.,
   their utility diminishes with the size of the universe-or only apply to
   very restricted function classes. This work provides the first general
   purpose, range-independent algorithm for private maximization that
   guarantees approximate differential privacy. Its applicability is
   demonstrated on two fundamental tasks in data mining and machine
   learning.}},
ISSN = {{1049-5258}},
Unique-ID = {{ISI:000452647101062}},
}

@inproceedings{ ISI:000452647100105,
Author = {Hardt, Moritz and Price, Eric},
Editor = {{Ghahramani, Z and Welling, M and Cortes, C and Lawrence, ND and Weinberger, KQ}},
Title = {{The Noisy Power Method: A Meta Algorithm with Applications}},
Booktitle = {{ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)}},
Series = {{Advances in Neural Information Processing Systems}},
Year = {{2014}},
Volume = {{27}},
Note = {{28th Conference on Neural Information Processing Systems (NIPS),
   Montreal, CANADA, DEC 08-13, 2014}},
Abstract = {{We provide a new robust convergence analysis of the well-known power
   method for computing the dominant singular vectors of a matrix that we
   call the noisy power method. Our result characterizes the convergence
   behavior of the algorithm when a significant amount noise is introduced
   after each matrix-vector multiplication. The noisy power method can be
   seen as a meta-algorithm that has recently found a number of important
   applications in a broad range of machine learning problems including
   alternating minimization for matrix completion, streaming principal
   component analysis (PCA), and privacy-preserving spectral analysis. Our
   general analysis subsumes several existing ad-hoc convergence bounds and
   resolves a number of open problems in multiple applications:
   Streaming PCA. A recent work of Mitliagkas et al. (NIPS 2013) gives a
   space-efficient algorithm for PCA in a streaming model where samples are
   drawn from a gaussian spiked covariance model. We give a simpler and
   more general analysis that applies to arbitrary distributions confirming
   experimental evidence of Mitliagkas et al. Moreover, even in the spiked
   covariance model our result gives quantitative improvements in a natural
   parameter regime. It is also notably simpler and follows easily from our
   general convergence analysis of the noisy power method together with a
   matrix Chernoff bound.
   Private PCA. We provide the first nearly-linear time algorithm for the
   problem of differentially private principal component analysis that
   achieves nearly tight worst-case error bounds. Complementing our
   worst-case bounds, we show that the error dependence of our algorithm on
   the matrix dimension can be replaced by an essentially tight dependence
   on the coherence of the matrix. This result resolves the main problem
   left open by Hardt and Roth (STOC 2013). The coherence is always bounded
   by the matrix dimension but often substantially smaller thus leading to
   strong average-case improvements over the optimal worst-case bound.}},
ISSN = {{1049-5258}},
Unique-ID = {{ISI:000452647100105}},
}

@inproceedings{ ISI:000452118900031,
Author = {Koufi, Vassiliki and Malamateniou, Flora and Prentza, Andriana and
   Vassilacopoulos, George},
Editor = {{Mantas, J and Househ, MS and Hasman, A}},
Title = {{A Framework for Privacy-preserving Classification of Next-generation PHR
   data}},
Booktitle = {{INTEGRATING INFORMATION TECHNOLOGY AND MANAGEMENT FOR QUALITY OF CARE}},
Series = {{Studies in Health Technology and Informatics}},
Year = {{2014}},
Volume = {{202}},
Pages = {{119-122}},
Note = {{12th Annual International Conference on Informatics, Management, and
   Technology in Healthcare (ICIMTH), Athens, GREECE, JUL 10-13, 2014}},
Abstract = {{Personal Health Records (PHRs), integrated with data from various
   sources, such as social care data, Electronic Health Record data and
   genetic information, are envisaged as having a pivotal role in
   transforming healthcare. These data, lumped under the term `big data',
   are usually complex, noisy, heterogeneous, longitudinal and voluminous
   thus prohibiting their meaningful use by clinicians. Deriving value from
   these data requires the utilization of innovative data analysis
   techniques, which, however, may be hindered due to potential security
   and privacy breaches that may arise from improper release of personal
   health information. This paper presents a HIPAA-compliant machine
   learning framework that enables privacy-preserving classification of
   next-generation PHR data. The predictive models acquired can act as
   supporting tools to clinical practice by enabling more effective
   prevention, diagnosis and treatment of new incidents. The proposed
   framework has a huge potential for complementing medical staff expertise
   as it outperforms the manual inspection of PHR data while protecting
   patient privacy.}},
DOI = {{10.3233/978-1-61499-423-7-119}},
ISSN = {{0926-9630}},
EISSN = {{1879-8365}},
ISBN = {{978-1-61499-423-7; 978-1-61499-422-0}},
Unique-ID = {{ISI:000452118900031}},
}

@article{ ISI:000323677100011,
Author = {Sarwate, Anand D. and Chaudhuri, Kamalika},
Title = {{Signal Processing and Machine Learning with Differential Privacy
   {[}Algorithms and challenges for continuous data]}},
Journal = {{IEEE SIGNAL PROCESSING MAGAZINE}},
Year = {{2013}},
Volume = {{30}},
Number = {{5}},
Pages = {{86-94}},
Month = {{SEP}},
Abstract = {{Private companies, government entities, and institutions such as
   hospitals routinely gather vast amounts of digitized personal
   information about the individuals who are their customers, clients, or
   patients. Much of this information is private or sensitive, and a key
   technological challenge for the future is how to design systems and
   processing techniques for drawing inferences from this large-scale data
   while maintaining the privacy and security of the data and individual
   identities. Individuals are often willing to share data, especially for
   purposes such as public health, but they expect that their identity or
   the fact of their participation will not be disclosed. In recent years,
   there have been a number of privacy models and privacy-preserving data
   analysis algorithms to answer these challenges. In this article, we will
   describe the progress made on differentially private machine learning
   and signal processing.}},
DOI = {{10.1109/MSP.2013.2259911}},
ISSN = {{1053-5888}},
ORCID-Numbers = {{Sarwate, Anand/0000-0001-6123-5282}},
Unique-ID = {{ISI:000323677100011}},
}

@article{ ISI:000319635100009,
Author = {Vatsalan, Dinusha and Christen, Peter and Verykios, Vassilios S.},
Title = {{A taxonomy of privacy-preserving record linkage techniques}},
Journal = {{INFORMATION SYSTEMS}},
Year = {{2013}},
Volume = {{38}},
Number = {{6, SI}},
Pages = {{946-969}},
Month = {{SEP}},
Abstract = {{The process of identifying which records in two or more databases
   correspond to the same entity is an important aspect of data quality
   activities such as data pre-processing and data integration. Known as
   record linkage, data matching or entity resolution, this process has
   attracted interest from researchers in fields such as databases and data
   warehousing, data mining, information systems, and machine learning.
   Record linkage has various challenges, including scalability to large
   databases, accurate matching and classification, and privacy and
   confidentiality. The latter challenge arises because commonly personal
   identifying data, such as names, addresses and dates of birth of
   individuals, are used in the linkage process. When databases are linked
   across organizations, the issue of how to protect the privacy and
   confidentiality of such sensitive information is crucial to successful
   application of record linkage.
   In this paper we present an overview of techniques that allow the
   linking of databases between organizations while at the same time
   preserving the privacy of these data. Known as `privacy-preserving
   record linkage' (PPRL), various such techniques have been developed. We
   present a taxonomy of PPRL techniques to characterize these techniques
   along 15 dimensions, and conduct a survey of PPRL techniques. We then
   highlight shortcomings of current techniques and discuss avenues for
   future research. (C) 2012 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.is.2012.11.005}},
ISSN = {{0306-4379}},
EISSN = {{1873-6076}},
ORCID-Numbers = {{Vatsalan, Dinusha/0000-0001-6713-7667}},
Unique-ID = {{ISI:000319635100009}},
}

@article{ ISI:000321494200021,
Author = {Samet, Saeed and Miri, Ali and Granger, Eric},
Title = {{Incremental learning of privacy-preserving Bayesian networks}},
Journal = {{APPLIED SOFT COMPUTING}},
Year = {{2013}},
Volume = {{13}},
Number = {{8}},
Pages = {{3657-3667}},
Month = {{AUG}},
Abstract = {{Bayesian Networks (BNs) have received significant attention in various
   academic and industrial applications, such as modeling knowledge in
   image processing, engineering, medicine and bio-informatics. Preserving
   the privacy of sensitive data, owned by different parties, is often a
   critical issue. However, in many practical applications, BNs must train
   from data that gradually becomes available at different period of times,
   on which the traditional batch learning algorithms are not suitable or
   applicable. In this paper, an algorithm based on a new and efficient
   version of Sufficient Statistics is proposed for incremental learning
   with BNs. The standard kappa 2 algorithm is also modified to be utilized
   inside the incremental learning algorithm. Next, some secure building
   blocks such as secure comparison, and factorial, which are resistant
   against colluding attacks and could be applied securely over public
   channels like internet, are presented to be used inside the main
   protocol. Then a privacy-preserving protocol is proposed for incremental
   learning of BNs, in which the structure and probabilities are estimated
   incrementally from homogeneously distributed and gradually available
   data among two or multi-parties. Finally, security and complexity
   analysis along with the experimental results are presented to compare
   with the batch algorithm and to show its performance and applicability
   in real world applications. (C) 2013 Elsevier B. V. All rights reserved.}},
DOI = {{10.1016/j.asoc.2013.03.011}},
ISSN = {{1568-4946}},
ORCID-Numbers = {{Samet, Saeed/0000-0002-5116-5484}},
Unique-ID = {{ISI:000321494200021}},
}

@article{ ISI:000316774700028,
Author = {Banu, R. Vidya and Nagaveni, N.},
Title = {{Evaluation of a perturbation-based technique for privacy preservation in
   a multi-party clustering scenario}},
Journal = {{INFORMATION SCIENCES}},
Year = {{2013}},
Volume = {{232}},
Pages = {{437-448}},
Month = {{MAY 20}},
Abstract = {{Data processing techniques and the growth of the intemet have resulted
   in a data explosion. The data that are now available may contain
   sensitive information that could, if misused, jeopardise the privacy of
   individuals. In today's web world, the privacy of personal and personal
   business information is a growing concern for individuals, corporate
   entities and governments. Preserving personal and sensitive information
   is critical to the success of today's data mining techniques. Preserving
   the privacy of data is even more crucial in critical sectors such as
   defence, health care and finance. Privacy Preserving Data Mining (PPDM)
   addresses such issues by balancing the preservation of privacy and the
   utilisation of data.
   Traditionally, Geometrical Data Transformation Methods (GDTMs) have been
   widely used for privacy preserving clustering. The drawback of these
   methods is that geometric transformation functions are invertible, which
   results in a lower level of privacy protection. In this work, a
   Principal Component Analysis (PCA)-based technique that preserves the
   privacy of sensitive information in a multi-party clustering scenario is
   proposed. The performance of this technique is evaluated further by
   applying a classical K-means clustering algorithm, as well as a machine
   learning-based clustering method on synthetic and real world datasets.
   The accuracy of clustering is computed before and after
   privacy-preserving transformation. The proposed PCA-based transformation
   method resulted in superior privacy protection and better performance
   when compared to the traditional GDTMs. (C) 2012 Elsevier Inc. All
   rights reserved.}},
DOI = {{10.1016/j.ins.2012.02.045}},
ISSN = {{0020-0255}},
Unique-ID = {{ISI:000316774700028}},
}

@article{ ISI:000209204000004,
Author = {Wang, Shitong and Deng, Zhaohong and Chung, Fu-lai and Hu, Wenjun},
Title = {{From Gaussian kernel density estimation to kernel methods}},
Journal = {{INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS}},
Year = {{2013}},
Volume = {{4}},
Number = {{2}},
Pages = {{119-137}},
Month = {{APR}},
Abstract = {{This paper explores how a kind of probabilistic systems, namely,
   Gaussian kernel density estimation (GKDE), can be used to interpret
   several classical kernel methods, including the well-known support
   vector machine (SVM), support vector regression (SVR), one-class kernel
   classifier, i.e., support vector data description (SVDD) or equivalently
   minimal enclosing ball (MEB), and the fuzzy systems (FS). For the SVM,
   we reveal that the classical SVM with Gaussian density kernel attempts
   to find a noisy GKDE based Bayesian classifier with equal prior
   probabilities for each class. For the SVR, the classification based
   epsilon-SVR attempts to obtain two noisy GKDEs for each class in the
   constructed binary classification dataset, and the decision boundary
   just corresponds to the mapping function of the original regression
   problem. For the MEB or SVDD, we reveal the equivalence between it and
   the integrated-squared-errors (ISE) criterion based GKDE and by using
   this equivalence a MEB based classifier with privacy-preserving function
   is proposed for one kind of classification tasks where the datasets
   contain privacy-preserving clouds. For the FS, we show that the GKDE for
   a regression dataset is equivalent to the construction of a zero-order
   Takagi-Sugeno-Kang (TSK) fuzzy system based on the same dataset. Our
   extensive experiments confirm the obtained conclusions and demonstrated
   the effectiveness of the proposed new machine learning and modeling
   methods.}},
DOI = {{10.1007/s13042-012-0078-8}},
ISSN = {{1868-8071}},
EISSN = {{1868-808X}},
ORCID-Numbers = {{CHUNG, Fu Lai Korris/0000-0001-5294-8168}},
Unique-ID = {{ISI:000209204000004}},
}

@inproceedings{ ISI:000342910700046,
Author = {Krawczyk, Bartosz and Wozniak, Michal},
Editor = {{Pan, JS and Wozniak, M and Quintian, H and Polycarpou, MM and DdeCarvalho, ACPLF and Corchado, E}},
Title = {{Distributed Privacy-Preserving Minimal Distance Classification}},
Booktitle = {{HYBRID ARTIFICIAL INTELLIGENT SYSTEMS}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2013}},
Volume = {{8073}},
Pages = {{462-471}},
Note = {{8th International Conference on Hybrid Artificial Intelligent Systems
   (HAIS), Salamanca, SPAIN, SEP 11-13, 2013}},
Organization = {{IEEE Secc Espana; IEEE Syst Man \& Cybernet Capitulo Espanol; AEPIA;
   Univ Salamanca; World Federat Soft Comp; MIR Labs; IT4Innovat Ctr
   Excellence; Int Federat Computat Logic; Minist}},
Abstract = {{The paper focuses on the problem of preserving privacy for a minimal
   distance classifier working in the distributed environment. On the basis
   of the study of available works devoted to privacy aspects of machine
   learning methods, we propose the novel definition and taxonomy of
   privacy. This taxonomy was used to develop new effective classification
   algorithms which can work in distributed computational environment and
   assure a chosen privacy level. Instead of using additional algorithms
   for secure computing, the privacy assurance is embedded in the
   classification process itself. This lead to a significant reduction of
   the overall computational complexity what was confirmed by the computer
   experiments which were carried out on diverse benchmark datasets.}},
ISSN = {{0302-9743}},
ISBN = {{978-3-642-40846-5}},
ResearcherID-Numbers = {{Wozniak, Michal/A-4806-2008}},
ORCID-Numbers = {{Wozniak, Michal/0000-0003-0146-4205}},
Unique-ID = {{ISI:000342910700046}},
}

@inproceedings{ ISI:000335342800142,
Author = {Jinsai, Jiang and Umano, Motohide and Seta, Kazuhisa},
Book-Group-Author = {{IEEE}},
Title = {{Privacy Preserving Extraction of Fuzzy Rules from Distributed Data}},
Booktitle = {{2013 IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS (FUZZ - IEEE 2013)}},
Series = {{IEEE International Conference on Fuzzy Systems}},
Year = {{2013}},
Note = {{IEEE International Conference on Fuzzy Systems (FUZZ), Hyderabad, INDIA,
   JUL 07-10, 2013}},
Organization = {{IEEE; IEEE Computat Intelligence Soc; IEEE Computat Intelligence Soc,
   Hyderabad Chapter; IEEE Computat Intelligence Soc, Calcutta Chapter}},
Abstract = {{Data mining has emerged as a significant technology for discovering
   knowledge in vast quantities of data. It is however accompanied by the
   danger that private information will be revealed in the processing of
   data mining. Hence, privacy-preserving data mining has received a
   growing amount of attention in recent years. In this paper, we propose a
   method to extract global fuzzy rules from distributed data in a
   privacy-preserving manner. This method transfers only values necessary
   for the extraction process without collecting any data at one place and
   can obtain the global fuzzy rules at all places. Each data set can be
   characterized by comparing the local fuzzy rules for each distributed
   data to the global ones for all data. We illustrate a result for
   experiments using Wine data from UCI Machine Learning Repository.}},
ISSN = {{1098-7584}},
ISBN = {{978-1-4799-0020-6}},
Unique-ID = {{ISI:000335342800142}},
}

@inproceedings{ ISI:000332881600082,
Author = {Wang, Boyang and Li, Ming and Chow, Sherman S. M. and Li, Hui},
Book-Group-Author = {{IEEE}},
Title = {{Computing Encrypted Cloud Data Efficiently under Multiple Keys}},
Booktitle = {{2013 IEEE CONFERENCE ON COMMUNICATIONS AND NETWORK SECURITY (CNS)}},
Series = {{IEEE Conference on Communications and Network Security}},
Year = {{2013}},
Pages = {{504-513}},
Note = {{1st IEEE International Conference on Communications and Network Security
   (CNS), Washington, DC, OCT 14-16, 2013}},
Organization = {{IEEE; VeriSign Corp; Virginia Techs Hume Ctr}},
Abstract = {{The emergence of cloud computing brings users abundant opportunities to
   utilize the power of cloud to perform computation on data contributed by
   multiple users. These cloud data should be encrypted under multiple keys
   due to privacy concerns. However, existing secure computation techniques
   are either limited to single key or still far from practical. In this
   paper, we design two efficient schemes for secure outsourced computation
   over cloud data encrypted under multiple keys. Our schemes employ two
   non-colluding cloud servers to jointly compute polynomial functions over
   multiple users' encrypted cloud data without learning the inputs,
   intermediate or final results, and require only minimal interactions
   between the two cloud servers but not the users. We demonstrate our
   schemes' efficiency experimentally via applications in machine learning.
   Our schemes are also applicable to privacy-preserving data aggregation
   such as in smart metering.}},
ISSN = {{2474-025X}},
ISBN = {{978-1-4799-0895-0}},
ORCID-Numbers = {{Chow, Sherman S. M./0000-0001-7306-453X}},
Unique-ID = {{ISI:000332881600082}},
}

@inproceedings{ ISI:000330831300192,
Author = {Ishibuchi, Hisao and Yamane, Masakazu and Nojima, Yusuke},
Editor = {{Hu, X and Lin, TY and Raghavan, V and Wah, B and BaezaYates, R and Fox, G and Shahabi, C and Smith, M and Yang, Q and Ghani, R and Fan, W and Lempel, R and Nambiar, R}},
Title = {{Learning from Multiple Data Sets with Different Missing Attributes and
   Privacy Policies: Parallel Distributed Fuzzy Genetics-Based Machine
   Learning Approach}},
Booktitle = {{2013 IEEE INTERNATIONAL CONFERENCE ON BIG DATA}},
Series = {{IEEE International Conference on Big Data}},
Year = {{2013}},
Note = {{IEEE International Conference on Big Data (Big Data), Santa Clara, CA,
   OCT 06-09, 2013}},
Organization = {{IEEE; IEEE Comp Soc; CCF; Yahoo Labs; CISCO}},
Abstract = {{This paper discusses parallel distributed genetics-based machine
   learning (GBML) of fuzzy rule-based classifiers from multiple data sets.
   We assume that each data set has a similar but different set of
   attributes. In other words, each data set has different missing
   attributes. Our task is the design of a fuzzy rule-based classifier from
   those data sets. In this paper, we first show that fuzzy rules can
   handle missing attributes easily. Next we explain how parallel
   distributed fuzzy GBML can handle multiple data sets with different
   missing attributes. Then we examine the accuracy of obtained fuzzy
   rule-based classifiers from various settings of available training data
   such as a single data set with no missing attribute and multiple data
   sets with many missing attributes. Experimental results show that the
   use of multiple data sets often increases the accuracy of obtained fuzzy
   rule-based classifiers even when they have missing attributes. We also
   discuss the learning from a data set under a severe privacy preserving
   policy where only the error rate of each candidate classifier is
   available. It is assumed that no information about each individual
   pattern is available. This means that we cannot use any information on
   the class label or the attribute values of each pattern. We explain how
   such a black-box data set can be utilized for classifier design.}},
ISSN = {{2639-1589}},
ISBN = {{978-1-4799-1292-6; 978-1-4799-1293-3}},
ResearcherID-Numbers = {{Ishibuchi, Hisao/B-3599-2009}},
ORCID-Numbers = {{Ishibuchi, Hisao/0000-0001-9186-6472}},
Unique-ID = {{ISI:000330831300192}},
}

@inproceedings{ ISI:000326249300022,
Author = {Nikolaenko, Valeria and Weinsberg, Udi and Ioannidis, Stratis and Joye,
   Marc and Boneh, Dan and Taft, Nina},
Book-Group-Author = {{IEEE}},
Title = {{Privacy-Preserving Ridge Regression on Hundreds of Millions of Records}},
Booktitle = {{2013 IEEE SYMPOSIUM ON SECURITY AND PRIVACY (SP)}},
Series = {{IEEE Symposium on Security and Privacy}},
Year = {{2013}},
Pages = {{334-348}},
Note = {{34th IEEE Symposium on Security and Privacy (SP), San Francisco, CA, MAY
   19-22, 2013}},
Organization = {{IEEE; IEEE Comp Soc; Cisco; Google; Symantec Res Labs; Carnegie Mellon,
   SEI, CERT; Microsoft Res; Natl Security Agcy; Yahoo; Facebook;
   Intelligent Automat Inc; MIT Lincoln Lab; Technicolor; VMware}},
Abstract = {{Ridge regression is an algorithm that takes as input a large number of
   data points and finds the best-fit linear curve through these points.
   The algorithm is a building block for many machine-learning operations.
   We present a system for privacy-preserving ridge regression. The system
   outputs the best-fit curve in the clear, but exposes no other
   information about the input data. Our approach combines both homomorphic
   encryption and Yao garbled circuits, where each is used in a different
   part of the algorithm to obtain the best performance. We implement the
   complete system and experiment with it on real data-sets, and show that
   it significantly outperforms pure implementations based only on
   homomorphic encryption or Yao circuits.}},
DOI = {{10.1109/SP.2013.30}},
ISSN = {{1081-6011}},
ISBN = {{978-0-7695-4977-4; 978-1-4673-6166-8}},
Unique-ID = {{ISI:000326249300022}},
}

@inproceedings{ ISI:000324398900071,
Author = {Kikuchi, Hiroaki and Ito, Kouichi and Ushida, Mebae and Tsuda, Hiroshi
   and Yamaoka, Yuji},
Editor = {{Barolli, L and Xhafa, F and Takizawa, M and Enokido, T and Hsu, HH}},
Title = {{Privacy-Preserving Distributed Decision Tree Learning with Boolean Class
   Attributes}},
Booktitle = {{2013 IEEE 27TH INTERNATIONAL CONFERENCE ON ADVANCED INFORMATION
   NETWORKING AND APPLICATIONS (AINA)}},
Series = {{International Conference on Advanced Information Networking and
   Applications}},
Year = {{2013}},
Pages = {{538-545}},
Note = {{IEEE 27th International Conference on Advanced Information Networking
   and Applications (IEEE AINA), Barcelona, SPAIN, MAR 25-28, 2013}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Comp Soc Tech Comm Distributed Proc (TCDP);
   Tech Univ Catalonia; Fukuoka Inst Technol (FIT); IEEE Tech Comm
   Distributed Proc (TCDP)}},
Abstract = {{This paper studies a privacy-preserving decision tree learning protocol
   (PPDT) for vertically partitioned datasets. In the vertically
   partitioned datasets, a single class (target) attribute are shared by
   both parities or carefully treated by either party in the existing
   studies. The proposed scheme allows both parties to have independent
   class attributes in secure way and to combine multiple class attributes
   in arbitrary boolean function, which gives parties a flexibility in
   data-mining. Our proposed PPDT protocol reduces the CPU intensive
   computation of logarithm by approximating with the piecewise linear
   function defined by light-weight fundamental operations of addition and
   constant-multiplication so that information gain for attribute can be
   evaluated in the secure function evaluation scheme. Using the UCI
   Machine Learning dataset and the synthesized dataset, the proposed
   protocol is evaluated in terms of the accuracy and the size of tree.}},
DOI = {{10.1109/AINA.2013.140}},
ISSN = {{1550-445X}},
ISBN = {{978-1-4673-5550-6}},
Unique-ID = {{ISI:000324398900071}},
}

@inproceedings{ ISI:000350802400226,
Author = {Hardt, Moritz},
Book-Group-Author = {{IEEE}},
Title = {{Robust Subspace Iteration and Privacy-Preserving Spectral Analysis}},
Booktitle = {{2013 51ST ANNUAL ALLERTON CONFERENCE ON COMMUNICATION, CONTROL, AND
   COMPUTING (ALLERTON)}},
Series = {{Annual Allerton Conference on Communication Control and Computing}},
Year = {{2013}},
Pages = {{1624-1626}},
Note = {{51st IEEE Annual Allerton Conference on Communication, Control, and
   Computing, Monticello, IL, OCT 02-04, 2013}},
Organization = {{Coordinated Sci Lab; Univ Illinois, Dept Elect \& Comp Engn; IEEE; IEEE
   Informat Theory Soc; IEEE Control Syst Soc}},
Abstract = {{We discuss a new robust convergence analysis of the well-known subspace
   iteration algorithm for computing the dominant singular vectors of a
   matrix, also known as simultaneous iteration or power method. The result
   characterizes the convergence behavior of the algorithm when a large
   amount noise is introduced after each matrix-vector multiplication.
   While interesting in its own right, the main motivation comes from the
   problem of privacy-preserving spectral analysis where noise is added in
   order to achieve the privacy guarantee known as differential privacy.
   This result leads to nearly tight worst-case bounds for the problem of
   computing a differentially private low-rank approximation in the
   spectral norm. Our results extend to privacy-preserving principal
   component analysis. We obtain improvements for several variants of
   differential privacy that have been considered. The running time of our
   algorithm is nearly linear in the input sparsity leading to strong
   improvements in running time over previous work.
   Complementing our worst-case bounds, we show that the error dependence
   of our algorithm on the matrix dimension can be replaced by a tight
   dependence on the coherence of the matrix. This parameter is always
   bounded by the matrix dimension but often much smaller. Indeed, the
   assumption of low coherence is essential in several machine learning and
   signal processing applications.}},
ISSN = {{2474-0195}},
ISBN = {{978-1-4799-3409-6}},
Unique-ID = {{ISI:000350802400226}},
}

@article{ ISI:000309083300003,
Author = {Samet, Saeed and Miri, Ali},
Title = {{Privacy-preserving back-propagation and extreme learning machine
   algorithms}},
Journal = {{DATA \& KNOWLEDGE ENGINEERING}},
Year = {{2012}},
Volume = {{79-80}},
Pages = {{40-61}},
Month = {{SEP-OCT}},
Abstract = {{Neural network systems are highly capable of deriving knowledge from
   complex data, and they are used to extract patterns and trends which are
   otherwise hidden in many applications. Preserving the privacy of
   sensitive data and individuals' information is a major challenge in many
   of these applications. One of the most popular algorithms in neural
   network learning systems is the back-propagation (BP) algorithm, which
   is designed for single-layer and multi-layer models and can be applied
   to continuous data and differentiable activation functions. Another
   recently introduced learning technique is the extreme learning machine
   (ELM) algorithm. Although it works only on single-layer models. ELM can
   out-perform the BP algorithm by reducing the communication required
   between parties in the learning phase. In this paper, we present new
   privacy-preserving protocols for both the BP and ELM algorithms when
   data is horizontally and vertically partitioned among several parties.
   These new protocols, which preserve the privacy of both the input data
   and the constructed learning model, can be applied to online incoming
   records and/or batch learning. Furthermore, the final model is securely
   shared among all parties, who can use it jointly to predict the
   corresponding output for their target data. (c) 2012 Elsevier B.V. All
   rights reserved.}},
DOI = {{10.1016/j.datak.2012.06.001}},
ISSN = {{0169-023X}},
EISSN = {{1872-6933}},
ORCID-Numbers = {{Samet, Saeed/0000-0002-5116-5484}},
Unique-ID = {{ISI:000309083300003}},
}

@article{ ISI:000298381000013,
Author = {Fong, Pui K. and Weber-Jahnke, Jens H.},
Title = {{Privacy Preserving Decision Tree Learning Using Unrealized Data Sets}},
Journal = {{IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING}},
Year = {{2012}},
Volume = {{24}},
Number = {{2}},
Pages = {{353-364}},
Month = {{FEB}},
Abstract = {{Privacy preservation is important for machine learning and data mining,
   but measures designed to protect private information often result in a
   trade-off: reduced utility of the training samples. This paper
   introduces a privacy preserving approach that can be applied to decision
   tree learning, without concomitant loss of accuracy. It describes an
   approach to the preservation of the privacy of collected data samples in
   cases where information from the sample database has been partially
   lost. This approach converts the original sample data sets into a group
   of unreal data sets, from which the original samples cannot be
   reconstructed without the entire group of unreal data sets. Meanwhile,
   an accurate decision tree can be built directly from those unreal data
   sets. This novel approach can be applied directly to the data storage as
   soon as the first sample is collected. The approach is compatible with
   other privacy preserving approaches, such as cryptography, for extra
   protection.}},
DOI = {{10.1109/TKDE.2010.226}},
ISSN = {{1041-4347}},
Unique-ID = {{ISI:000298381000013}},
}

@inproceedings{ ISI:000320608600160,
Author = {Prakash, M. and Singaravel, G.},
Book-Group-Author = {{IEEE}},
Title = {{A New Model for Privacy Preserving Sensitive Data Mining}},
Booktitle = {{2012 THIRD INTERNATIONAL CONFERENCE ON COMPUTING COMMUNICATION \&
   NETWORKING TECHNOLOGIES (ICCCNT)}},
Series = {{International Conference on Computing Communication and Network
   Technologies}},
Year = {{2012}},
Note = {{3rd International Conference on Computing Communication and Networking
   Technologies (ICCCNT), Coimbatore, INDIA, JUL 26-28, 2012}},
Abstract = {{Data Mining and Knowledge Discovery is an indispensable technology for
   business and researches in many fields such as statistics, machine
   learning, pattern recognition, databases and high performance computing.
   In which Privacy Preserving Data Mining has the potential to increase
   the reach and benefits of data mining technology. This allows publishing
   a micro data without disclosing private information. Publishing data
   about individuals without revealing sensitive information about them is
   an important problem. k-anonymity and I-Diversity has been proposed as a
   mechanism for protecting privacy in microdata publishing. But both the
   mechanisms are insufficient to protect the privacy issues like
   Homogeneity attack, Skewness Attack, Similarity attack and Background
   Knowledge Attack. A new privacy measure called ``(n, f)-proximity{''} is
   proposed which is more flexible model. Here first introduction about
   data mining is presented, and then research challenges are given.
   Followed by privacy preservation measures and problems with k-anonymity
   and l-Diversity are discussed. The rest of the paper is organised as (n,
   f)-proximity model, experimental results and analysis followed by
   conclusion.}},
ISSN = {{2162-7665}},
ResearcherID-Numbers = {{M, Prakash/G-6742-2014}},
ORCID-Numbers = {{M, Prakash/0000-0001-8008-4424}},
Unique-ID = {{ISI:000320608600160}},
}

@inproceedings{ ISI:000392950200213,
Author = {Shin, Moonshik and Yoo, Sunyong and Lee, Kwang H. and Lee, Doheon},
Book-Group-Author = {{IEEE}},
Title = {{Electronic Medical Records Privacy Preservation through k-Anonymity
   Clustering Method}},
Booktitle = {{6TH INTERNATIONAL CONFERENCE ON SOFT COMPUTING AND INTELLIGENT SYSTEMS,
   AND THE 13TH INTERNATIONAL SYMPOSIUM ON ADVANCED INTELLIGENT SYSTEMS}},
Series = {{Joint International Conference on Soft Computing and Intelligent Systems
   SCIS and International Symposium on Advanced Intelligent Systems ISIS}},
Year = {{2012}},
Pages = {{1119-1124}},
Note = {{6th International Conference on Soft Computing and Intelligent Systems
   (SCIS) / 13th International Symposium on Advanced Intelligence Systems
   (ISIS), Kobe, JAPAN, NOV 20-24, 2012}},
Abstract = {{Electronic Medical Records (EMRs) enable the sharing of patient medical
   data whenever it is needed and also are used as a tool for building new
   medical technology and patient recommendation systems. Since EMRs
   include patients' private data, access is restricted to researchers.
   Thus, an anonymizing technique is necessary that keeps patients' private
   data safe while not damaging useful medical information. k-member
   clustering anonymization approaches k-anonymization as a clustering
   issue. The objective of the k-member clustering problem is to gather
   records that will minimize the data distortion during data
   generalization. Most of the previous clustering techniques include
   random seed selection. However, randomly selecting a cluster seed will
   provide inconsistent performance. The authors propose a k-member cluster
   seed selection algorithm (KMCSSA) that is distinct from the previous
   clustering methods. Instead of randomly selecting a cluster seed, the
   proposed method selects the seed based on the closeness centrality to
   provide consistent information loss (IL) and to reduce the information
   distortion. An adult database from University of California Irvine
   Machine Learning Repository was used for the experiment. By comparing
   the proposed method with two previous methods, the experimental results
   shows that KMCSSA provides superior performance with respect to
   information loss. The authors provide a privacy protection algorithm
   that derives consistent information loss and reduces the overall
   information distortion.}},
ISSN = {{2377-6870}},
ISBN = {{978-1-4673-2743-5; 978-1-4673-2742-8}},
ORCID-Numbers = {{Yoo, Sunyong/0000-0003-0925-1853}},
Unique-ID = {{ISI:000392950200213}},
}

@inproceedings{ ISI:000314992600026,
Author = {Mivule, Kato and Turner, Claude and Ji, Soo-Yeon},
Editor = {{Dagli, CH}},
Title = {{Towards A Differential Privacy and Utility Preserving Machine Learning
   Classifier}},
Booktitle = {{COMPLEX ADAPTIVE SYSTEMS 2012}},
Series = {{Procedia Computer Science}},
Year = {{2012}},
Volume = {{12}},
Pages = {{176-181}},
Note = {{Conference on Complex Adaptive Systems, Washington, DC, NOV 14-16, 2012}},
Organization = {{Missouri Univ Sci \& Technol; Lockheed Martin; Mocana; Tata Consultancy
   Serv; GAK3; Drexel Univ Online; Hark.com}},
Abstract = {{Many organizations transact in large amounts of data often containing
   personal identifiable information (PII) and various confidential data.
   Such organizations are bound by state, federal, and international laws
   to ensure that the confidentiality of both individuals and sensitive
   data is not compromised. However, during the privacy preserving process,
   the utility of such datasets diminishes even while confidentiality is
   achieved--a problem that has been defined as NP-Hard. In this paper, we
   investigate a differential privacy machine learning ensemble classifier
   approach that seeks to preserve data privacy while maintaining an
   acceptable level of utility. The first step of the methodology applies a
   strong data privacy granting technique on a dataset using differential
   privacy. The resulting perturbed data is then passed through a machine
   learning ensemble classifier, which aims to reduce the classification
   error, or, equivalently, to increase utility. Then, the association
   between increasing the number of weak decision tree learners and data
   utility, which informs us as to whether the ensemble machine learner
   would classify more correctly is examined. As results, we found that a
   combined adjustment of the privacy granting noise parameters and an
   increase in the number of weak learners in the ensemble machine might
   lead to a lower classification error.}},
DOI = {{10.1016/j.procs.2012.09.050}},
ISSN = {{1877-0509}},
Unique-ID = {{ISI:000314992600026}},
}

@article{ ISI:000289635000010,
Author = {Chaudhuri, Kamalika and Monteleoni, Claire and Sarwate, Anand D.},
Title = {{Differentially Private Empirical Risk Minimization}},
Journal = {{JOURNAL OF MACHINE LEARNING RESEARCH}},
Year = {{2011}},
Volume = {{12}},
Pages = {{1069-1109}},
Month = {{MAR}},
Abstract = {{Privacy-preserving machine learning algorithms are crucial for the
   increasingly common setting in which personal data, such as medical or
   financial records, are analyzed. We provide general techniques to
   produce privacy-preserving approximations of classifiers learned via
   (regularized) empirical risk minimization (ERM). These algorithms are
   private under the epsilon-differential privacy definition due to Dwork
   et al. (2006). First we apply the output perturbation ideas of Dwork et
   al. (2006), to ERM classification. Then we propose a new method,
   objective perturbation, for privacy-preserving machine learning
   algorithm design. This method entails perturbing the objective function
   before optimizing over classifiers. If the loss and regularizer satisfy
   certain convexity and differentiability criteria, we prove theoretical
   results showing that our algorithms preserve privacy, and provide
   generalization bounds for linear and nonlinear kernels. We further
   present a privacy-preserving technique for tuning the parameters in
   general machine learning algorithms, thereby providing end-to-end
   privacy guarantees for the training process. We apply these results to
   produce privacy-preserving analogues of regularized logistic regression
   and support vector machines. We obtain encouraging results from
   evaluating their performance on real demographic and benchmark data
   sets. Our results show that both theoretically and empirically,
   objective perturbation is superior to the previous state-of-the-art,
   output perturbation, in managing the inherent tradeoff between privacy
   and learning performance.}},
ISSN = {{1532-4435}},
ORCID-Numbers = {{Sarwate, Anand/0000-0001-6123-5282}},
Unique-ID = {{ISI:000289635000010}},
}

@article{ ISI:000281791800002,
Author = {Das, Kamalika and Bhaduri, Kanishka and Kargupta, Hillol},
Title = {{A local asynchronous distributed privacy preserving feature selection
   algorithm for large peer-to-peer networks}},
Journal = {{KNOWLEDGE AND INFORMATION SYSTEMS}},
Year = {{2010}},
Volume = {{24}},
Number = {{3}},
Pages = {{341-367}},
Month = {{SEP}},
Abstract = {{In this paper we develop a local distributed privacy preserving
   algorithm for feature selection in a large peer-to-peer environment.
   Feature selection is often used in machine learning for data compaction
   and efficient learning by eliminating the curse of dimensionality. There
   exist many solutions for feature selection when the data are located at
   a central location. However, it becomes extremely challenging to perform
   the same when the data are distributed across a large number of peers or
   machines. Centralizing the entire dataset or portions of it can be very
   costly and impractical because of the large number of data sources, the
   asynchronous nature of the peer-to-peer networks, dynamic nature of the
   data/network, and privacy concerns. The solution proposed in this paper
   allows us to perform feature selection in an asynchronous fashion with a
   low communication overhead where each peer can specify its own privacy
   constraints. The algorithm works based on local interactions among
   participating nodes. We present results on real-world dataset in order
   to test the performance of the proposed algorithm.}},
DOI = {{10.1007/s10115-009-0274-3}},
ISSN = {{0219-1377}},
Unique-ID = {{ISI:000281791800002}},
}

@article{ ISI:000276801300011,
Author = {Han, Shuguo and Ng, Wee Keong and Wan, Li and Lee, Vincent C. S.},
Title = {{Privacy-Preserving Gradient-Descent Methods}},
Journal = {{IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING}},
Year = {{2010}},
Volume = {{22}},
Number = {{6}},
Pages = {{884-899}},
Month = {{JUN}},
Abstract = {{Gradient descent is a widely used paradigm for solving many optimization
   problems. Gradient descent aims to minimize a target function in order
   to reach a local minimum. In machine learning or data mining, this
   function corresponds to a decision model that is to be discovered. In
   this paper, we propose a preliminary formulation of gradient descent
   with data privacy preservation. We present two approaches-stochastic
   approach and least square approach-under different assumptions. Four
   protocols are proposed for the two approaches incorporating various
   secure building blocks for both horizontally and vertically partitioned
   data. We conduct experiments to evaluate the scalability of the proposed
   secure building blocks and the accuracy and efficiency of the protocols
   for four different scenarios. The excremental results show that the
   proposed secure building blocks are reasonably scalable and the proposed
   protocols allow us to determine a better secure protocol for the
   applications for each scenario.}},
DOI = {{10.1109/TKDE.2009.153}},
ISSN = {{1041-4347}},
ResearcherID-Numbers = {{Ng, Wee Keong/A-3724-2011}},
ORCID-Numbers = {{Ng, Wee Keong/0000-0001-7106-2768}},
Unique-ID = {{ISI:000276801300011}},
}

@article{ ISI:000278243500003,
Author = {Soysal, Murat and Schmidt, Ece Guran},
Title = {{Machine learning algorithms for accurate flow-based network traffic
   classification: Evaluation and comparison}},
Journal = {{PERFORMANCE EVALUATION}},
Year = {{2010}},
Volume = {{67}},
Number = {{6}},
Pages = {{451-467}},
Month = {{JUN}},
Abstract = {{The task of network management and monitoring relies on an accurate
   characterization of network traffic generated by different applications
   and network protocols. We employ three supervised machine learning (ML)
   algorithms, Bayesian Networks, Decision Trees and Multilayer Perceptrons
   for the flow-based classification of six different types of Internet
   traffic including peer-to-peer (P2P) and content delivery (Akamai)
   traffic. The dependency of the traffic classification performance on the
   amount and composition of training data is investigated followed by
   experiments that show that ML algorithms such as Bayesian Networks and
   Decision Trees are suitable for Internet traffic flow classification at
   a high speed, and prove to be robust with respect to applications that
   dynamically change their source ports. Finally, the importance of
   correctly classified training instances is highlighted by an experiment
   that is conducted with wrongly labeled training data. (C) 2010 Elsevier
   B.V. All rights reserved.}},
DOI = {{10.1016/j.peva.2010.01.001}},
ISSN = {{0166-5316}},
EISSN = {{1872-745X}},
ResearcherID-Numbers = {{Soysal, Murat/H-5564-2011}},
Unique-ID = {{ISI:000278243500003}},
}

@inproceedings{ ISI:000287889800056,
Author = {Guijarro-Berdinas, Bertha and Fernandez-Lorenzo, Santiago and
   Sanchez-Marono, Noelia and Fontenla-Romero, Oscar},
Editor = {{Diamantaras, K and Duch, W and Iliadis, LS}},
Title = {{A Privacy-Preserving Distributed and Incremental Learning Method for
   Intrusion Detection}},
Booktitle = {{ARTIFICIAL NEURAL NETWORKS-ICANN 2010, PT I}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2010}},
Volume = {{6352}},
Number = {{I}},
Pages = {{415-421}},
Note = {{20th International Conference on Artificial Neural Networks,
   Thessaloniki, GREECE, SEP 15-18, 2010}},
Organization = {{European Neural Network Soc; Aristotle Univ Thessaloniki; Univ
   Macedonia; Technol Educ Inst Thess; Hellenic Int Univ; Democritus Univ
   Thrace; Alexander TEI Thessaloniki}},
Abstract = {{Computer systems are facing an increased number of security threats,
   specially regarding Intrusion detection (ID). From the point of view of
   Machine learning, ID presents many of the new cutting-edge challenges:
   tackle with massive databases, distributed learning and
   privacy-preserving classification. In this work, a new approach for ID
   capable of dealing with these problems is presented using the KDDCup99
   dataset as a benchmark, where data have to be classified to detect an
   attack. The method uses Artificial Neural Networks with incremental
   learning capability, Genetic Algorithms and a feature selection method
   to determine relevant inputs. As supported by the experimental results,
   this method is able to rapidly obtain an accurate model based on the
   information of distributed databases without exchanging any compromised
   data, obtaining similar results compared with other authors but offering
   features that make the proposed approach more suitable for an ID
   application.}},
ISSN = {{0302-9743}},
ISBN = {{978-3-642-15818-6}},
ResearcherID-Numbers = {{Sanchez-Marono, Noelia/H-1623-2015
   Guijarro-Berdinas, Bertha/D-5446-2011
   Fontenla-Romero, Oscar/A-1142-2015}},
ORCID-Numbers = {{Sanchez-Marono, Noelia/0000-0003-4025-1405
   Guijarro-Berdinas, Bertha/0000-0001-8901-5441
   Fontenla-Romero, Oscar/0000-0003-4203-8720}},
Unique-ID = {{ISI:000287889800056}},
}

@inproceedings{ ISI:000283109500002,
Author = {Peddinti, Sai Teja and Saxena, Nitesh},
Editor = {{Atallah, MJ and Hopper, NJ}},
Title = {{On the Privacy of Web Search Based on Query Obfuscation: A Case Study of
   TrackMeNot}},
Booktitle = {{PRIVACY ENHANCING TECHNOLOGIES}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2010}},
Volume = {{6205}},
Pages = {{19-37}},
Note = {{10th International Symposium on Privacy Enhancing Technologies, Berlin,
   GERMANY, JUL 21-23, 2010}},
Organization = {{Microsoft Corp}},
Abstract = {{Web Search is one of the most rapidly growing applications on the
   internet today. However, the current practice followed by most search
   engines - of logging and analyzing users' queries - raises serious
   privacy concerns. One viable solution to search privacy is query
   obfuscation, whereby a client-side software attempts to mask real user
   queries via injection of certain noisy queries. In contrast to other
   privacy-preserving search mechanisms, query obfuscation does riot
   require server-side modifications or a third party infrastructure, thus
   allowing for ready deployment at the discretion of privacy-conscious
   users. In this paper, our higher level goal is to analyze whether query
   obfuscation can preserve users' privacy in practice against an
   adversarial search engine. We focus on TrackMeNot (TMN) {[}10,20], a
   popular search privacy tool based on the principle of query obfuscation.
   We demonstrate that a search engine, equipped with only a short-term
   history of a user's search queries, can break the privacy guarantees of
   TMN by only utilizing off-the-shelf machine learning classifiers.}},
ISSN = {{0302-9743}},
ISBN = {{978-3-642-14526-1}},
Unique-ID = {{ISI:000283109500002}},
}

@article{ ISI:000268546900002,
Author = {Luo, Hangzai and Fan, Jianping and Lin, Xiaodong and Zhou, Aoying and
   Bertino, Elisa},
Title = {{A distributed approach to enabling privacy-preserving model-based
   classifier training}},
Journal = {{KNOWLEDGE AND INFORMATION SYSTEMS}},
Year = {{2009}},
Volume = {{20}},
Number = {{2}},
Pages = {{157-185}},
Month = {{AUG}},
Abstract = {{This paper proposes a novel approach for privacy-preserving distributed
   model-based classifier training. Our approach is an important step
   towards supporting customizable privacy modeling and protection. It
   consists of three major steps. First, each data site independently
   learns a weak concept model (i.e., local classifier) for a given data
   pattern or concept by using its own training samples. An adaptive EM
   algorithm is proposed to select the model structure and estimate the
   model parameters simultaneously. The second step deals with combined
   classifier training by integrating the weak concept models that are
   shared from multiple data sites. To reduce the data transmission costs
   and the potential privacy breaches, only the weak concept models are
   sent to the central site and synthetic samples are directly generated
   from these shared weak concept models at the central site. Both the
   shared weak concept models and the synthetic samples are then
   incorporated to learn a reliable and complete global concept model. A
   computational approach is developed to automatically achieve a good
   trade off between the privacy disclosure risk, the sharing benefit and
   the data utility. The third step deals with validating the combined
   classifier by distributing the global concept model to all these data
   sites in the collaboration network while at the same time limiting the
   potential privacy breaches. Our approach has been validated through
   extensive experiments carried out on four UCI machine learning data sets
   and two image data sets.}},
DOI = {{10.1007/s10115-008-0167-x}},
ISSN = {{0219-1377}},
EISSN = {{0219-3116}},
ResearcherID-Numbers = {{Zhou, Xiaofang/C-6169-2013}},
ORCID-Numbers = {{Zhou, Xiaofang/0000-0001-6343-1455}},
Unique-ID = {{ISI:000268546900002}},
}

@inproceedings{ ISI:000265740400040,
Author = {Cui, Yingjie and Wong, W. K. and Cheung, David W.},
Editor = {{Zhou, X and Yokota, H and Deng, K and Liu, Q}},
Title = {{Privacy-Preserving Clustering with High Accuracy and Low Time Complexity}},
Booktitle = {{DATABASE SYSTEMS FOR ADVANCED APPLICATIONS, PROCEEDINGS}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2009}},
Volume = {{5463}},
Pages = {{456-470}},
Note = {{14th International Conference on Database Systems for Advanced
   Applications, Brisbane, AUSTRALIA, APR 21-23, 2009}},
Organization = {{Australian Res Council, Res Network Enterprise Informat Infrastruct;
   Australian Res Council, Res Network Intelligent Sensors, Sensor Networks
   \& Informat Proc; Australia Informat \& Commun Technol Ctr Excellence;
   Res Network Secure Australia; Univ Melbourne Australia; Univ New S
   Wales; Univ Sydney; Univ Queensland}},
Abstract = {{This paper proposes an efficient solution with high accuracy to the
   problem of privacy-preserving clustering. This problem has been studied
   mainly using two approaches: data perturbation and secure multiparty
   computation. In our research, we focus on the data perturbation
   approach, and propose an algorithm of linear time complexity based on
   1-d clustering to perturb the data. Performance study on real datasets
   from the UCI machine learning repository shows that our approach reaches
   better accuracy and hence lowers the distortion of clustering result
   than previous approaches.}},
ISSN = {{0302-9743}},
ISBN = {{978-3-642-00886-3}},
Unique-ID = {{ISI:000265740400040}},
}

@inproceedings{ ISI:000271116900027,
Author = {Guijarro-Berdinas, Bertha and Martinez-Rego, David and
   Fernandez-Lorenzo, Santiago},
Editor = {{Omatu, S and Rocha, MP and Bravo, J and Fernandez, F and Corchado, E and Bustillo, A and Corchado, JM}},
Title = {{Privacy-Preserving Distributed Learning Based on Genetic Algorithms and
   Artificial Neural Networks}},
Booktitle = {{DISTRIBUTED COMPUTING, ARTIFICIAL INTELLIGENCE, BIOINFORMATICS, SOFT
   COMPUTING, AND AMBIENT ASSISTED LIVING, PT II, PROCEEDINGS}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2009}},
Volume = {{5518}},
Pages = {{195-202}},
Note = {{10th International Work-Conference on Artificial Neural Networks (IWANN
   2009), Salamanca, SPAIN, JUN 10-12, 2009}},
Abstract = {{In recent years, Machine Learning (ML) has witnessed a great increase of
   storage capacity of computer systems and ail enormous growth of
   available information to work with thanks to the WWW. This has raised an
   opportunity for new real life applications of ML methods and also new
   cutting-edge ML challenges like: tackle with massive databases,
   Distributed Learning and Privacy-preserving Classification. In this
   paper a new method capable of dealing with this three problems is
   presented. The method is based on Artificial Neural Networks with
   incremental learning and Genetic Algorithms. As Supported by the
   experimental results, this method is able to fastly obtain an accurate
   model based on the information of distributed databases without
   exchanging any data during the training process, without degrading its
   classification accuracy when compared with other non-distributed
   classical ML methods. This makes the proposed method very efficient and
   adequate for Privacy-Preserving Learning applications.}},
ISSN = {{0302-9743}},
ISBN = {{978-3-642-02480-1}},
ResearcherID-Numbers = {{Guijarro-Berdinas, Bertha/D-5446-2011
   Martinez-Rego, David/I-2710-2015}},
ORCID-Numbers = {{Guijarro-Berdinas, Bertha/0000-0001-8901-5441
   }},
Unique-ID = {{ISI:000271116900027}},
}

@article{ ISI:000260311400005,
Author = {Siersdorfer, Stefan and Sizov, Sergej},
Title = {{Meta Methods for Model Sharing in Personal Information Systems}},
Journal = {{ACM TRANSACTIONS ON INFORMATION SYSTEMS}},
Year = {{2008}},
Volume = {{26}},
Number = {{4}},
Month = {{SEP}},
Note = {{2nd International Workshop on Personal Information Management, Seattle,
   WA, AUG 10-11, 2006}},
Abstract = {{This article introduces a methodology for automatically organizing
   document collections into thematic categories for Personal Information
   Management (PIM) through collaborative sharing of machine learning
   models in an efficient and privacy-preserving way. Our objective is to
   combine multiple independently learned models from several users to
   construct an advanced ensemble-based decision model by taking the
   knowledge of multiple users into account in a decentralized manner, for
   example, in a peer-to-peer overlay network. High accuracy of the
   corresponding supervised (classification) and unsupervised (clustering)
   methods is achieved by restrictively leaving out uncertain documents
   rather than assigning them to inappropriate topics or clusters with low
   confidence. We introduce a formal probabilistic model for the resulting
   ensemble based meta methods and explain how it can be used for
   constructing estimators and for goal-oriented tuning. Comprehensive
   evaluation results on different reference data sets illustrate the
   viability of our approach.}},
DOI = {{10.1145/1402256.1402261}},
Article-Number = {{22}},
ISSN = {{1046-8188}},
EISSN = {{1558-2868}},
Unique-ID = {{ISI:000260311400005}},
}

@article{ ISI:000252674400003,
Author = {Vaidya, Jaideep and Yu, Hwanjo and Jiang, Xiaoqian},
Title = {{Privacy-preserving SVM classification}},
Journal = {{KNOWLEDGE AND INFORMATION SYSTEMS}},
Year = {{2008}},
Volume = {{14}},
Number = {{2}},
Pages = {{161-178}},
Month = {{FEB}},
Note = {{10th Pacific-Asia Conference on Knowledge Discovery and Data Mining,
   Singapore, SINGAPORE, APR   09, 2005-APR 12, 2006}},
Organization = {{USAF Off Sci Res; Asian Off Aerosp Res \& Dev; USA ITC, PAC Asian Res
   Off; Informat Dev Author Singapore; Lee Fdn; SAS; SPSS Inc; Embassy
   United States Amer, Singapore}},
Abstract = {{Traditional Data Mining and Knowledge Discovery algorithms assume free
   access to data, either at a centralized location or in federated form.
   Increasingly, privacy and security concerns restrict this access, thus
   derailing data mining projects. What is required is distributed
   knowledge discovery that is sensitive to this problem. The key is to
   obtain valid results, while providing guarantees on the nondisclosure of
   data. Support vector machine classification is one of the most widely
   used classification methodologies in data mining and machine learning.
   It is based on solid theoretical foundations and has wide practical
   application. This paper proposes a privacy-preserving solution for
   support vector machine (SVM) classification, PP-SVM for short. Our
   solution constructs the global SVM classification model from data
   distributed at multiple parties, without disclosing the data of each
   party to others. Solutions are sketched out for data that is vertically,
   horizontally, or even arbitrarily partitioned. We quantify the security
   and efficiency of the proposed method, and highlight future challenges.}},
DOI = {{10.1007/s10115-007-0073-7}},
ISSN = {{0219-1377}},
ResearcherID-Numbers = {{Jiang, Xiaoqian/K-6752-2012
   }},
ORCID-Numbers = {{Jiang, Xiaoqian/0000-0001-9933-2205}},
Unique-ID = {{ISI:000252674400003}},
}

@inproceedings{ ISI:000263194700075,
Author = {Samet, Saeed and Miri, Ali},
Book-Group-Author = {{IEEE}},
Title = {{Privacy-Preserving Protocols for Perceptron Learning Algorithm in Neural
   Networks}},
Booktitle = {{2008 4TH INTERNATIONAL IEEE CONFERENCE INTELLIGENT SYSTEMS, VOLS 1 AND 2}},
Year = {{2008}},
Pages = {{459-464}},
Note = {{4th International IEEE Conference Intelligent Systems, Varna, BULGARIA,
   SEP 06-08, 2008}},
Organization = {{IEEE}},
Abstract = {{Neural networks have become increasingly important in areas such as
   medical diagnosis, bio-informatics, intrusion detection, and homeland
   security. In most of these applications, one major issue is preserving
   privacy of individual's private information and sensitive data. In this
   paper, we propose two secure protocols for perceptron learning algorithm
   when input data is horizontally and vertically partitioned among the
   parties. These protocols can be applied in both linearly separable and
   non-separable datasets, while not only data belonging to each party
   remains private, but the final learning model is also securely shared
   among those parties. Parties then can jointly and securely apply the
   constructed model to predict the output corresponding to their target
   data. Also, these protocols can be used incrementally, i.e. they process
   new coming data, adjusting the previously constructed network.}},
ISBN = {{978-1-4244-1739-1}},
Unique-ID = {{ISI:000263194700075}},
}

@inproceedings{ ISI:000256127100013,
Author = {Han, Shuguo and Ng, Wee Keong},
Editor = {{Washio, T and Suzuki, E and Ting, KM and Inokuchi, A}},
Title = {{Privacy-preserving linear fisher discriminant analysis}},
Booktitle = {{ADVANCES IN KNOWLEDGE DISCOVERY AND DATA MINING, PROCEEDINGS}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2008}},
Volume = {{5012}},
Pages = {{136-147}},
Note = {{12th Pacific-Asia Conference on Knowledge Discovery and Data Mining,
   Osaka, JAPAN, MAY 20-23, 2008}},
Organization = {{Japanese Soc Artificial Intelligence; Osaka Convent \& Tourism Bur;
   Commemorat Org Japan World Exposit 70; Kayamori Fdn Informat Sci
   Advancement; Air Force Off Sci Res; Asian Off Aerosp Res \& Dev; Future
   Syst Corp; Salford Syst; Math Syst}},
Abstract = {{Privacy-preserving data mining enables two or more parties to
   collaboratively perform data mining while preserving the data privacy of
   the participating parties. So far, various data mining and machine
   learning algorithms have been enhanced to incorporate privacy
   preservation. In this paper, we propose privacy-preserving solutions for
   Fisher Discriminant Analysis (FDA) over horizontally and vertically
   partitioned data. FDA is one of the widely used discriminant algorithms
   that seeks to separate different classes as much as possible for
   discriminant analysis or dimension reduction. It has been applied to
   face recognition, speech recognition, and handwriting recognition. The
   secure solutions are designed based on two basic secure building blocks
   that we have proposed-the Secure Matrix Multiplication protocol and the
   Secure Inverse of Matrix Sum protocol-which are in turn based on
   cryptographic techniques. We conducted experiments to evaluate the
   scalability of the proposed secure building blocks and overheads to
   achieve privacy when performing FDA.}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-540-68124-3}},
ResearcherID-Numbers = {{Ng, Wee Keong/A-3724-2011}},
ORCID-Numbers = {{Ng, Wee Keong/0000-0001-7106-2768}},
Unique-ID = {{ISI:000256127100013}},
}

@inproceedings{ ISI:000254511400008,
Author = {Yang, Zhiqiang and Zhong, Sheng and Wright, Rebecca N.},
Editor = {{Bonchi, F and Ferrari, E and Malin, B and Saygin, Y}},
Title = {{Towards privacy-preserving model selection}},
Booktitle = {{PRIVACY, SECURITY, AND TRUST IN KDD}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2008}},
Volume = {{4890}},
Pages = {{138+}},
Note = {{1st International Workshop on Privacy, Security and Trust in KDD, San
   Jose, CA, AUG 12, 2007}},
Organization = {{ACM SIGKDD}},
Abstract = {{Model selection is an important problem in statistics, machine learning,
   and data mining. In this paper, we investigate the problem of enabling
   multiple parties to perform model selection on their distributed data in
   a privacy-preserving fashion without revealing their data to each other.
   We specifically study cross validation, a standard method of model
   selection, in the setting in which two parties hold a vertically
   partitioned database. For a specific kind of vertical partitioning, we
   show how the participants can carry out privacy-preserving cross
   validation in order to select among a number of candidate models without
   revealing their data to each other.}},
ISSN = {{0302-9743}},
ISBN = {{978-3-540-78477-7}},
Unique-ID = {{ISI:000254511400008}},
}

@inproceedings{ ISI:000266628300079,
Author = {Wan, Li and Han, Shuguo and Ng, Wee Keong and Lee, Vincent C. S.},
Editor = {{Berkhin, P and Caruana, R and Wu, X and Gaffney, S}},
Title = {{Privacy-Preservation for Gradient Descent Methods}},
Booktitle = {{KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL
   CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING}},
Year = {{2007}},
Pages = {{775+}},
Note = {{13th International Conference on Knowledge Discovery and Data Mining,
   San Jose, CA, AUG 12-15, 2007}},
Organization = {{ACM SIGKDD; ACM SIGMOD}},
Abstract = {{Gradient descent is a widely used paradigm for solving many optimization
   problems. Stochastic gradient descent performs a series of iterations to
   minimize a target function in order to reach a local minimum. In machine
   learning or data mining, this function corresponds to a decision model
   that is to be discovered. The gradient descent paradigm underlies many
   commonly used techniques in data mining and machine learning, such as
   neural networks, Bayesian networks, genetic algorithms, and simulated
   annealing. To the best of our knowledge, there has not been any work
   that extends the notion of privacy preservation or secure multiparty
   computation to gradient-descent-based techniques. In this paper, we
   propose a preliminary approach to enable privacy preservation in
   gradient descent methods in general and demonstrate its feasibility in
   specific gradient descent methods.}},
ISBN = {{978-1-59593-609-7}},
ResearcherID-Numbers = {{Ng, Wee Keong/A-3724-2011}},
ORCID-Numbers = {{Ng, Wee Keong/0000-0001-7106-2768}},
Unique-ID = {{ISI:000266628300079}},
}

@inproceedings{ ISI:000250830700053,
Author = {Samet, Saeed and Miri, Ali and Orozco-Barbosa, Luis},
Editor = {{Hernando, J and FernandezMedina, E and Malek, M}},
Title = {{Privacy preserving k-means clustering in multi-party environment}},
Booktitle = {{SECRYPT 2007: PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON
   SECURITY AND CRYPTOGRAPHY}},
Year = {{2007}},
Pages = {{381+}},
Note = {{International Conference on Security and Cryptography, Barcelona, SPAIN,
   JUL 28-31, 2007}},
Abstract = {{Extracting meaningful and valuable knowledge from databases is often
   done by various data mining algorithms. Nowadays, databases are
   distributed among two or more parties because of different reasons such
   as physical and geographical restrictions and the most important issue
   is privacy. Related data is normally maintained by more than one
   organization, each of which wants to keep its individual information
   private. Thus, privacy-preserving techniques and protocols are designed
   to perform data mining on distributed environments when privacy is
   highly concerned. Cluster analysis is a technique in data mining, by
   which data can be divided into some meaningful clusters, and it has an
   important role in different fields such as bio-informatics, marketing,
   machine learning, climate and medicine. k-means Clustering is a
   prominent algorithm in this category which creates a one-level
   clustering of data. In this paper we introduce privacy-preserving
   protocols for this algorithm, along with a protocol for Secure
   comparison, known as the Millionaires' Problem, as a sub-protocol, to
   handle the clustering of horizontally or vertically partitioned data
   among two or more parties.}},
DOI = {{10.5220/0002121703810385}},
ISBN = {{978-989-8111-12-8}},
ORCID-Numbers = {{Samet, Saeed/0000-0002-5116-5484
   Orozco-Barbosa, Luis/0000-0003-1510-1608}},
Unique-ID = {{ISI:000250830700053}},
}

@article{ ISI:000220258300005,
Author = {Verykios, VS and Elmagarmid, AK and Bertino, E and Saygin, Y and
   Dasseni, E},
Title = {{Association rule hiding}},
Journal = {{IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING}},
Year = {{2004}},
Volume = {{16}},
Number = {{4}},
Pages = {{434-447}},
Month = {{APR}},
Abstract = {{Large repositories of data contain sensitive information that must be
   protected against unauthorized access. The protection of the
   confidentiality of this information has been a long-term goal for the
   database security research community and for the government statistical
   agencies. Recent advances in data mining and machine learning algorithms
   have increased the disclosure risks that one may encounter when
   releasing data to outside parties. A key problem, and still not
   sufficiently investigated, is the need to balance the confidentiality of
   the disclosed data with the legitimate needs of the data users. Every
   disclosure limitation method affects, in some way, and modifies true
   data values and relationships. In this paper, we investigate
   confidentiality issues of a broad category of rules, the association
   rules. In particular, we present three strategies and five algorithms
   for hiding a group of association rules, which is characterized as
   sensitive. One rule is characterized as sensitive if its disclosure risk
   is above a certain privacy threshold. Sometimes, sensitive rules should
   not be disclosed to the public since, among other things, they may be
   used for inferring sensitive data or they may provide business
   competitors with an advantage. We also perform an evaluation study of
   the hiding algorithms in order to analyze their time complexity and the
   impact that they have in the original database.}},
DOI = {{10.1109/TKDE.2004.1269668}},
ISSN = {{1041-4347}},
Unique-ID = {{ISI:000220258300005}},
}
